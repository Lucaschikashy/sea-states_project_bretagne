{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Wind turbine project\n",
    "\n",
    "This notebook is a brief example of the possibilities offered by the toolbox for modeling extreme values, adapted from the tools provided from the ResourceCode website.\n",
    "\n",
    "It relies on the `pyextreme` library which get installed with the Resourcecode toolbox. Here we demonstrate 2 examples of univariate modeling as shown in class. For more information, see https://georgebv.github.io/pyextremes/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# I. Wave Dynamics\n",
    "\n",
    "Part I of the wind turbine project spans 2 classes and consists of selecting a study site and analyzing the wave conditions at the study site. Last week you already began to select your study site, using criteria based on a wind speed threshold, water depth, and distance from the coast. Over the next 2 classes, you will: (A) characterize the mean wave conditions, and sea- sonal variability, and (B) estimate the extreme wave conditions. You will be asked to write up a concise report of your results of Parts I. A & B of the project (due on 24/10/2025). Some of this information will be needed in Parts II and III of your project to model wave transformation and to estimate the impacts of waves on an offshore wind turbine (OWT) for normal operating conditions and survival during extreme events. For the report concerning Part I of the project, please respond to the questions in I.A. (listed below) and I.B. (provided in next week’s class).\n",
    "\n",
    "## I.A. Characterizing the study site mean wave conditions\n",
    "\n",
    "The objective during this class is to characterize the mean wave conditions, as well as the seasonal and interannual variability. To do so, you can download wave time series from your study site from the ResourceCode wave database, which provides access to long-term (1994- 2020) hindcast simulations of wave conditions extending from 12°W to 13.5°E longitude, and from 36°N to 63°N latitude, covering the European and UK’s North Atlantic coast, Irish sea, the Northern Sea, and La Manche. This database was developed at the Ifremer by the LOPS (Laboratory of Ocean Physics and Satellite remote sensing), validated by the LOPS and LCSM (Marine Structures Laboratory), and analyzed at the LHEEA (Laboratory of Hydrody- namics, Energy and Atmospheric Environment) at Ecole Centrale Nantes.\n",
    "\n",
    "The hindcast uses the WAVEWATCH III (WW3) version 7.08 (WW3DG, 2019) spectral wave model, implemented with unstructured grids that have higher resolution in the coastal zone, thus enabling the reproduction of the wave climate in relatively shallow water. It was forced by ERA5 wind felds with a resolution of 0.25°and by the surface currents generated from an atlas of harmonic tidal constituents obtained from outputs of the MARS 2D circulation model and the FES2014 model. The model is particularly well suited for predicting offshore wave conditions, as will be discussed further in class in the lecture presenting spectral wave models (S6). From the ResourceCode website, look at the ‘Explore’ tab to consult the map showing where wave time series are available (https://resourcecode.ifremer.fr/). Use the available toolbox to download the wave time series (available via the ‘Tools’ tab). Refer to the ResourceCode manual (available on the website and in the class repository) for more information about the available variables (about wind, waves, currents, bathymetry,...) and how they are calculated.\n",
    "\n",
    "---\n",
    "\n",
    "### Import Required Libraries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import matplotlib.pyplot as plt\n",
    "import pymannkendall as mk\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "from pyextremes import (\n",
    "    plot_mean_residual_life,\n",
    "    plot_parameter_stability, \n",
    "    EVA\n",
    ")\n",
    "import resourcecode\n",
    "from IPython import get_ipython\n",
    "\n",
    "from resourcecode.eva import (\n",
    "    get_fitted_models,\n",
    "    get_gpd_parameters,\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.makedirs('fig', exist_ok=True)\n",
    "plt.savefig('fig/diagnostic_plot_bm.png', dpi=200, bbox_inches='tight')\n",
    "\n",
    "# Enable inline plotting for Jupyter notebooks\n",
    "get_ipython().run_line_magic(\"matplotlib\", \"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "---\n",
    "### I.A.1. Mean Wave Conditions\n",
    "\n",
    "Please download the variables corresponding to the significant wave height, mean wave period ($T_{m02}$), and mean wave direction. Then plot the time series of these variables during the 26-year period from 1994 to 2020, and calculate the mean significant wave height, period, and direction over the entire available time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Chosen Study Site\n",
    "\n",
    "**Site:** Bretagne Sud 1\n",
    "**Coordinates:** $(47.3236111, -3.5522222)$. Coordinates were obtained by downloading the implantation zone KML files from https://www.eoliennesenmer.fr/facades-maritimes-en-france/facade-nord-atlantique-manche-ouest/projet-en-bretagne-sud/bretagne-sud-1. The KML files were then uploaded to Google Earth Pro, where a point at the centre of the zone was chosen, and those coordinates were used.\n",
    "\n",
    "resourcecode.data.get_closest_point returns two things: the identifier of the grid node it matched (point_id) and the horizontal separation between your target coordinates and that node expressed in meters. So dist_m is that offset—how far (in meters) the chosen ResourceCode point lies from the latitude/longitude you supplied. If it’s near zero you’re right on top of a node; larger values mean the dataset had to pick the nearest available grid point.\n",
    "\n",
    "Therefore, the point analysed is 1408.67 m away from the desired coordinates, which is insignificant and should not impact results much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Variables: definitions and units\n",
    "\n",
    "**Core met-ocean time series**\n",
    "\n",
    "| name   | meaning                                   | units |\n",
    "|--------|-------------------------------------------|-------|\n",
    "| `hs`   | significant wave height $H_{m0}=4\\sqrt{m_0}$ | m     |\n",
    "| `t02`  | mean zero-crossing period $T_{m02}=2\\pi\\sqrt{m_0/m_2}$ | s     |\n",
    "| `dir`  | mean wave direction                       | °     |\n",
    "| `spr`  | directional spreading                     | °     |\n",
    "| `fp`   | spectral peak frequency                   | Hz    |\n",
    "| `Tp`   | peak period $=1/fp$                     | s     |\n",
    "| `uwnd` | eastward wind component                   | m·s⁻¹ |\n",
    "| `vwnd` | northward wind component                  | m·s⁻¹ |\n",
    "| `wspd` | wind speed $\\sqrt{uwnd^2+vwnd^2}$       | m·s⁻¹ |\n",
    "| `wdir` | wind direction                            | °     |\n",
    "| `ucur` | eastward surface current                  | m·s⁻¹ |\n",
    "| `vcur` | northward surface current                 | m·s⁻¹ |\n",
    "| `cspd` | current speed $\\sqrt{ucur^2+vcur^2}$    | m·s⁻¹ |\n",
    "| `cdir` | current direction                         | °     |\n",
    "| `dpt`  | water depth                               | m     |\n",
    "\n",
    "**Spectral moments**\n",
    "\n",
    "| name | meaning                               | units  |\n",
    "|------|----------------------------------------|--------|\n",
    "| `m0` | zeroth moment $\\int S(\\omega)\\,d\\omega$ | m²     |\n",
    "| `m1` | first moment $\\int \\omega S(\\omega)\\,d\\omega$ | m²·s⁻¹ |\n",
    "| `m2` | second moment $\\int \\omega^2 S(\\omega)\\,d\\omega$ | m²·s⁻² |\n",
    "\n",
    "**Extreme value analysis (pyextremes)**\n",
    "\n",
    "| item     | meaning                                  |\n",
    "|----------|------------------------------------------|\n",
    "| BM       | block-maxima extraction                  |\n",
    "| POT      | peaks-over-threshold with declustering   |\n",
    "| GEV $\\mu,\\sigma,\\xi$ | location, scale, shape for BM     |\n",
    "| GPD $\\sigma,\\xi$     | scale, shape for POT at a threshold |\n",
    "| `r`      | min time separation between clusters     |\n",
    "| `alpha`  | confidence level for intervals           |\n",
    "| $z_T$  | return level for period $T$ years      |\n",
    "\n",
    "**Direction conventions**\n",
    "\n",
    "All directions are expressed **clockwise from North**.\n",
    "\n",
    "| Variable | Convention | Notes |\n",
    "|-----------|-------------|-------|\n",
    "| `wdir` | *coming-from* | Derived from (`uwnd`, `vwnd`) using `resourcecode.utils.zmcomp2metconv`. |\n",
    "| `dir_from` | *coming-from* | Use if dataset `dir` is *going-to*: convert by `dir_from = (dir + 180) % 360`. |\n",
    "| `cdir_to` | *going-to* | Derived from (`ucur`, `vcur`); use as-is for current flow direction. |\n",
    "| `cdir_from` | *coming-from* (optional) | For comparison with wave/wind directions, compute `cdir_from = (cdir_to + 180) % 360`. |\n",
    "\n",
    "> Always state the convention in figure captions and keep it consistent across the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = resourcecode.Client()\n",
    "# load the resourcecode dataset from Bretagne Sud 1\n",
    "# find the closest point to the coordinates\n",
    "lat = 47.3236111\n",
    "long = -3.5522222\n",
    "point_id, dist_m = resourcecode.data.get_closest_point(latitude=lat, longitude=long)\n",
    "print(point_id, dist_m)\n",
    "\n",
    "# get the data from the closest point\n",
    "data = client.get_dataframe_from_criteria(\n",
    "    \"\"\"\n",
    "{\n",
    "    \"node\": 117231,\n",
    "    \"start\": 0,\n",
    "    \"end\": 99999903600,\n",
    "    \"parameter\": [\"hs\",\"t02\",\"dir\",\"uwnd\",\"vwnd\",\"ucur\",\"vcur\",\"dpt\"]\n",
    "}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive only what ResourceCode does not already provide\n",
    "# Assumes your download included: hs, t02, dir, uwnd, vwnd (and optionally ucur, vcur, dpt)\n",
    "\n",
    "# Wind: speed (m/s) and coming-from direction (deg)\n",
    "data[\"wspd\"], data[\"wdir\"] = resourcecode.utils.zmcomp2metconv(data[\"uwnd\"], data[\"vwnd\"])\n",
    "\n",
    "# Currents (optional)\n",
    "if {\"ucur\", \"vcur\"}.issubset(data.columns):\n",
    "    data[\"cspd\"], data[\"cdir\"] = resourcecode.utils.zmcomp2metconv(data[\"ucur\"], data[\"vcur\"])\n",
    "\n",
    "# Waves: use provided mean zero-crossing period\n",
    "if \"t02\" in data.columns:\n",
    "    data[\"Tm02\"] = data[\"t02\"]\n",
    "else:\n",
    "    raise KeyError(\"Missing 't02' in the request. Add 't02' to parameter list.\")\n",
    "\n",
    "# Keep dataset wave direction as-is; document convention once in the notebook.\n",
    "# Do NOT overwrite 'dir' or try to recompute Tm02 from hs/dir.\n",
    "\n",
    "data = data.sort_index()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "**Q: Do any trends in the wave height, period, or direction exist over the 26 year time period? Do you expect there to be changes in the mean conditions during the 30-year lifetime of the wind turbine? If so, why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IA.1 — Mean wave conditions (1994–2020): time series, means, linear trends\n",
    "\n",
    "def circmean_deg(x_deg):\n",
    "    \"\"\"Circular mean of degrees in [0, 360).\"\"\"\n",
    "    x = pd.Series(x_deg).dropna().values\n",
    "    if x.size == 0:\n",
    "        return np.nan\n",
    "    r = np.deg2rad(x)\n",
    "    s = np.sin(r).sum()\n",
    "    c = np.cos(r).sum()\n",
    "    return (np.degrees(np.arctan2(s, c)) + 360.0) % 360.0\n",
    "\n",
    "def verdict(p):\n",
    "    return \"significant\" if p < 0.05 else \"not significant\"\n",
    "\n",
    "# helper\n",
    "def wrap360(a):\n",
    "    return (a % 360.0 + 360.0) % 360.0\n",
    "\n",
    "# Select analysis window\n",
    "start = pd.Timestamp(\"1994-01-01\")\n",
    "end   = pd.Timestamp(\"2020-12-31 23:59:59\")\n",
    "needed = [\"hs\", \"Tm02\", \"dir\"]\n",
    "missing = [v for v in needed if v not in data.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing variables for IA.1: {missing}\")\n",
    "\n",
    "df = data.loc[start:end, needed].copy()\n",
    "\n",
    "# Monthly means (direction handled circularly)\n",
    "monthly = pd.DataFrame({\n",
    "    \"hs\":   df[\"hs\"].resample(\"M\").mean(),\n",
    "    \"Tm02\": df[\"Tm02\"].resample(\"M\").mean(),\n",
    "})\n",
    "monthly[\"dir\"] = df[\"dir\"].resample(\"M\").apply(circmean_deg)\n",
    "\n",
    "# Overall means (direction: circular mean)\n",
    "mean_hs   = df[\"hs\"].mean()\n",
    "mean_tm02 = df[\"Tm02\"].mean()\n",
    "mean_dir  = circmean_deg(df[\"dir\"])\n",
    "\n",
    "# Linear trends on monthly means\n",
    "t_years = (monthly.index - monthly.index[0]).days / 365.2425\n",
    "\n",
    "# Hs trend\n",
    "hs_ok = monthly[\"hs\"].dropna()\n",
    "t_hs = t_years[hs_ok.index.get_indexer(hs_ok.index)]\n",
    "hs_reg = stats.linregress(t_hs, hs_ok.values)\n",
    "hs_slope_dec = hs_reg.slope * 10.0      # m per decade\n",
    "hs_delta_30  = hs_reg.slope * 30.0      # m over 30 years\n",
    "\n",
    "# Tm02 trend\n",
    "tm_ok = monthly[\"Tm02\"].dropna()\n",
    "t_tm = t_years[tm_ok.index.get_indexer(tm_ok.index)]\n",
    "tm_reg = stats.linregress(t_tm, tm_ok.values)\n",
    "tm_slope_dec = tm_reg.slope * 10.0      # s per decade\n",
    "tm_delta_30  = tm_reg.slope * 30.0      # s over 30 years\n",
    "\n",
    "# Direction trend: unwrap, regress, report slope in deg/dec\n",
    "dir_ok = monthly[\"dir\"].dropna()\n",
    "t_dir = t_years[dir_ok.index.get_indexer(dir_ok.index)]\n",
    "dir_unwrap = np.degrees(np.unwrap(np.deg2rad(dir_ok.values)))\n",
    "dir_reg = stats.linregress(t_dir, dir_unwrap)\n",
    "dir_slope_dec = dir_reg.slope * 10.0    # deg per decade\n",
    "dir_delta_30  = dir_reg.slope * 30.0    # deg over 30 years\n",
    "\n",
    "# Plot monthly series (1994–2020)\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 9), sharex=True)\n",
    "\n",
    "# Hs with fitted line\n",
    "axes[0].plot(monthly.index, monthly[\"hs\"], lw=0.8)\n",
    "yhat_hs = hs_reg.intercept + hs_reg.slope * t_years\n",
    "axes[0].plot(monthly.index, yhat_hs, lw=1.2)\n",
    "axes[0].set_ylabel(\"$H_S$ (m)\")\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_title(f\"$H_S$ monthly mean | {hs_slope_dec:.3f} m/dec, p={hs_reg.pvalue:.3f} ({verdict(hs_reg.pvalue)})\")\n",
    "\n",
    "# Tm02 with fitted line\n",
    "axes[1].plot(monthly.index, monthly[\"Tm02\"], lw=0.8)\n",
    "yhat_tm = tm_reg.intercept + tm_reg.slope * t_years\n",
    "axes[1].plot(monthly.index, yhat_tm, lw=1.2)\n",
    "axes[1].set_ylabel(\"$T_{m02}$ (s)\")\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_title(f\"Tm02 monthly mean | {tm_slope_dec:.3f} s/dec, p={tm_reg.pvalue:.3f} ({verdict(tm_reg.pvalue)})\")\n",
    "\n",
    "# Direction (circular monthly mean) with fitted line\n",
    "axes[2].plot(monthly.index, monthly[\"dir\"], lw=0.8)\n",
    "yhat_dir_unwrap = dir_reg.intercept + dir_reg.slope * t_years\n",
    "yhat_dir = wrap360(yhat_dir_unwrap)\n",
    "axes[2].plot(monthly.index, yhat_dir, lw=1.2)  # add fit\n",
    "axes[2].set_ylabel(\"Dir (deg, coming-from)\")\n",
    "axes[2].grid(alpha=0.3)\n",
    "axes[2].set_title(\n",
    "    f\"Direction monthly circular mean | {dir_slope_dec:.2f}°/dec, \"\n",
    "    f\"p={dir_reg.pvalue:.3f} ({verdict(dir_reg.pvalue)})\"\n",
    ")\n",
    "axes[2].set_xlabel(\"Year\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fig/IA1_mean_wave_conditions_timeseries.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Printed summary for the report\n",
    "print(\"IA.1 — Mean conditions over 1994–2020\")\n",
    "print(f\"  Mean Hs   : {mean_hs:.3f} m\")\n",
    "print(f\"  Mean Tm02 : {mean_tm02:.3f} s\")\n",
    "print(f\"  Mean Dir  : {mean_dir:.1f}° (coming-from)\")\n",
    "\n",
    "print(\"\\nLinear trends on monthly means (least squares):\")\n",
    "print(f\"  Hs   : {hs_slope_dec:.3f} m/dec  (p={hs_reg.pvalue:.3f}, n={hs_ok.size}), Δ30y={hs_delta_30:.3f} m\")\n",
    "print(f\"  Tm02 : {tm_slope_dec:.3f} s/dec  (p={tm_reg.pvalue:.3f}, n={tm_ok.size}), Δ30y={tm_delta_30:.3f} s\")\n",
    "print(f\"  Dir  : {dir_slope_dec:.2f} °/dec (p={dir_reg.pvalue:.3f}, n={dir_ok.size}), Δ30y={dir_delta_30:.2f} °\")\n",
    "\n",
    "print(\"\\nInterpretation rule-of-thumb: treat p<0.05 as evidence of a trend. Use Δ30y to state expected change over a turbine lifetime.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_hs  = mk.original_test(hs_ok.values, alpha=0.05)\n",
    "result_period = mk.original_test(tm_ok.values, alpha=0.050)\n",
    "result_direction = mk.original_test(dir_ok.values, alpha=0.05)\n",
    "\n",
    "print(\"\\nResults for Significant Wave Height (Hs):\")\n",
    "print(f\"  Trend: {result_hs.trend}\")\n",
    "print(f\"  H (Test Statistic): {result_hs.h}\")\n",
    "print(f\"  P-value: {result_hs.p:.4f}\")\n",
    "print(f\"  Z-Score: {result_hs.z:.4f}\")\n",
    "print(f\"  Tau: {result_hs.Tau:.4f}\")\n",
    "print(f\"  Sen's Slope: {result_hs.slope:.4f}\")\n",
    "print(f\"  Intercept: {result_hs.intercept:.4f}\")\n",
    "\n",
    "print(\"\\nResults for Wave Period (Tm02):\")\n",
    "print(f\"  Trend: {result_period.trend}\")\n",
    "print(f\"  H (Test Statistic): {result_period.h}\")\n",
    "print(f\"  P-value: {result_period.p:.4f}\")\n",
    "print(f\"  Z-Score: {result_period.z:.4f}\")\n",
    "print(f\"  Tau: {result_period.Tau:.4f}\")\n",
    "print(f\"  Sen's Slope: {result_period.slope:.4f}\")\n",
    "print(f\"  Intercept: {result_period.intercept:.4f}\")\n",
    "\n",
    "print(\"\\nResults for Wave Direction:\")\n",
    "print(f\"  Trend: {result_direction.trend}\")\n",
    "print(f\"  H (Test Statistic): {result_direction.h}\")\n",
    "print(f\"  P-value: {result_direction.p:.4f}\")\n",
    "print(f\"  Z-Score: {result_direction.z:.4f}\")\n",
    "print(f\"  Tau: {result_direction.Tau:.4f}\")\n",
    "print(f\"  Sen's Slope: {result_direction.slope:.4f}\")\n",
    "print(f\"  Intercept: {result_direction.intercept:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "**Answer:** No statistically significant trends were detected in wave height ($H_s$), period ($T_{m02}$), or direction using either Ordinary Least Squares (OLS) or the Mann-Kendall (MK) test on monthly mean data. Expected 30-year changes in mean conditions remain negligible relative to natural variability.\n",
    "\n",
    "---\n",
    "### Mean Conditions and Trend Analysis (1994-2020)\n",
    "\n",
    "**Mean conditions**\n",
    "-   $\\overline{H_s} = 2.092\\ \\text{m}$\n",
    "-   $\\overline{T_{m02}} = 5.785\\ \\text{s}$\n",
    "-   Mean direction $= 275.5^\\circ$ (coming from W)\n",
    "\n",
    "---\n",
    "**Trend Analysis Methods**\n",
    "-   Aggregate hourly data to monthly means to reduce noise while retaining seasonal information influence.\n",
    "-   **Method 1: Ordinary Least Squares (OLS)**: Perform regression vs time in years. Direction handled with circular monthly mean, then unwrapped.\n",
    "-   **Method 2: Mann-Kendall (MK) Test**: Apply the non-parametric test to detect monotonic trends in the monthly mean time series.\n",
    "-   Significance for both methods assessed at $\\alpha=0.05$.\n",
    "\n",
    "---\n",
    "**OLS Results** (slope per decade; $\\Delta 30\\text{y}$ is implied 30-year change; $n=324$ monthly means)\n",
    "-   $H_s$: $+0.015\\ \\text{m/dec}$, $p=0.776$, $\\Delta 30\\text{y}\\approx +0.046\\ \\text{m}$. (No significant trend)\n",
    "-   $T_{m02}$: $+0.086\\ \\text{s/dec}$, $p=0.150$, $\\Delta 30\\text{y}\\approx +0.259\\ \\text{s}$. (No significant trend)\n",
    "-   Direction: $+0.75^\\circ/\\text{dec}$, $p=0.349$, $\\Delta 30\\text{y}\\approx +2.26^\\circ$. (No significant trend)\n",
    "\n",
    "---\n",
    "**MK Results (on monthly means)**\n",
    "-   $H_s$: $p=0.8098$ (no significant trend); Sen's slope $\\approx +1.0\\times10^{-4}$ m/month ($\\approx +0.012$ m/decade).\n",
    "-   $T_{m02}$: $p=0.1957$ (no significant trend); Sen's slope $\\approx +7.0\\times10^{-4}$ s/month ($\\approx +0.084$ s/decade).\n",
    "-   Direction: $p=0.4405$ (no significant trend); Sen's slope $\\approx +5.5\\times10^{-3}$ deg/month ($\\approx +0.66^\\circ$/decade).\n",
    "\n",
    "*(Note: Sen's slopes converted approximately from per-month to per-decade for comparison with OLS results)*\n",
    "\n",
    "---\n",
    "**Interpretation**\n",
    "-   Both OLS and Mann-Kendall analyses performed on monthly mean data consistently indicate no statistically significant secular trends in mean $H_s$, $T_{m02}$, or direction over the 1994-2020 period.\n",
    "-   The magnitudes of the calculated slopes (both OLS and Sen's slope) are very small, suggesting that any potential underlying linear or monotonic change over the 26 years is minimal compared to the observed variability.\n",
    "-   Expected 30-year changes based on these negligible trends are small compared to the substantial seasonal and interannual variability present in the data.\n",
    "-   For design purposes over the next 30 years, emphasis should likely remain on characterizing the existing variability (seasonal, interannual) and extreme conditions rather than adjusting significantly for potential drifts in mean conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### I.A.2. Most Common Wave Conditions\n",
    "\n",
    "To identify the most common operating conditions, we will create a 2D histogram (scatter diagram) of significant wave height ($H_{m0}$) versus mean wave period ($T_{m02}$). We will also plot a wave rose to identify the most common wave incidence direction(s).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I.A.2 — Most common wave conditions: (T_m02, H_m0) scatter diagram + deep/linear regimes + wave rose\n",
    "# --------------------------\n",
    "# Inputs and thresholds\n",
    "# --------------------------\n",
    "g = 9.81  # m/s^2\n",
    "lat = float(lat) if \"lat\" in globals() else np.nan\n",
    "long = float(long) if \"long\" in globals() else np.nan\n",
    "\n",
    "# Clean series\n",
    "T = pd.to_numeric(data[\"t02\"], errors=\"coerce\").to_numpy()\n",
    "Hs = pd.to_numeric(data[\"hs\"],  errors=\"coerce\").to_numpy()\n",
    "Dpt = pd.to_numeric(data[\"dpt\"], errors=\"coerce\").to_numpy()\n",
    "Dir = pd.to_numeric(data[\"dir\"], errors=\"coerce\").to_numpy() if \"dir\" in data.columns else None\n",
    "\n",
    "mask = np.isfinite(T) & np.isfinite(Hs) & np.isfinite(Dpt)\n",
    "if Dir is not None:\n",
    "    mask = mask & np.isfinite(Dir)\n",
    "\n",
    "T = T[mask]\n",
    "Hs = Hs[mask]\n",
    "Dpt = Dpt[mask]\n",
    "Dir = Dir[mask] if Dir is not None else None\n",
    "\n",
    "h_mean = float(np.nanmean(Dpt))\n",
    "h_std  = float(np.nanstd(Dpt))\n",
    "\n",
    "# Deep and shallow (linear shallow-water) period thresholds at mean depth\n",
    "T_deep    = 4.0 * np.sqrt(h_mean / g)     # deep-water if T < T_deep\n",
    "T_shallow = 25.0 * np.sqrt(h_mean / g)    # shallow-water (linear nondispersive) if T > T_shallow\n",
    "\n",
    "print(f\"Water depth: mean h = {h_mean:.2f} m (std = {h_std:.2f} m) at ({lat}, {long})\")\n",
    "print(f\"Deep-water threshold: T < {T_deep:.2f} s\")\n",
    "print(f\"Shallow-water (linear) threshold: T > {T_shallow:.2f} s\")\n",
    "\n",
    "# --------------------------\n",
    "# 2D histogram: period on x, Hs on y\n",
    "# --------------------------\n",
    "# Bin ranges, robust to outliers\n",
    "T_max_plot = np.nanpercentile(T, 99.5)\n",
    "Hs_max_plot = np.nanpercentile(Hs, 99.5)\n",
    "T_min_plot = max(0.0, np.nanpercentile(T, 0.5))\n",
    "Hs_min_plot = max(0.0, np.nanpercentile(Hs, 0.5))\n",
    "\n",
    "xbins = np.linspace(T_min_plot, T_max_plot, 120)\n",
    "ybins = np.linspace(Hs_min_plot, Hs_max_plot, 120)\n",
    "\n",
    "H2, xedges, yedges = np.histogram2d(T, Hs, bins=[xbins, ybins], density=True)\n",
    "# Peak (most common) conditions from density maximum\n",
    "imax = np.unravel_index(np.nanargmax(H2), H2.shape)\n",
    "T_mode = 0.5 * (xedges[imax[0]] + xedges[imax[0] + 1])\n",
    "Hs_mode = 0.5 * (yedges[imax[1]] + yedges[imax[1] + 1])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(9, 7))\n",
    "plt.hist2d(T, Hs, bins=[xbins, ybins], density=True, cmap=\"viridis\")\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label(\"Density\")\n",
    "\n",
    "# Thresholds: vertical lines in T\n",
    "plt.axvline(T_deep, color=\"red\", linestyle=\"--\", linewidth=1.5, label=f\"Deep-water: T<{T_deep:.2f}s\")\n",
    "plt.axvline(T_shallow, color=\"black\", linestyle=\":\", linewidth=1.5, label=f\"Shallow (linear): T>{T_shallow:.2f}s\")\n",
    "\n",
    "# Annotate regions\n",
    "y_txt = Hs_min_plot + 0.92 * (Hs_max_plot - Hs_min_plot)\n",
    "plt.text(T_min_plot + 0.02*(T_max_plot-T_min_plot), y_txt, \"Deep-water\", color=\"red\", fontsize=10)\n",
    "plt.text(T_shallow + 0.02*(T_max_plot-T_min_plot), y_txt, \"Shallow (linear)\", color=\"black\", fontsize=10)\n",
    "\n",
    "# Most common bin marker\n",
    "plt.plot(T_mode, Hs_mode, marker=\"o\", markersize=6, color=\"white\", mec=\"k\", label=f\"Mode ≈ ({T_mode:.2f}s, {Hs_mode:.2f}m)\")\n",
    "plt.annotate(f\"{T_mode:.2f}s, {Hs_mode:.2f}m\", xy=(T_mode, Hs_mode),\n",
    "             xytext=(5, -12), textcoords=\"offset points\", color=\"white\", fontsize=9)\n",
    "\n",
    "plt.xlabel(\"Mean Wave Period $T_{m02}$ [s]\")\n",
    "plt.ylabel(\"Significant Wave Height $H_{m0}$ [m]\")\n",
    "plt.title(\"Most Common Wave Conditions: $T_{m02}$ vs $H_{m0}$\")\n",
    "plt.legend(loc=\"lower right\", frameon=True)\n",
    "Path(\"graphs\").mkdir(parents=True, exist_ok=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"graphs/wave_T_vs_Hs_density.png\", dpi=200)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Quantify shares for reference\n",
    "pct_deep = np.mean(T < T_deep) * 100.0\n",
    "pct_shallow = np.mean(T > T_shallow) * 100.0\n",
    "pct_transition = 100.0 - pct_deep - pct_shallow\n",
    "print(f\"Share deep-water (T<{T_deep:.2f}s): {pct_deep:.2f}%\")\n",
    "print(f\"Share shallow (linear) (T>{T_shallow:.2f}s): {pct_shallow:.4f}%\")\n",
    "print(f\"Share transition: {pct_transition:.2f}%\")\n",
    "\n",
    "# --------------------------\n",
    "# Wave rose: most common wave incidence directions\n",
    "# --------------------------\n",
    "if Dir is not None:\n",
    "    # Define Hs classes for a classic wave rose (adjust as needed)\n",
    "    hs_bins = [0, 1, 2, 3, 4, 6, np.inf]\n",
    "    hs_labels = [\"0–1\", \"1–2\", \"2–3\", \"3–4\", \"4–6\", \"≥6\"]  # m\n",
    "\n",
    "    # Direction bins (every 10°)\n",
    "    n_dir_bins = 36\n",
    "    dir_edges_deg = np.linspace(0, 360, n_dir_bins + 1)\n",
    "    dir_centers_deg = 0.5 * (dir_edges_deg[:-1] + dir_edges_deg[1:])\n",
    "    dir_edges_rad = np.deg2rad(dir_edges_deg)\n",
    "\n",
    "    # Prepare stacked counts per Hs class\n",
    "    counts_stack = []\n",
    "    for i in range(len(hs_bins) - 1):\n",
    "        mask_h = (Hs >= hs_bins[i]) & (Hs < hs_bins[i + 1])\n",
    "        hist, _ = np.histogram(Dir[mask_h] % 360.0, bins=dir_edges_deg)\n",
    "        counts_stack.append(hist.astype(float))\n",
    "    counts_stack = np.array(counts_stack)  # shape: [n_hs_classes, n_dir_bins]\n",
    "\n",
    "    # Convert counts to percentages\n",
    "    total_counts = counts_stack.sum(axis=0)\n",
    "    total_all = total_counts.sum()\n",
    "    frac_stack = (counts_stack / total_all) * 100.0  # percent of total occurrences\n",
    "\n",
    "    # Polar plot\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, projection=\"polar\")\n",
    "    ax.set_theta_zero_location(\"N\")\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    bottom = np.zeros(n_dir_bins)\n",
    "    width = np.diff(dir_edges_rad)\n",
    "\n",
    "    for i, label in enumerate(hs_labels):\n",
    "        ax.bar(np.deg2rad(dir_centers_deg), frac_stack[i], width=width,\n",
    "               bottom=bottom, align=\"center\", edgecolor=\"k\", linewidth=0.3, label=f\"{label} m\")\n",
    "        bottom += frac_stack[i]\n",
    "\n",
    "    ax.set_title(\"Wave Rose (coming-from)\\nStacked by $H_{m0}$ class [% of total]\", va=\"bottom\")\n",
    "    ax.set_rlabel_position(225)\n",
    "    ax.legend(loc=\"lower left\", bbox_to_anchor=(0.95, 0.05), frameon=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"graphs/wave_rose_stacked_Hs.png\", dpi=200)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Also print the dominant direction sector by total frequency\n",
    "    dom_bin = int(np.argmax(total_counts))\n",
    "    dom_dir_center = dir_centers_deg[dom_bin]\n",
    "    print(f\"Most common incidence direction bin center: {dom_dir_center:.1f}° (coming-from)\")\n",
    "else:\n",
    "    print(\"Column 'dir' not found. Skipping wave rose.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "**Q: What is the water depth at this location? Indicate on the histrogram for what wave conditions the waves are considered deep water waves? linear waves?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['dpt'].std())\n",
    "\n",
    "mean_water_depth = data['dpt'].mean()\n",
    "print(f'mean water depth at location (lat, long) = ({lat}, {long}) is {mean_water_depth:.2f} m')\n",
    "\n",
    "# Calculating wave length using the linear wave theory\n",
    "# wave length = g * T^2 / (2 * pi)\n",
    "g = 9.81 # m/s^2\n",
    "wave_length = g * (data['t02']**2) / (2 * np.pi)\n",
    "water_depth = data['dpt']\n",
    "\n",
    "# check condition for deep water waves\n",
    "deep_water_length_condition = 2 * mean_water_depth\n",
    "transition_water_length_condition = 25 * mean_water_depth\n",
    "\n",
    "deep_water_condition = wave_length < deep_water_length_condition\n",
    "# calculate the percentage of deep water waves\n",
    "deep_water_percentage = deep_water_condition.mean()\n",
    "print(f'percentage of deep water waves: {deep_water_percentage:.2%}')\n",
    "\n",
    "# Compute corresponding period for deep water waves\n",
    "deep_water_period_condition = 4 * np.sqrt(mean_water_depth / g)\n",
    "transition_water_period_condition = 25 * np.sqrt(mean_water_depth / g)\n",
    "\n",
    "print(f'deep water period: {deep_water_period_condition:.2f} s')\n",
    "# check condition for linear waves\n",
    "#linear_waves_condition = wave_length > 20 * mean_water_depth\n",
    "\n",
    "### plot the wave length distribution ###\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Corrected plt.hist call: remove wave_length.count\n",
    "plt.hist(wave_length, bins=50, density=True, alpha=0.7, label='Wave Length Distribution')\n",
    "\n",
    "# Add the threshold line (assuming L < 2h for deep water)\n",
    "plt.axvline(x=deep_water_length_condition, color='r', linestyle='--',\n",
    "            label=f'Deep Water Threshold (L < {deep_water_length_condition:.1f}m)')\n",
    "plt.axvline(x=transition_water_length_condition, color='g', linestyle='--',\n",
    "            label=f'Upper Transition Threshold to Linear Waves (L < {transition_water_length_condition:.1f}m)')\n",
    "plt.xlabel('Wave Length (m)')\n",
    "plt.ylabel('Density') # Changed label to Density since density=True\n",
    "plt.title('Wave Length Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.close() # Close after showing\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "### plot the wave period distribution ###\n",
    "plt.hist(data['t02'], bins=50, density=True, alpha=0.7, label='Wave Period Distribution')\n",
    "\n",
    "# Add the threshold line (assuming L < 2h for deep water)\n",
    "plt.axvline(x=deep_water_period_condition, color='r', linestyle='--',\n",
    "            label=f'Deep Water Threshold (T < {deep_water_period_condition:.1f}s)')\n",
    "plt.axvline(x=transition_water_period_condition, color='g', linestyle='--',\n",
    "            label=f'Transition to Linear Waves (T < {transition_water_period_condition:.1f}s)')\n",
    "plt.xlabel('Wave Period (s)')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Density') # Changed label to Density since density=True\n",
    "plt.title('Wave Period Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# --- 6. Result Summary ---\n",
    "print(\"\\n--- Result Summary ---\")\n",
    "print(f\"Analysis based on Mean Depth h = {mean_water_depth:.2f} m\")\n",
    "\n",
    "print(\"\\nThresholds:\")\n",
    "print(f\"  Deep Water:     L < {deep_water_length_condition:.1f} m   |   T < {deep_water_period_condition:.2f} s\")\n",
    "print(f\"  Intersection of Deep and Shallow Water:  L > {transition_water_length_condition:.1f} m  |   T > {transition_water_period_condition:.2f} s\")\n",
    "\n",
    "# Compute the percentage of deep water waves based on the approximate wavelength\n",
    "is_deep_water_wave = wave_length < deep_water_length_condition\n",
    "is_transition_zone_wave = (\n",
    "    (deep_water_length_condition < wave_length) &\n",
    "    (wave_length < transition_water_length_condition)\n",
    ")\n",
    "\n",
    "is_deep_water_period = data['t02'] < deep_water_period_condition\n",
    "is_transition_zone_period = (\n",
    "    (deep_water_period_condition < data['t02']) &\n",
    "    (data['t02'] < transition_water_period_condition)\n",
    ")\n",
    "# get a percentage of the boolean series of true and false\n",
    "deep_water_percentage_L_approx = is_deep_water_wave.mean()\n",
    "transition_percentage_L_approx = is_transition_zone_wave.mean()\n",
    "deep_water_percentage_T_approx = is_deep_water_period.mean()\n",
    "transition_percentage_T_approx = is_transition_zone_period.mean()\n",
    "\n",
    "print(\"\\nClassification based on Approximate Wavelength (L_approx = gT² / 2π):\")\n",
    "print(f\"  Percentage Deep Water Wave Lenghts: {deep_water_percentage_L_approx:.2%}\")\n",
    "print(f\"  Percentage Transition Zone Wave Lenghts: {transition_percentage_L_approx:.2%}\")\n",
    "\n",
    "# Characterizing the transition band for T\n",
    "print(\"\\nClassification based on Approximate Wavelength (L_approx = gT² / 2π):\")\n",
    "print(f\"  Percentage Deep Water Wave Periods: {deep_water_percentage_T_approx:.2%}\")\n",
    "print(f\"  Percentage Transition Zone Wave Periods: {transition_percentage_T_approx:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Wave Regime Classification\n",
    "\n",
    "**Method**\n",
    "\n",
    "- Hourly conditions classified into deep, transitional, and shallow regimes using $H_s$, $T_{m02}$, depth $h$, wavelength $L$.\n",
    "- Fixed mean depth $h=93.32\\ \\text{m}$ (depth variability small: $\\sigma_h=1.2\\ \\text{m}$).\n",
    "- Two diagnostics:\n",
    "  1) Period criterion via $T\\sqrt{g/h}$.\n",
    "  2) Approximate deep-water wavelength $L_{\\text{approx}}=\\dfrac{g\\,T_{m02}^2}{2\\pi}$ for a fast screen.\n",
    "- $g=9.81\\ \\text{m s}^{-2}$.\n",
    "\n",
    "---\n",
    "\n",
    "**Thresholds** (at $h=93.32\\ \\text{m}$)\n",
    "\n",
    "- **Deep water**: $h/L>1/2\\ \\Rightarrow\\ L<186.6\\ \\text{m}$; equivalently $T\\sqrt{g/h}<4\\ \\Rightarrow\\ T<12.34\\ \\text{s}$.\n",
    "- **Shallow water**: $h/L<1/25\\ \\Rightarrow\\ L>2333.0\\ \\text{m}$; equivalently $T\\sqrt{g/h}>25\\ \\Rightarrow\\ T>77.11\\ \\text{s}$.\n",
    "- **Transition zone**: between these limits.\n",
    "\n",
    "---\n",
    "\n",
    "**Results**\n",
    "\n",
    "- **Classification by approximate wavelength** $(L_{\\text{approx}})$:\n",
    "  - Deep water ($L_{\\text{approx}}<186.6\\ \\text{m}$): **99.76%**\n",
    "  - Transition zone ($186.6\\le L_{\\text{approx}}\\le 2333.0\\ \\text{m}$): **0.24%**\n",
    "  - Shallow water ($L_{\\text{approx}}>2333.0\\ \\text{m}$): **0.00%**\n",
    "\n",
    "- **Classification by period** $(T_{m02})$:\n",
    "  - Deep water ($T<12.34\\ \\text{s}$): **99.98%**\n",
    "  - Transition zone ($12.34\\le T\\le 77.11\\ \\text{s}$): **0.02%**\n",
    "  - Shallow water ($T>77.11\\ \\text{s}$): **0.00%**\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "- The site operates **almost entirely in deep water**. Period- and wavelength-based diagnostics both yield >99.7% deep conditions; the period test is slightly more conservative here.\n",
    "- **Transition cases are rare** (0.02–0.24%) and occur near the deep/transition boundary; none reach shallow-water criteria.\n",
    "- Using $L_{\\text{approx}}$ is adequate for screening at this depth because periods rarely approach the transitional threshold. For edge cases, the direct period criterion $T\\sqrt{g/h}$ remains the most transparent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### I.A.3. Seasonal Variability\n",
    "\n",
    "Next, we will evaluate temporal variability by calculating and plotting the mean wave height, period, and direction as a function of the month of the year (e.g., mean for January, February, etc.).\n",
    "\n",
    "**Data and method.** Hourly hindcast at coordinates. Monthly climatology across all years.  \n",
    "- Height and period: arithmetic mean with interquartile range (IQR).  \n",
    "- Direction: circular mean with circular standard deviation, expressed as *coming-from* degrees.  \n",
    "- Angles unwrapped around the overall mean for continuity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IA.3 Seasonal variability: monthly climatology of Hs, Tm02, and direction (mean ± IQR) ---\n",
    "\n",
    "# inputs\n",
    "need  = {\"hs\", \"Tm02\", \"dir\"}\n",
    "if not need.issubset(data.columns):\n",
    "    raise KeyError(f\"IA.3 needs columns {sorted(need)} in `data`.\")\n",
    "\n",
    "df3 = data.loc[start:end, [\"hs\", \"Tm02\", \"dir\"]].copy()\n",
    "if not isinstance(df3.index, pd.DatetimeIndex):\n",
    "    raise TypeError(\"`data` index must be a DatetimeIndex.\")\n",
    "\n",
    "# group by calendar month across all years\n",
    "g = df3.groupby(df3.index.month)  # 1..12\n",
    "\n",
    "# numeric climatology for Hs and Tm02\n",
    "hs_mean = g[\"hs\"].mean()\n",
    "hs_q25  = g[\"hs\"].quantile(0.25)\n",
    "hs_q75  = g[\"hs\"].quantile(0.75)\n",
    "\n",
    "tm_mean = g[\"Tm02\"].mean()\n",
    "tm_q25  = g[\"Tm02\"].quantile(0.25)\n",
    "tm_q75  = g[\"Tm02\"].quantile(0.75)\n",
    "\n",
    "n_samples = g.size()\n",
    "\n",
    "# circular climatology for direction (vectorised; stable across pandas versions)\n",
    "dd = df3[[\"dir\"]].copy()\n",
    "m = dd[\"dir\"].notna()\n",
    "dd.loc[m, \"sin\"] = np.sin(np.deg2rad(dd.loc[m, \"dir\"]))\n",
    "dd.loc[m, \"cos\"] = np.cos(np.deg2rad(dd.loc[m, \"dir\"]))\n",
    "\n",
    "dg = dd.groupby(dd.index.month)\n",
    "sin_mean = dg[\"sin\"].mean()\n",
    "cos_mean = dg[\"cos\"].mean()\n",
    "n_dir    = dg[\"dir\"].count()\n",
    "\n",
    "R = np.hypot(sin_mean, cos_mean)                                  # mean resultant length\n",
    "dir_mean = (np.degrees(np.arctan2(sin_mean, cos_mean)) + 360) % 360\n",
    "dir_cstd = np.degrees(np.sqrt(np.maximum(0.0, -2.0 * np.log(np.clip(R, 1e-12, 1.0)))))  # circular std (deg)\n",
    "\n",
    "# unwrap monthly direction around overall circular mean to avoid 0/360 jump\n",
    "sin_all = np.sin(np.deg2rad(df3[\"dir\"].dropna())).mean()\n",
    "cos_all = np.cos(np.deg2rad(df3[\"dir\"].dropna())).mean()\n",
    "overall_dir = (np.degrees(np.arctan2(sin_all, cos_all)) + 360) % 360\n",
    "dir_mean_unwrapped = overall_dir + ((dir_mean - overall_dir + 540.0) % 360.0 - 180.0)\n",
    "\n",
    "# assemble single DataFrame and ensure months 1..12 exist and in order\n",
    "clim = pd.DataFrame({\n",
    "    \"hs_mean\": hs_mean,\n",
    "    \"hs_q25\":  hs_q25,\n",
    "    \"hs_q75\":  hs_q75,\n",
    "    \"Tm02_mean\": tm_mean,\n",
    "    \"Tm02_q25\":  tm_q25,\n",
    "    \"Tm02_q75\":  tm_q75,\n",
    "    \"N_samples\": n_samples,\n",
    "    \"dir_mean\": dir_mean,\n",
    "    \"R\": R,\n",
    "    \"dir_cstd\": dir_cstd,\n",
    "    \"n_dir\": n_dir,\n",
    "    \"dir_mean_unwrapped\": dir_mean_unwrapped,\n",
    "}).reindex(range(1, 13))\n",
    "clim.index.name = \"month\"\n",
    "clim[\"month_name\"] = pd.to_datetime(clim.index, format=\"%m\").month_name().str.slice(0, 3)\n",
    "\n",
    "# save numeric table\n",
    "clim.to_csv(\"fig/IA3_seasonal_climatology.csv\", float_format=\"%.3f\")\n",
    "\n",
    "# plot\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 9), sharex=True)\n",
    "x = np.arange(1, 13)\n",
    "labels = clim[\"month_name\"].values\n",
    "\n",
    "# Hs: mean ± IQR\n",
    "ax = axes[0]\n",
    "ax.plot(x, clim[\"hs_mean\"].values, lw=1.8)\n",
    "ax.fill_between(x, clim[\"hs_q25\"].values, clim[\"hs_q75\"].values, alpha=0.25)\n",
    "ax.set_ylabel(\"Hs (m)\")\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_title(\"Monthly climatology: Hs (mean ± IQR)\")\n",
    "\n",
    "# Tm02: mean ± IQR\n",
    "ax = axes[1]\n",
    "ax.plot(x, clim[\"Tm02_mean\"].values, lw=1.8)\n",
    "ax.fill_between(x, clim[\"Tm02_q25\"].values, clim[\"Tm02_q75\"].values, alpha=0.25)\n",
    "ax.set_ylabel(\"Tm02 (s)\")\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_title(\"Monthly climatology: Tm02 (mean ± IQR)\")\n",
    "\n",
    "# Direction: circular mean ± circular std (unwrapped for continuity)\n",
    "ax = axes[2]\n",
    "y = clim[\"dir_mean_unwrapped\"].values\n",
    "yerr = clim[\"dir_cstd\"].values\n",
    "ax.errorbar(x, y, yerr=yerr, fmt=\"-o\", lw=1.5, capsize=3)\n",
    "pad = 5.0\n",
    "ymin, ymax = np.nanmin(y - yerr), np.nanmax(y + yerr)\n",
    "ax.set_ylim(ymin - pad, ymax + pad)\n",
    "ax.set_ylabel(\"Direction (deg, coming-from)\")\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_title(\"Monthly climatology: wave direction (circular mean ± circular std)\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fig/IA3_seasonal_climatology.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# quick textual summary\n",
    "hs_max_mo = int(clim[\"hs_mean\"].idxmax())\n",
    "hs_min_mo = int(clim[\"hs_mean\"].idxmin())\n",
    "tm_max_mo = int(clim[\"Tm02_mean\"].idxmax())\n",
    "tm_min_mo = int(clim[\"Tm02_mean\"].idxmin())\n",
    "print(\"IA.3 — Seasonal climatology (1994–2020)\")\n",
    "print(f\"  Hs peaks in {pd.to_datetime(hs_max_mo, format='%m').month_name()} \"\n",
    "      f\"and is lowest in {pd.to_datetime(hs_min_mo, format='%m').month_name()}.\")\n",
    "print(f\"  Tm02 peaks in {pd.to_datetime(tm_max_mo, format='%m').month_name()} \"\n",
    "      f\"and is lowest in {pd.to_datetime(tm_min_mo, format='%m').month_name()}.\")\n",
    "print(f\"  Direction mean (overall) ≈ {overall_dir:.1f}° (coming-from).\")\n",
    "print(\"  Saved: fig/IA3_seasonal_climatology.png and fig/IA3_seasonal_climatology.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "\n",
    "**Q: Do you observe any seasonal trends in wave height, period, or direction? If so, why?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "**Significant wave height, $H_s$.** Clear winter maximum and summer minimum. IQR widens in winter and narrows in summer.  \n",
    "**Interpretation.** Boreal-winter extratropical cyclones increase wind speed and fetch over the open North Atlantic, augmenting wind-sea and incoming swell towards south Brittany. Summer conditions are dominated by weaker winds and shorter fetch, so mean $H_s$ decreases and variability contracts. Because wave energy density scales as\n",
    "$$\n",
    "E=\\tfrac{1}{8}\\,\\rho g\\,H_s^2,\n",
    "$$\n",
    "higher winter $H_s$ implies markedly greater wave energy arriving at the site.\n",
    "\n",
    "**Mean zero-crossing period, $T_{m02}$.** Co-varies with $H_s$: longer in winter, shorter in summer.  \n",
    "**Interpretation.** Winter storms generate longer-period swell over long fetches. Deep-water dispersion $\\big(L\\approx gT^2/2\\pi,\\ c_g \\approx gT/4\\pi\\big)$ favours the far-field propagation of longer-period energy into the Bay of Biscay. In summer, local wind-sea contribution increases and typical periods shorten.\n",
    "\n",
    "### Wave direction (coming-from, clockwise from North)\n",
    "\n",
    "**Convention.** $0^\\circ=\\mathrm{N},\\ 90^\\circ=\\mathrm{E},\\ 180^\\circ=\\mathrm{S},\\ 270^\\circ=\\mathrm{W}$.\n",
    "\n",
    "**Seasonal pattern.** The monthly circular mean increases from winter ($\\sim 270^\\circ$) to summer ($\\sim 280^\\circ$), then decreases again in autumn.\n",
    "\n",
    "**Interpretation** Larger angles imply a more westerly approach. Thus, waves are slightly more **W–WSW** in summer and shift a little toward **SW** in winter.\n",
    "\n",
    "**Regional context.** Along the southern Brittany–Bay of Biscay sector, the summer expansion of the Azores High favours a more zonal (westerly) approach at the shelf break. In winter, frequent lows entering the Bay introduce a modest southerly component in the incident swell, yielding the observed decrease in direction angle.\n",
    "\n",
    "**Dispersion note.** The vertical bars are circular standard deviations (spread of hourly directions), not standard errors. They are of similar magnitude across months, so the dataset does **not** support a strong seasonal change in directional spread.\n",
    "\n",
    "## Implications\n",
    "\n",
    "- **Operations.** Highest loads and sea states occur in winter months; scheduling for installation or maintenance is more feasible in late spring–summer.  \n",
    "- **Variability.** Broad winter IQRs indicate stronger interannual modulation of sea states; design and planning should not rely on a single “typical” winter value.  \n",
    "- **Directionality.** The prevailing W–WSW approach is stable enough to justify directional binning around that sector for further analyses and for extreme value modelling in Part II.\n",
    "\n",
    "## Notes and caveats\n",
    "\n",
    "- Monthly means smooth synoptic extremes; use EVA for design loads.  \n",
    "- Direction statistics use circular metrics; reported means reflect modal approach rather than arithmetic averages near $0/360^\\circ$.  \n",
    "- The site is predominantly deep water, so seasonal patterns reflect atmospheric forcing and basin geometry rather than depth-limited effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### I.A.4. Mean Wind and Current Conditions\n",
    "\n",
    "We will also evaluate the wind and current conditions by plotting rose diagrams of their respective velocities and directions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IA.4 Mean wind and current conditions: roses + means (overall, seasonal) ---\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "# Calm thresholds used for ROSES only (kept in means table for reference)\n",
    "CALM_WIND = 0.5    # m/s\n",
    "CALM_CURR = 0.05   # m/s\n",
    "\n",
    "# Speed bins\n",
    "# Wind: [0.5–2), 2–4, 4–6, …, ≥16 m/s\n",
    "WIND_BINS = np.r_[0.5, np.arange(2, 18, 2), np.inf]\n",
    "# Currents: [0.05–0.1), 0.1–0.2, 0.2–0.3, 0.3–0.5, ≥0.5 m/s\n",
    "CURR_BINS = np.array([0.05, 0.10, 0.15, 0.2, np.inf])\n",
    "\n",
    "# Direction sectors\n",
    "WIND_SECTORS = 16   # 22.5°\n",
    "CURR_SECTORS = 36   # 10°\n",
    "\n",
    "# Date window to match the project\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def season_label(month):\n",
    "    # DJF, MAM, JJA, SON\n",
    "    if month in (12, 1, 2):\n",
    "        return \"DJF\"\n",
    "    if month in (3, 4, 5):\n",
    "        return \"MAM\"\n",
    "    if month in (6, 7, 8):\n",
    "        return \"JJA\"\n",
    "    return \"SON\"\n",
    "\n",
    "def rose_counts(dir_deg, spd, spd_bins, sectors=16, calm_thresh=0.0):\n",
    "    \"\"\"\n",
    "    Return stacked counts per direction sector and speed bin, plus metadata.\n",
    "    Directions: degrees, 0°=N, clockwise positive.\n",
    "    Excludes values with spd <= calm_thresh.\n",
    "    \"\"\"\n",
    "    mask = np.isfinite(dir_deg) & np.isfinite(spd) & (spd > calm_thresh)\n",
    "    d = np.asarray(dir_deg)[mask] % 360.0\n",
    "    s = np.asarray(spd)[mask]\n",
    "\n",
    "    if d.size == 0:\n",
    "        counts = np.zeros((sectors, len(spd_bins)-1), dtype=int)\n",
    "        return counts, np.array([]), 0\n",
    "\n",
    "    width = 360.0 / sectors\n",
    "    sector_idx = np.floor(d / width).astype(int)\n",
    "    sector_idx[sector_idx == sectors] = sectors - 1\n",
    "\n",
    "    bin_idx = np.digitize(s, spd_bins) - 1\n",
    "    bin_idx = np.clip(bin_idx, 0, len(spd_bins)-2)\n",
    "\n",
    "    counts = np.zeros((sectors, len(spd_bins)-1), dtype=int)\n",
    "    for k in range(d.size):\n",
    "        counts[sector_idx[k], bin_idx[k]] += 1\n",
    "\n",
    "    return counts, mask, d.size  # d.size = included samples after calm filter\n",
    "\n",
    "def plot_rose(dir_deg, spd, spd_bins, fname, title, calm_thresh=0.0, sectors=16):\n",
    "    \"\"\"\n",
    "    Polar stacked-bar rose. Heights are percentages of included samples.\n",
    "    Direction convention: 0° at North, clockwise positive.\n",
    "    \"\"\"\n",
    "    counts, mask, n_used = rose_counts(dir_deg, spd, spd_bins, sectors, calm_thresh)\n",
    "    if n_used == 0:\n",
    "        print(f\"{title}: no data above calm threshold.\")\n",
    "        return\n",
    "\n",
    "    sector_totals = counts.sum(axis=1).astype(float)\n",
    "    sector_totals[sector_totals == 0] = 1.0\n",
    "    frac = counts / sector_totals[:, None]\n",
    "    pct_sector = 100.0 * counts.sum(axis=1) / n_used\n",
    "\n",
    "    theta = np.deg2rad(np.arange(0, 360, 360/sectors))\n",
    "    width = 2.0 * np.pi / sectors\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    ax.set_theta_zero_location(\"N\")\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    bottoms = np.zeros(sectors)\n",
    "    labels = []\n",
    "    for j in range(len(spd_bins)-1):\n",
    "        label = f\"{spd_bins[j]:g}–{spd_bins[j+1]:g} m/s\" if np.isfinite(spd_bins[j+1]) else f\"{spd_bins[j]:g}+ m/s\"\n",
    "        labels.append(label)\n",
    "        heights = pct_sector * frac[:, j]\n",
    "        ax.bar(theta, heights, width=width, bottom=bottoms, align=\"edge\",\n",
    "               edgecolor=\"black\", linewidth=0.3)\n",
    "        bottoms += heights\n",
    "\n",
    "    ax.set_rlabel_position(225)\n",
    "    rmax = max(5.0, np.ceil(bottoms.max() / 5.0) * 5.0)\n",
    "    ax.set_ylim(0, rmax)\n",
    "    ax.set_yticks(np.linspace(0, rmax, 5))\n",
    "    ax.set_yticklabels([f\"{v:.0f}%\" for v in np.linspace(0, rmax, 5)])\n",
    "\n",
    "    ax.set_title(title + f\"\\n(calms ≤ {calm_thresh} m/s excluded; n={n_used})\",\n",
    "                 va=\"bottom\", fontsize=11)\n",
    "    ax.legend(labels, loc=\"lower left\", bbox_to_anchor=(0.9, -0.02),\n",
    "              frameon=True, fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def mean_table(dir_deg, spd, label, calm_thresh=None):\n",
    "    \"\"\"\n",
    "    Mean speed (arithmetic) and mean direction (circular) overall and by season.\n",
    "    Directions in degrees [0, 360).\n",
    "    calm_thresh listed for reference; does not alter means.\n",
    "    \"\"\"\n",
    "    idx = data.loc[start:end].index\n",
    "    df = pd.DataFrame({\"spd\": spd.loc[idx], \"dir\": dir_deg.loc[idx]})\n",
    "    df[\"season\"] = df.index.month.map(season_label)\n",
    "\n",
    "    def agg(g):\n",
    "        return pd.Series({\n",
    "            \"mean_speed_mps\": g[\"spd\"].mean(),\n",
    "            \"mean_dir_deg\": circmean_deg(g[\"dir\"]),\n",
    "            \"n\": g[\"dir\"].count()\n",
    "        })\n",
    "\n",
    "    overall = agg(df)\n",
    "    by_season = df.groupby(\"season\", sort=False).apply(agg)\n",
    "\n",
    "    overall.name = label\n",
    "    out = {\"overall\": overall.to_frame().T, \"season\": by_season}\n",
    "    out[\"meta\"] = {\"calm_threshold_listed_only_mps\": calm_thresh}\n",
    "    return out\n",
    "\n",
    "# -----------------------\n",
    "# Prepare series and conventions\n",
    "# -----------------------\n",
    "# Wind: already \"coming-from\" via zmcomp2metconv\n",
    "wspd = data[\"wspd\"].copy()\n",
    "wdir_from = data[\"wdir\"].copy()  # coming-from, 0°=N, clockwise\n",
    "\n",
    "# Currents: convert to \"going-to\" for the rose\n",
    "if {\"cspd\", \"cdir\"}.issubset(data.columns):\n",
    "    cspd = data[\"cspd\"].copy()\n",
    "    cdir_to = (data[\"cdir\"] + 180.0) % 360.0\n",
    "else:\n",
    "    cspd = None\n",
    "    cdir_to = None\n",
    "\n",
    "# Restrict to analysis window\n",
    "sel = slice(start, end)\n",
    "wspd = wspd.loc[sel]\n",
    "wdir_from = wdir_from.loc[sel]\n",
    "if cspd is not None:\n",
    "    cspd = cspd.loc[sel]\n",
    "    cdir_to = cdir_to.loc[sel]\n",
    "\n",
    "# -----------------------\n",
    "# 1) Full-period roses\n",
    "# -----------------------\n",
    "plot_rose(\n",
    "    dir_deg=wdir_from, spd=wspd, spd_bins=WIND_BINS,\n",
    "    fname=\"fig/IA4_wind_rose.png\",\n",
    "    title=\"Wind rose (coming-from; 0°=N, clockwise)\",\n",
    "    calm_thresh=CALM_WIND, sectors=WIND_SECTORS\n",
    ")\n",
    "\n",
    "if cspd is not None:\n",
    "    plot_rose(\n",
    "        dir_deg=cdir_to, spd=cspd, spd_bins=CURR_BINS,\n",
    "        fname=\"fig/IA4_current_rose.png\",\n",
    "        title=\"Current rose (going-to; 0°=N, clockwise)\",\n",
    "        calm_thresh=CALM_CURR, sectors=CURR_SECTORS\n",
    "    )\n",
    "\n",
    "# -----------------------\n",
    "# 2) Seasonal roses (DJF, MAM, JJA, SON)\n",
    "# -----------------------\n",
    "def plot_rose_seasons(dir_series, spd_series, spd_bins, calm_thresh, sectors, title_base, fname):\n",
    "    seasons = [\"DJF\", \"MAM\", \"JJA\", \"SON\"]\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10), subplot_kw=dict(polar=True))\n",
    "    axes = axes.ravel()\n",
    "    for i, s in enumerate(seasons):\n",
    "        sel = dir_series.index.map(lambda t: season_label(t.month) == s)\n",
    "        d = dir_series[sel]\n",
    "        v = spd_series[sel]\n",
    "        counts, mask, n_used = rose_counts(d.values, v.values, spd_bins, sectors, calm_thresh)\n",
    "        theta = np.deg2rad(np.arange(0, 360, 360/sectors))\n",
    "        width = 2.0 * np.pi / sectors\n",
    "\n",
    "        ax = axes[i]\n",
    "        ax.set_theta_zero_location(\"N\")\n",
    "        ax.set_theta_direction(-1)\n",
    "\n",
    "        if n_used == 0:\n",
    "            ax.set_title(f\"{s} (n=0)\")\n",
    "            continue\n",
    "\n",
    "        sector_totals = counts.sum(axis=1).astype(float)\n",
    "        sector_totals[sector_totals == 0] = 1.0\n",
    "        frac = counts / sector_totals[:, None]\n",
    "        pct_sector = 100.0 * counts.sum(axis=1) / n_used\n",
    "\n",
    "        bottoms = np.zeros(sectors)\n",
    "        for j in range(len(spd_bins)-1):\n",
    "            heights = pct_sector * frac[:, j]\n",
    "            ax.bar(theta, heights, width=width, bottom=bottoms, align=\"edge\",\n",
    "                   edgecolor=\"black\", linewidth=0.3)\n",
    "            bottoms += heights\n",
    "\n",
    "        ax.set_rlabel_position(225)\n",
    "        rmax = max(5.0, np.ceil(bottoms.max() / 5.0) * 5.0)\n",
    "        ax.set_ylim(0, rmax)\n",
    "        ax.set_yticks(np.linspace(0, rmax, 5))\n",
    "        ax.set_yticklabels([f\"{v:.0f}%\" for v in np.linspace(0, rmax, 5)])\n",
    "        ax.set_title(f\"{s} (calms ≤ {calm_thresh} m/s; n={n_used})\", fontsize=10)\n",
    "\n",
    "    fig.suptitle(title_base, y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0.02, 1, 0.95])\n",
    "    plt.savefig(fname, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_rose_seasons(\n",
    "    wdir_from, wspd, WIND_BINS, CALM_WIND, WIND_SECTORS,\n",
    "    \"Wind roses by season (coming-from; 0°=N, clockwise)\",\n",
    "    \"fig/IA4_wind_rose_seasons.png\"\n",
    ")\n",
    "\n",
    "if cspd is not None:\n",
    "    plot_rose_seasons(\n",
    "        cdir_to, cspd, CURR_BINS, CALM_CURR, CURR_SECTORS,\n",
    "        \"Current roses by season (going-to; 0°=N, clockwise)\",\n",
    "        \"fig/IA4_current_rose_seasons.png\"\n",
    "    )\n",
    "\n",
    "# -----------------------\n",
    "# 3) Mean conditions tables and CSV\n",
    "# -----------------------\n",
    "wind_stats = mean_table(wdir_from, wspd, label=\"wind\", calm_thresh=CALM_WIND)\n",
    "if cspd is not None:\n",
    "    curr_stats = mean_table(cdir_to, cspd, label=\"current\", calm_thresh=CALM_CURR)\n",
    "\n",
    "frames = []\n",
    "wind_overall = wind_stats[\"overall\"].assign(kind=\"wind\")\n",
    "wind_season  = wind_stats[\"season\"].assign(kind=\"wind\", level=\"season\").reset_index().rename(columns={\"season\":\"group\"})\n",
    "frames += [wind_overall.assign(level=\"overall\", group=\"all\"), wind_season]\n",
    "\n",
    "if cspd is not None:\n",
    "    curr_overall = curr_stats[\"overall\"].assign(kind=\"current\")\n",
    "    curr_season  = curr_stats[\"season\"].assign(kind=\"current\", level=\"season\").reset_index().rename(columns={\"season\":\"group\"})\n",
    "    frames += [curr_overall.assign(level=\"overall\", group=\"all\"), curr_season]\n",
    "\n",
    "stats_df = pd.concat(frames, ignore_index=True)\n",
    "stats_df = stats_df[[\"kind\", \"level\", \"group\", \"mean_speed_mps\", \"mean_dir_deg\", \"n\"]]\n",
    "stats_df.to_csv(\"fig/IA4_mean_wind_current_stats.csv\", index=False, float_format=\"%.3f\")\n",
    "\n",
    "# Print seasonal values for wind and current\n",
    "\n",
    "season_order = CategoricalDtype([\"DJF\", \"MAM\", \"JJA\", \"SON\"], ordered=True)\n",
    "\n",
    "def print_seasonals(kind_label, dir_note):\n",
    "    df = stats_df.query(\"kind == @kind_label and level == 'season'\").copy()\n",
    "    df[\"group\"] = df[\"group\"].astype(season_order)\n",
    "    df = df.sort_values(\"group\")\n",
    "    print(f\"\\nIA.4 — Seasonal means, {kind_label} ({dir_note})\")\n",
    "    print(df[[\"group\", \"mean_speed_mps\", \"mean_dir_deg\", \"n\"]]\n",
    "          .rename(columns={\"group\": \"season\"})\n",
    "          .to_string(index=False,\n",
    "                     formatters={\n",
    "                         \"mean_speed_mps\": lambda v: f\"{v:.3f}\",\n",
    "                         \"mean_dir_deg\":  lambda v: f\"{v:.1f}\",\n",
    "                         \"n\":             lambda v: f\"{int(v)}\"\n",
    "                     }))\n",
    "\n",
    "print_seasonals(\"wind\", \"coming-from\")\n",
    "print_seasonals(\"current\", \"going-to\")\n",
    "\n",
    "print(\"IA.4 — Mean conditions (1994–2020)\")\n",
    "print(stats_df.query(\"level == 'overall'\"))\n",
    "print(\"\\nSaved figures:\")\n",
    "print(\"  fig/IA4_wind_rose.png\")\n",
    "if cspd is not None:\n",
    "    print(\"  fig/IA4_current_rose.png\")\n",
    "print(\"  fig/IA4_wind_rose_seasons.png\")\n",
    "if cspd is not None:\n",
    "    print(\"  fig/IA4_current_rose_seasons.png\")\n",
    "print(\"Saved table: fig/IA4_mean_wind_current_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IA.4 Monthly climatology: wind & current (mean ± IQR; direction circular mean ± std) ---\n",
    "need_wind = {\"wspd\", \"wdir\"}\n",
    "curr_need = {\"cspd\", \"cdir\"}\n",
    "missing = need_wind - set(data.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"IA.4 monthly climatology missing columns {sorted(missing)} in `data`.\")\n",
    "\n",
    "has_currents = curr_need.issubset(data.columns)\n",
    "cols = [\"wspd\", \"wdir\"] + ([\"cspd\", \"cdir\"] if has_currents else [])\n",
    "df_wc = data.loc[start:end, cols].copy()\n",
    "if not isinstance(df_wc.index, pd.DatetimeIndex):\n",
    "    raise TypeError(\"`data` index must be a DatetimeIndex.\")\n",
    "\n",
    "months = pd.Index(range(1, 13), name=\"month\")\n",
    "month_group = df_wc.groupby(df_wc.index.month)\n",
    "\n",
    "wspd_mean = month_group[\"wspd\"].mean().reindex(months)\n",
    "wspd_q25 = month_group[\"wspd\"].quantile(0.25).reindex(months)\n",
    "wspd_q75 = month_group[\"wspd\"].quantile(0.75).reindex(months)\n",
    "\n",
    "if has_currents:\n",
    "    cspd_mean = month_group[\"cspd\"].mean().reindex(months)\n",
    "    cspd_q25 = month_group[\"cspd\"].quantile(0.25).reindex(months)\n",
    "    cspd_q75 = month_group[\"cspd\"].quantile(0.75).reindex(months)\n",
    "\n",
    "def _direction_series(series):\n",
    "    df = series.to_frame(name=\"theta\")\n",
    "    mask = df[\"theta\"].notna()\n",
    "    df.loc[mask, \"sin\"] = np.sin(np.deg2rad(df.loc[mask, \"theta\"]))\n",
    "    df.loc[mask, \"cos\"] = np.cos(np.deg2rad(df.loc[mask, \"theta\"]))\n",
    "    group = df.groupby(df.index.month)\n",
    "    sin_mean = group[\"sin\"].mean()\n",
    "    cos_mean = group[\"cos\"].mean()\n",
    "    counts = group[\"theta\"].count()\n",
    "    R = np.hypot(sin_mean, cos_mean)\n",
    "    mean = (np.degrees(np.arctan2(sin_mean, cos_mean)) + 360.0) % 360.0\n",
    "    cstd = np.degrees(np.sqrt(np.maximum(0.0, -2.0 * np.log(np.clip(R, 1e-12, 1.0)))))\n",
    "    mean = mean.where(counts > 0)\n",
    "    cstd = cstd.where(counts > 0)\n",
    "    R = R.where(counts > 0)\n",
    "    return mean, cstd, R, counts\n",
    "\n",
    "wdir_mean, wdir_cstd, wdir_R, wdir_n = _direction_series(df_wc[\"wdir\"])\n",
    "wdir_mean = wdir_mean.reindex(months)\n",
    "wdir_cstd = wdir_cstd.reindex(months)\n",
    "wdir_R = wdir_R.reindex(months)\n",
    "wdir_n = wdir_n.reindex(months)\n",
    "wdir_raw = df_wc[\"wdir\"].dropna()\n",
    "if wdir_raw.empty:\n",
    "    wind_overall_dir = np.nan\n",
    "    wdir_mean_unwrapped = wdir_mean.copy()\n",
    "else:\n",
    "    sin_all = np.sin(np.deg2rad(wdir_raw)).mean()\n",
    "    cos_all = np.cos(np.deg2rad(wdir_raw)).mean()\n",
    "    wind_overall_dir = (np.degrees(np.arctan2(sin_all, cos_all)) + 360.0) % 360.0\n",
    "    wdir_mean_unwrapped = wind_overall_dir + ((wdir_mean - wind_overall_dir + 540.0) % 360.0 - 180.0)\n",
    "\n",
    "if has_currents:\n",
    "    cdir_mean, cdir_cstd, cdir_R, cdir_n = _direction_series(df_wc[\"cdir\"])\n",
    "    cdir_mean = cdir_mean.reindex(months)\n",
    "    cdir_cstd = cdir_cstd.reindex(months)\n",
    "    cdir_R = cdir_R.reindex(months)\n",
    "    cdir_n = cdir_n.reindex(months)\n",
    "    cdir_raw = df_wc[\"cdir\"].dropna()\n",
    "    if cdir_raw.empty:\n",
    "        curr_overall_dir = np.nan\n",
    "        cdir_mean_unwrapped = cdir_mean.copy()\n",
    "    else:\n",
    "        sin_all = np.sin(np.deg2rad(cdir_raw)).mean()\n",
    "        cos_all = np.cos(np.deg2rad(cdir_raw)).mean()\n",
    "        curr_overall_dir = (np.degrees(np.arctan2(sin_all, cos_all)) + 360.0) % 360.0\n",
    "        cdir_mean_unwrapped = curr_overall_dir + ((cdir_mean - curr_overall_dir + 540.0) % 360.0 - 180.0)\n",
    "\n",
    "clim = pd.DataFrame(index=months)\n",
    "clim[\"month_name\"] = pd.to_datetime(clim.index, format=\"%m\").month_name().str.slice(0, 3)\n",
    "clim[\"wspd_mean\"] = wspd_mean\n",
    "clim[\"wspd_q25\"] = wspd_q25\n",
    "clim[\"wspd_q75\"] = wspd_q75\n",
    "clim[\"wdir_mean\"] = wdir_mean\n",
    "clim[\"wdir_mean_unwrapped\"] = wdir_mean_unwrapped\n",
    "clim[\"wdir_cstd\"] = wdir_cstd\n",
    "clim[\"wdir_R\"] = wdir_R\n",
    "clim[\"wdir_n\"] = wdir_n\n",
    "\n",
    "if has_currents:\n",
    "    clim[\"cspd_mean\"] = cspd_mean\n",
    "    clim[\"cspd_q25\"] = cspd_q25\n",
    "    clim[\"cspd_q75\"] = cspd_q75\n",
    "    clim[\"cdir_mean\"] = cdir_mean\n",
    "    clim[\"cdir_mean_unwrapped\"] = cdir_mean_unwrapped\n",
    "    clim[\"cdir_cstd\"] = cdir_cstd\n",
    "    clim[\"cdir_R\"] = cdir_R\n",
    "    clim[\"cdir_n\"] = cdir_n\n",
    "\n",
    "clim.to_csv(\"fig/IA4_wind_current_monthly_climatology.csv\", float_format=\"%.3f\")\n",
    "\n",
    "x = np.arange(1, 13)\n",
    "labels = clim[\"month_name\"].tolist()\n",
    "n_rows = 4 if has_currents else 2\n",
    "fig, axes = plt.subplots(n_rows, 1, figsize=(12, 3 * n_rows), sharex=True)\n",
    "axes = np.atleast_1d(axes)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(x, clim[\"wspd_mean\"], lw=1.8)\n",
    "ax.fill_between(x, clim[\"wspd_q25\"], clim[\"wspd_q75\"], alpha=0.25)\n",
    "ax.set_ylabel(\"Wind speed (m/s)\")\n",
    "ax.set_title(\"Monthly climatology: wind speed (mean ± IQR)\")\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "y = clim[\"wdir_mean_unwrapped\"].to_numpy()\n",
    "yerr = clim[\"wdir_cstd\"].to_numpy()\n",
    "ax.errorbar(x, y, yerr=yerr, fmt=\"-o\", capsize=3, lw=1.5)\n",
    "if np.isfinite(y).any():\n",
    "    lower = y - np.nan_to_num(yerr, nan=0.0)\n",
    "    upper = y + np.nan_to_num(yerr, nan=0.0)\n",
    "    finite = np.isfinite(lower) & np.isfinite(upper)\n",
    "    if finite.any():\n",
    "        pad = 5.0\n",
    "        ax.set_ylim(lower[finite].min() - pad, upper[finite].max() + pad)\n",
    "ax.set_ylabel(\"Wind direction (deg)\")\n",
    "ax.set_title(\"Monthly climatology: wind direction (circular mean ± std)\")\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "if has_currents:\n",
    "    ax = axes[2]\n",
    "    ax.plot(x, clim[\"cspd_mean\"], lw=1.8)\n",
    "    ax.fill_between(x, clim[\"cspd_q25\"], clim[\"cspd_q75\"], alpha=0.25)\n",
    "    ax.set_ylabel(\"Current speed (m/s)\")\n",
    "    ax.set_title(\"Monthly climatology: current speed (mean ± IQR)\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    ax = axes[3]\n",
    "    y = clim[\"cdir_mean_unwrapped\"].to_numpy()\n",
    "    yerr = clim[\"cdir_cstd\"].to_numpy()\n",
    "    ax.errorbar(x, y, yerr=yerr, fmt=\"-o\", capsize=3, lw=1.5)\n",
    "    if np.isfinite(y).any():\n",
    "        lower = y - np.nan_to_num(yerr, nan=0.0)\n",
    "        upper = y + np.nan_to_num(yerr, nan=0.0)\n",
    "        finite = np.isfinite(lower) & np.isfinite(upper)\n",
    "        if finite.any():\n",
    "            pad = 5.0\n",
    "            ax.set_ylim(lower[finite].min() - pad, upper[finite].max() + pad)\n",
    "    ax.set_ylabel(\"Current direction (deg)\")\n",
    "    ax.set_title(\"Monthly climatology: current direction (circular mean ± std)\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "axes[-1].set_xticks(x)\n",
    "axes[-1].set_xticklabels(labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fig/IA4_wind_current_monthly_climatology.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"IA.4 — Monthly wind/current climatology saved to fig/IA4_wind_current_monthly_climatology.(csv|png)\")\n",
    "if has_currents:\n",
    "    print(\"  Included both wind and current statistics.\")\n",
    "else:\n",
    "    print(\"  Current data unavailable; plotted wind only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "**Q: What are the mean conditions (e.g. velocity and direction)? (if you have time: do you observe any seasonal variability?)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### IA.4  Interpretation of wind and current conditions\n",
    "\n",
    "\n",
    "**Answer.** Winds are W-WNW year-round and strongest in winter (overall mean 7.22 m/s; DJF mean 8.67 m/s). Currents are weak (about 0.10 m/s) and dominantly tidal along a NE-SW axis, so the overall mean direction is not physically representative.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Data basis and conventions\n",
    "\n",
    "\n",
    "- Period analysed: 1994-2020.  \n",
    "- Calms excluded in roses: wind $<0.5$ m/s, current $<0.05$ m/s.  \n",
    "- Wind: **coming-from**; Current: **going-to**; $0^\\circ=\\text{N}$, clockwise.  \n",
    "- Bins used: wind speeds $[0.5,2),[2,4),\\ldots,\\ge 16$ m/s; current speeds $[0.05,0.10),[0.10,0.15),[0.15,0.20),\\ge 0.20$ m/s; wind sectors $22.5^\\circ$; current sectors $10^\\circ$.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Summary statistics\n",
    "\n",
    "\n",
    "**Overall means (1994-2020)**\n",
    "\n",
    "\n",
    "| Variable | Mean speed | Mean direction |\n",
    "|---|---:|---:|\n",
    "| Wind (coming-from) | 7.22 m/s | 296.6 |\n",
    "| Current (going-to) | 0.096 m/s | 285.2* |\n",
    "\n",
    "\n",
    "\\*For reversing tidal currents, a single circular mean direction is not physically informative.\n",
    "\n",
    "\n",
    "**Seasonal means**\n",
    "\n",
    "\n",
    "| Season | Wind speed (m/s) | Wind dir (deg) | Current speed (m/s) | Current dir (deg)* |\n",
    "|---|---:|---:|---:|---:|\n",
    "| DJF | 8.67 | 266.3 | 0.096 | 211.3 |\n",
    "| MAM | 6.95 | 321.1 | 0.096 | 28.3 |\n",
    "| JJA | 6.00 | 302.6 | 0.096 | 228.5 |\n",
    "| SON | 7.31 | 287.5 | 0.096 | 19.0 |\n",
    "\n",
    "\n",
    "\\*Seasonal current means are shown for completeness but the roses indicate a bidirectional tidal regime.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Wind climate: what the roses show\n",
    "\n",
    "\n",
    "- **Directionality.** Dominant **W-WNW** approach. Seasonal veer is modest: closer to **W** in winter (DJF $\\sim266^\\circ$), rotating towards **WNW** in spring and summer (MAM-JJA $\\sim321^\\circ$ to $\\sim303^\\circ$), then easing back in autumn (SON $\\sim288^\\circ$).\n",
    "- **Intensity.** Clear seasonal cycle: **DJF > SON > MAM > JJA** by mean speed. The full-period rose shows most occurrences in the **4-12 m/s** bands, with winter contributing the higher **8-14 m/s** fractions.\n",
    "- **Variability.** Sectoral spread remains broader in MAM/SON than in DJF/JJA, consistent with synoptic variability during shoulder seasons.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Current climate: what the roses show\n",
    "\n",
    "\n",
    "- **Regime.** Two narrow, opposing **going-to** headings dominate, approximately **NE (30^\\circ)** and **SW (210^\\circ)**, confirming a reversing tidal signal.\n",
    "- **Magnitudes.** Most occurrences lie **below 0.30 m/s**; the highest occupied bin is typically **0.15-0.20 m/s**, with scarce excursions beyond. The mean speed of **0.096 m/s** reflects the prevalence of weak flows.\n",
    "- **Seasonality.** The **axis does not shift** with season, and the speed distribution changes little between DJF and JJA in the roses, highlighting tide-dominated currents with weak seasonal modulation.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Site context notes\n",
    "\n",
    "\n",
    "- The study area is **south of Lorient (southern Brittany)**, exposed to the **open North Atlantic**. The prevailing **W-WNW** winds align with the basin-scale westerlies and frequent winter cyclones.  \n",
    "- The **NE-SW** tidal current axis is consistent with coastal geometry that channels reversing flows; without detailed bathymetry/harmonic analysis, treat this as a qualitative inference from the roses.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Caveats and suggested refinements\n",
    "\n",
    "\n",
    "- **Currents:** Do not use circular means for direction. Prefer a **principal axis** metric and report the **two modal headings** with their shares.  \n",
    "- **Quantiles:** Add $P_{50}$, $P_{90}$, $P_{95}$ for wind and current speeds to complement the means (e.g., $P_{90}$ wind speed $\\approx$ x m/s).  \n",
    "- **Calms:** Roses exclude calms by design; if calm frequency is relevant to operations, report the calm fraction separately.  \n",
    "- **Next step:** If needed for design or logistics, compute a tidal ellipse or principal-component axis for the currents, and provide **hour-of-tide** roses to expose phase dependence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### I.A.5. Comparison to Wave Buoy Measurements\n",
    "\n",
    "We can validate the ResourceCode hindcast data against observations from a nearby wave buoy, for example, from the Candhis website.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "**Q: Comparing the hindcast data from ResourceCode to the observations during the time period with overlapping data, what is the RMSD in the wave height, period, and direction between the observations and simulations?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IA.5 — Candhis buoy validation (Belle-Île 05602, year 2020) ---\n",
    "# Goal: RMSD between ResourceCode hindcast and Candhis observations for Hs, Tm02, and mean direction.\n",
    "# Notes:\n",
    "# - Candhis direction: use THETAM (mean, coming-from, true north, clockwise).\n",
    "# - ResourceCode wave direction assumed coming-from in `data['dir']`. If not, set RC_IS_COMING_FROM=False.\n",
    "# - Time matching: nearest within ±30 min.\n",
    "# - QC: drop sentinels (999.*), non-numeric, |SKEW|>0.3, KURT>5, and (if present) non-valid QUALITE.\n",
    "\n",
    "# ---------- Config ----------\n",
    "RC_IS_COMING_FROM = True          # set to False if your ResourceCode 'dir' is going-to\n",
    "TOL = pd.Timedelta(\"30min\")       # matching tolerance\n",
    "CSV_PATHS = [\n",
    "    Path(\"data/Candhis_05602_2020_arch.csv\"),     # preferred relative path\n",
    "    Path(\"/mnt/data/Candhis_05602_2020_arch.csv\") # fallback (chat attachment)\n",
    "]\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "def _first_existing(paths):\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Candhis 2020 CSV not found at any known path.\")\n",
    "\n",
    "def circ_diff_deg(a_deg, b_deg):\n",
    "    \"\"\"Minimal angular difference a-b in degrees in [-180, 180).\"\"\"\n",
    "    d = (a_deg - b_deg + 180.0) % 360.0 - 180.0\n",
    "    return d\n",
    "\n",
    "def rmse(a, b):\n",
    "    a = np.asarray(a, float)\n",
    "    b = np.asarray(b, float)\n",
    "    m = np.isfinite(a) & np.isfinite(b)\n",
    "    if m.sum() == 0:\n",
    "        return np.nan, 0\n",
    "    return float(np.sqrt(np.mean((a[m] - b[m])**2))), int(m.sum())\n",
    "\n",
    "def circ_rmse_deg(a_deg, b_deg):\n",
    "    \"\"\"RMS of circular differences in degrees.\"\"\"\n",
    "    d = circ_diff_deg(np.asarray(a_deg, float), np.asarray(b_deg, float))\n",
    "    m = np.isfinite(d)\n",
    "    if m.sum() == 0:\n",
    "        return np.nan, 0\n",
    "    return float(np.sqrt(np.mean(d[m]**2))), int(m.sum())\n",
    "\n",
    "# ---------- Load Candhis 2020 ----------\n",
    "candhis_csv = _first_existing(CSV_PATHS)\n",
    "\n",
    "cand = pd.read_csv(\n",
    "    candhis_csv,\n",
    "    sep=\";\", engine=\"python\",\n",
    "    parse_dates=[\"DateHeure\"]\n",
    ")\n",
    "\n",
    "# Keep only fields needed + QC fields\n",
    "need_cols = [\"DateHeure\", \"HM0\", \"T02\", \"THETAM\", \"SKEW\", \"KURT\", \"QUALITE\"]\n",
    "for c in need_cols:\n",
    "    if c not in cand.columns:\n",
    "        # QUALITE may be absent; others must exist\n",
    "        if c == \"QUALITE\":\n",
    "            cand[\"QUALITE\"] = np.nan\n",
    "        else:\n",
    "            raise KeyError(f\"Missing '{c}' in Candhis CSV.\")\n",
    "\n",
    "# Convert to numeric and mark sentinels (e.g., 999.*) as NaN\n",
    "num_cols = [\"HM0\", \"T02\", \"THETAM\", \"SKEW\", \"KURT\"]\n",
    "for c in num_cols:\n",
    "    cand[c] = pd.to_numeric(cand[c], errors=\"coerce\")\n",
    "    # Drop obvious sentinels like 999, 999.999, etc.\n",
    "    cand.loc[cand[c] >= 999.0, c] = np.nan\n",
    "\n",
    "# QC filters\n",
    "qc = pd.Series(True, index=cand.index)\n",
    "\n",
    "# 1) Required numeric fields present\n",
    "qc &= cand[\"HM0\"].notna() & cand[\"T02\"].notna() & cand[\"THETAM\"].notna()\n",
    "\n",
    "# 2) Physical sanity\n",
    "qc &= (cand[\"HM0\"] > 0) & (cand[\"T02\"] > 0)\n",
    "\n",
    "# 3) Distribution checks\n",
    "#    Drop timestamps with |SKEW| > 0.3 or KURT > 5 (when available)\n",
    "skew_ok = cand[\"SKEW\"].abs() <= 0.3\n",
    "kurt_ok = cand[\"KURT\"] <= 5.0\n",
    "# If SKEW/KURT missing at a row, do not auto-drop for that reason\n",
    "skew_ok = skew_ok | cand[\"SKEW\"].isna()\n",
    "kurt_ok = kurt_ok | cand[\"KURT\"].isna()\n",
    "qc &= skew_ok & kurt_ok\n",
    "\n",
    "# 4) QUALITE (if present): keep only rows with non-empty labels considered valid\n",
    "#    Many archive files have QUALITE empty; when populated, typical \"good\" tags include letters.\n",
    "if cand[\"QUALITE\"].notna().any():\n",
    "    # Keep non-empty, non-null strings that are not \"M\", \"NA\", or \"ERR\"\n",
    "    q = cand[\"QUALITE\"].astype(str).str.strip().str.upper()\n",
    "    good = ~(q.isna() | (q == \"\") | q.isin({\"M\", \"NA\", \"ERR\"}))\n",
    "    qc &= good\n",
    "\n",
    "cand_qc = cand.loc[qc, [\"DateHeure\", \"HM0\", \"T02\", \"THETAM\"]].copy()\n",
    "cand_qc = cand_qc.rename(columns={\"DateHeure\": \"time\", \"HM0\": \"hs_obs\", \"T02\": \"Tm02_obs\", \"THETAM\": \"dir_obs_deg\"})\n",
    "cand_qc[\"time\"] = pd.to_datetime(cand_qc[\"time\"], utc=True)  # TU = UTC\n",
    "cand_qc = cand_qc.set_index(\"time\").sort_index()\n",
    "\n",
    "# Restrict strictly to 2020 UTC\n",
    "start_utc = pd.Timestamp(\"2020-01-01 00:00:00\", tz=\"UTC\")\n",
    "end_utc   = pd.Timestamp(\"2020-12-31 23:59:59\", tz=\"UTC\")\n",
    "cand_qc = cand_qc.loc[start_utc:end_utc]\n",
    "\n",
    "print(f\"Candhis 2020 after QC: {len(cand_qc)} rows\")\n",
    "\n",
    "# ---------- Prepare ResourceCode subset for 2020 ----------\n",
    "# Assumes your earlier cells defined `data` with columns: 'hs', 'Tm02', 'dir' and a DatetimeIndex in UTC or naive-UTC.\n",
    "if not {\"hs\", \"Tm02\", \"dir\"}.issubset(data.columns):\n",
    "    raise KeyError(\"ResourceCode `data` must include ['hs','Tm02','dir'].\")\n",
    "\n",
    "rc = data[[\"hs\", \"Tm02\", \"dir\"]].copy()\n",
    "\n",
    "# Ensure datetime is tz-aware UTC to compare with Candhis TU\n",
    "if rc.index.tz is None:\n",
    "    rc.index = rc.index.tz_localize(\"UTC\")\n",
    "\n",
    "rc_2020 = rc.loc[start_utc:end_utc].copy()\n",
    "rc_2020 = rc_2020.sort_index()\n",
    "\n",
    "# Direction convention alignment\n",
    "if RC_IS_COMING_FROM:\n",
    "    rc_2020[\"dir_from_deg\"] = rc_2020[\"dir\"] % 360.0\n",
    "else:\n",
    "    # Convert going-to -> coming-from to match THETAM\n",
    "    rc_2020[\"dir_from_deg\"] = (rc_2020[\"dir\"] + 180.0) % 360.0\n",
    "\n",
    "rc_2020 = rc_2020.rename(columns={\"hs\": \"hs_mod\", \"Tm02\": \"Tm02_mod\"})\n",
    "\n",
    "# Time matching (nearest within ±30 min)\n",
    "left  = cand_qc.reset_index().rename(columns={\"time\": \"t_obs\"})\n",
    "right = rc_2020.reset_index().rename(columns={\"index\": \"t_mod\"})\n",
    "\n",
    "pairs = pd.merge_asof(\n",
    "    left.sort_values(\"t_obs\"),\n",
    "    right.sort_values(\"t_mod\"),\n",
    "    left_on=\"t_obs\",\n",
    "    right_on=\"t_mod\",\n",
    "    tolerance=TOL,\n",
    "    direction=\"nearest\",\n",
    ")\n",
    "\n",
    "# Drop non-matches (NaN where no model within tolerance)\n",
    "pairs = pairs.dropna(subset=[\"hs_mod\", \"Tm02_mod\", \"dir_from_deg\"])\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "hs_rmse, n_hs = rmse(pairs[\"hs_mod\"], pairs[\"hs_obs\"])\n",
    "t02_rmse, n_t = rmse(pairs[\"Tm02_mod\"], pairs[\"Tm02_obs\"])\n",
    "dir_rmse, n_d = circ_rmse_deg(pairs[\"dir_from_deg\"], pairs[\"dir_obs_deg\"])\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"metric\": [\"RMSD_Hs (m)\", \"RMSD_Tm02 (s)\", \"RMSD_Dir_THETAM (deg)\"],\n",
    "    \"value\": [hs_rmse, t02_rmse, dir_rmse],\n",
    "    \"N_pairs\": [n_hs, n_t, n_d]\n",
    "})\n",
    "\n",
    "print(\"\\nIA.5 — Candhis vs ResourceCode (2020, Belle-Île 05602)\")\n",
    "print(summary.to_string(index=False, float_format=lambda v: f\"{v:.3f}\"))\n",
    "\n",
    "# Save outputs\n",
    "out_dir = Path(\"fig\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "summary.to_csv(out_dir / \"IA5_rmsd_2020_candhis05602.csv\", index=False)\n",
    "\n",
    "# Optional quick-look plots\n",
    "fig, ax = plt.subplots(3, 1, figsize=(12, 9), sharex=True)\n",
    "ax[0].plot(pairs[\"t_obs\"], pairs[\"hs_obs\"], label=\"Buoy HM0\", lw=0.9)\n",
    "ax[0].plot(pairs[\"t_obs\"], pairs[\"hs_mod\"], label=\"Model Hs\", lw=0.9)\n",
    "ax[0].set_ylabel(\"Hs (m)\")\n",
    "ax[0].grid(alpha=0.3)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(pairs[\"t_obs\"], pairs[\"Tm02_obs\"], label=\"Buoy T02\", lw=0.9)\n",
    "ax[1].plot(pairs[\"t_obs\"], pairs[\"Tm02_mod\"], label=\"Model Tm02\", lw=0.9)\n",
    "ax[1].set_ylabel(\"Tm02 (s)\")\n",
    "ax[1].grid(alpha=0.3)\n",
    "ax[1].legend()\n",
    "\n",
    "# Direction as coming-from; plot circularly unwrapped about buoy direction to visualise differences\n",
    "d_err = circ_diff_deg(pairs[\"dir_from_deg\"], pairs[\"dir_obs_deg\"])\n",
    "ax[2].plot(pairs[\"t_obs\"], d_err, lw=0.8)\n",
    "ax[2].axhline(0, color=\"k\", lw=0.8)\n",
    "ax[2].set_ylabel(\"Dir error (deg)\")\n",
    "ax[2].set_xlabel(\"2020 UTC\")\n",
    "ax[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_dir / \"IA5_timeseries_2020.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Scatter diagnostics (optional)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4.5))\n",
    "axes[0].scatter(pairs[\"hs_obs\"], pairs[\"hs_mod\"], s=4, alpha=0.5)\n",
    "mx = np.nanmax([pairs[\"hs_obs\"].max(), pairs[\"hs_mod\"].max()])\n",
    "axes[0].plot([0, mx], [0, mx], 'k:', lw=1)\n",
    "axes[0].set_xlabel(\"Buoy HM0 (m)\")\n",
    "axes[0].set_ylabel(\"Model Hs (m)\")\n",
    "axes[0].set_title(f\"RMSD={hs_rmse:.2f} m\")\n",
    "\n",
    "axes[1].scatter(pairs[\"Tm02_obs\"], pairs[\"Tm02_mod\"], s=4, alpha=0.5)\n",
    "mx = np.nanmax([pairs[\"Tm02_obs\"].max(), pairs[\"Tm02_mod\"].max()])\n",
    "axes[1].plot([0, mx], [0, mx], 'k:', lw=1)\n",
    "axes[1].set_xlabel(\"Buoy T02 (s)\")\n",
    "axes[1].set_ylabel(\"Model Tm02 (s)\")\n",
    "axes[1].set_title(f\"RMSD={t02_rmse:.2f} s\")\n",
    "\n",
    "axes[2].hist(d_err, bins=72)\n",
    "axes[2].set_xlabel(\"Dir error (deg, THETAM)\")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "axes[2].set_title(f\"circ-RMSD={dir_rmse:.1f}°\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_dir / \"IA5_scatter_2020.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### IA.5 — Buoy–model comparison (Belle-Île 05602 vs ResourceCode), year 2020\n",
    "\n",
    "**RMSD results** (±30 min pairing; QC applied; direction uses Candhis **THETAM**, coming-from):\n",
    "- $H_s$: **0.39 m**  \n",
    "- $T_{m02}$: **0.84 s**  \n",
    "- Mean direction: **21.2°** (circular RMSD)  \n",
    "Pairs used: **15 947** half-hours.\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "**Wave height.** Agreement is good over the 0–8 m range. Most error accrues during storms, as expected when model and buoy are not co-located.\n",
    "\n",
    "**Period.** Consistent overall. The scatter indicates a mild mismatch at the longest periods.\n",
    "\n",
    "**Direction.** A 21° circular RMSD is reasonable near a coast with mixed sea–swell and turning fronts. Spikes coincide with weak seas or multi-modal spectra where mean direction is less stable.\n",
    "\n",
    "---\n",
    "\n",
    "#### Spatial representativeness and depth effects\n",
    "\n",
    "- **Positions.** Buoy: $(47.3111^\\circ,\\,-3.3111^\\circ)$ at **45 m** depth.  \n",
    "  Model point: $(47.3236^\\circ,\\,-3.5522^\\circ)$ at **93.32 m** depth.  \n",
    "  Horizontal offset ≈ **18 km**.\n",
    "\n",
    "- **Depth regime difference.** Using the deep-water proxy $T_\\text{deep}\\approx 4\\sqrt{h/g}$:\n",
    "  - At **45 m**: $T_\\text{deep}\\approx 8.6$ s. Longer swell can enter transitional depth, with refraction and depth-induced transformation before reaching the buoy.\n",
    "  - At **93.32 m**: $T_\\text{deep}\\approx 12.3$ s. The offshore model point remains deep for a larger share of the spectrum.\n",
    "\n",
    "- **Implications.** Between 93 m and 45 m the wave field rotates and re-distributes energy. This raises direction error and can alter spectral shape enough to shift $T_{m02}$ slightly. Height differences during energetic events also reflect unmodelled small-scale sheltering by Belle-Île and local bathymetry not captured by the point-to-point comparison.\n",
    "\n",
    "---\n",
    "\n",
    "#### February–March 2020 gap\n",
    "\n",
    "The ~**1-month** hole is from the **observations**, not the model. After QC (sentinels removed; $|SKEW|\\le 0.3$; KURT $\\le 5$; QUALITE filter when present), many half-hours in late winter drop out. Typical causes are buoy maintenance, telemetry loss, or spectra failing distribution checks. The line plots bridge over missing points; no data were used there.\n",
    "\n",
    "---\n",
    "\n",
    "#### Answer to the study question\n",
    "\n",
    "For the 2020 overlap, the RMSD between ResourceCode and the Candhis Belle-Île buoy is **0.39 m** for $H_s$, **0.84 s** for $T_{m02}$, and **21.2°** for mean direction (THETAM, circular RMSD). Given the **18 km** separation and **93 m vs 45 m** depths, these errors are consistent with expected coastal transformation between the offshore model point and the nearer-shore buoy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## I.B. Estimating the extreme wave conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Block Maxima (BM) Method\n",
    "\n",
    "**Approach:**  \n",
    "This approach involves dividing the long-term time series of significant wave heights into non-overlapping blocks of equal duration, typically one year.\n",
    "\n",
    "**Process:**  \n",
    "The single highest significant wave height (Hₘ₀) is taken from each block (e.g., the annual maximum).\n",
    "\n",
    "**Distribution:**  \n",
    "These maximum values are then fitted to a *Generalized Extreme Value (GEV)* distribution.\n",
    "\n",
    "**Outcome:**  \n",
    "This model allows you to estimate the wave height corresponding to a specific return period, such as the 1-year or 50-year storm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample the data into 1 year blocks and extract the maximum value from each block by using pyextremes\n",
    "model = EVA(data.hs)\n",
    "model.get_extremes(\n",
    "    method=\"BM\",\n",
    "    extremes_type=\"high\",\n",
    "    block_size=\"365.2425D\",\n",
    "    errors=\"raise\",\n",
    "    min_last_block=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Lets check the directions of the extreme heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets find the directions of the extremes from the data\n",
    "extremes_indices = model.extremes.index\n",
    "data_extreme_directions = data.loc[model.extremes.index, \"dir\"]\n",
    "data_extreme_tm02 = data.loc[model.extremes.index, \"Tm02\"]\n",
    "data_extreme_wspd = data.loc[model.extremes.index, \"wspd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "extremes_df = (\n",
    "    pd.DataFrame({'timestamp': extremes_indices, 'hs': model.extremes.values})\n",
    "    .merge(\n",
    "        data[['dir','wspd','Tm02']].reset_index().rename(columns={'index':'timestamp'}),\n",
    "        on='timestamp',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "extremes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extremes\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Time series with extremes highlighted\n",
    "ax1.plot(data.index, data.hs, 'b-', alpha=0.3, linewidth=0.5, label='Full time series')\n",
    "ax1.scatter(model.extremes.index, model.extremes.values, color='red', s=20, alpha=0.8, label='Extremes (Block Maxima)')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Significant Wave Height (m)')\n",
    "ax1.set_title('Time Series with One-year Block Extremes')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Histogram of extremes\n",
    "ax2.hist(model.extremes.values, bins=15, alpha=0.7, color='red', edgecolor='black')\n",
    "ax2.set_xlabel('Extreme Wave Height (m)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Distribution of Block Maxima Extremes')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics about the extremes\n",
    "print(f\"Number of extremes: {len(model.extremes)}\")\n",
    "print(f\"Mean extreme value: {model.extremes.mean():.3f} m\")\n",
    "print(f\"Maximum extreme value: {model.extremes.max():.3f} m\")\n",
    "print(f\"Minimum extreme value: {model.extremes.min():.3f} m\")\n",
    "print(f\"Standard deviation: {model.extremes.std():.3f} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More detailed manual plotting of extremes\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "\n",
    "# Plot the full time series\n",
    "ax.plot(data.index, data.hs, 'b-', alpha=0.2, linewidth=0.3, label='Full time series')\n",
    "\n",
    "# Highlight extremes with different colors based on magnitude\n",
    "extremes = model.extremes\n",
    "colors = plt.cm.Reds(np.linspace(0.3, 1, len(extremes)))\n",
    "scatter = ax.scatter(extremes.index, extremes.values, c=extremes.values, \n",
    "                    cmap='Reds', s=30, alpha=0.8, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Wave Height (m)')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Significant Wave Height (m)', fontsize=12)\n",
    "ax.set_title('Block Maxima Extremes Over Time\\n(Red dots show annual maximum wave heights)', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets execute GEV fit on the extremes\n",
    "model.fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate return values for 1-year and 50-year periods\n",
    "T_years = [1, 50]\n",
    "\n",
    "summary = model.get_summary(T_years, return_period_size=\"365.2425D\", alpha=0.95)\n",
    "#print(f\"1-year return value: {return_values[0]:.2f} m\")\n",
    "#print(f\"50-year return value: {return_values[1]:.2f} m\")\n",
    "\n",
    "# Plot the return value plot\n",
    "model.plot_return_values()\n",
    "\n",
    "\n",
    "# Create a focused plot for 50-year return period with confidence intervals\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Get the 50-year return value and confidence intervals\n",
    "return_50 = summary.loc[50, 'return value']\n",
    "lower_50 = summary.loc[50, 'lower ci']\n",
    "upper_50 = summary.loc[50, 'upper ci']\n",
    "\n",
    "# Create a bar plot for the 50-year return period\n",
    "bars = ax.bar(['50-year Return Period'], [return_50], \n",
    "              color='steelblue', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add error bars for confidence intervals\n",
    "ax.errorbar(['50-year Return Period'], [return_50], \n",
    "           yerr=[[return_50 - lower_50], [upper_50 - return_50]], \n",
    "           fmt='none', color='red', capsize=10, capthick=2, linewidth=2)\n",
    "\n",
    "# Add text annotations\n",
    "ax.text(0, return_50 + 0.1, f'{return_50:.2f} m', ha='center', va='bottom', \n",
    "        fontsize=12, fontweight='bold')\n",
    "ax.text(0, upper_50 + 0.1, f'Upper 90% CI: {upper_50:.2f} m', ha='center', va='bottom', \n",
    "        fontsize=10, color='red')\n",
    "ax.text(0, lower_50 - 0.2, f'Lower 90% CI: {lower_50:.2f} m', ha='center', va='top', \n",
    "        fontsize=10, color='red')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_ylabel('Significant Wave Height (m)', fontsize=12)\n",
    "ax.set_title('50-Year Return Period with 90% Confidence Intervals\\nBlock Maxima Method (GEV Distribution)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, upper_50 + 0.5)\n",
    "\n",
    "# Add a horizontal line for reference\n",
    "ax.axhline(y=return_50, color='red', linestyle='--', alpha=0.7, linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/50_year_return_period_confidence.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"50-year return period: {return_50:.3f} m\")\n",
    "print(f\"90% Confidence Interval: [{lower_50:.3f}, {upper_50:.3f}] m\")\n",
    "print(f\"Confidence interval width: {upper_50 - lower_50:.3f} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with correct empirical return periods\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Get correct empirical return periods\n",
    "sorted_extremes = np.sort(model.extremes.values)[::-1]  # Sort descending\n",
    "n_extremes = len(sorted_extremes)\n",
    "\n",
    "# Correct empirical return period calculation\n",
    "# For block maxima, empirical return period = (n + 1) / (rank)\n",
    "empirical_return_periods = (n_extremes + 1) / np.arange(1, n_extremes + 1)\n",
    "\n",
    "# Plot your observed data with correct empirical return periods\n",
    "ax.semilogx(empirical_return_periods, sorted_extremes, 'ko', markersize=4, alpha=0.7, label='Observed Extremes (Empirical)')\n",
    "\n",
    "# Add fitted model for comparison\n",
    "summary = model.get_summary([0, 1, 2, 5, 10, 25, 50, 100], return_period_size=\"365.2425D\", alpha=0.90)\n",
    "ax.semilogx(summary.index, summary['return value'], 'r-', linewidth=2, label='Fitted Model')\n",
    "\n",
    "# Add confidence intervals\n",
    "ax.semilogx(summary.index, summary['upper ci'], 'r--', linewidth=1, alpha=0.7, label='Upper 90% CI')\n",
    "ax.semilogx(summary.index, summary['lower ci'], 'r--', linewidth=1, alpha=0.7, label='Lower 90% CI')\n",
    "\n",
    "# Fill between confidence intervals\n",
    "ax.fill_between(summary.index, summary['lower ci'], summary['upper ci'], alpha=0.2, color='red')\n",
    "\n",
    "# Highlight 50-year point\n",
    "ax.semilogx(50, summary.loc[50, 'return value'], 'ro', markersize=8, label='50-year Return Period')\n",
    "\n",
    "ax.set_xlabel('Return Period (years)')\n",
    "ax.set_ylabel('Significant Wave Height (m)')\n",
    "ax.set_title('Empirical vs Fitted Return Values')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with extended model to 1 year\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Get correct empirical return periods\n",
    "sorted_extremes = np.sort(model.extremes.values)[::-1]  # Sort descending\n",
    "n_extremes = len(sorted_extremes)\n",
    "\n",
    "# Correct empirical return period calculation\n",
    "empirical_return_periods = (n_extremes + 1) / np.arange(1, n_extremes + 1)\n",
    "\n",
    "# Plot your observed data with correct empirical return periods\n",
    "ax.semilogx(empirical_return_periods, sorted_extremes, 'ko', markersize=4, alpha=0.7, label='Observed Extremes (Empirical)')\n",
    "\n",
    "# Add fitted model with extended range to 1 year\n",
    "summary = model.get_summary([1, 1.5, 2, 3, 5, 10, 25, 50, 100], return_period_size=\"365.2425D\", alpha=0.90)\n",
    "ax.semilogx(summary.index, summary['return value'], 'r-', linewidth=2, label='Fitted Model')\n",
    "\n",
    "# Add confidence intervals\n",
    "ax.semilogx(summary.index, summary['upper ci'], 'r--', linewidth=1, alpha=0.7, label='Upper 90% CI')\n",
    "ax.semilogx(summary.index, summary['lower ci'], 'r--', linewidth=1, alpha=0.7, label='Lower 90% CI')\n",
    "\n",
    "# Fill between confidence intervals\n",
    "ax.fill_between(summary.index, summary['lower ci'], summary['upper ci'], alpha=0.2, color='red')\n",
    "\n",
    "# Highlight 50-year point\n",
    "ax.semilogx(50, summary.loc[50, 'return value'], 'ro', markersize=8, label='50-year Return Period')\n",
    "\n",
    "ax.set_xlabel('Return Period (years)')\n",
    "ax.set_ylabel('Significant Wave Height (m)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets save the diagnostic plot to folder graphs in the current directory   \n",
    "model.plot_diagnostic(alpha=0.95)\n",
    "plt.savefig('graphs/diagnostic_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### Modelling univariate time series: Block maxima + GEVD (Generalized Extreme Value Distribution)\n",
    "\n",
    "We show as an example here a **BM** (block maxima) model fitted to the $H_s$ time series. In this approach, the maximum value is identified within a \"block\" or fixed period in time, and then a GEVP distribution is fit to the data to estimate the return values.  \n",
    "\n",
    "The same plot can readily be obtained for the other sea-state parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED VERSION: COMPREHENSIVE SEASONAL ANALYSIS OF EXTREME WAVE HEIGHTS\n",
    "# All-in-one cell for complete seasonality extraction (with DatetimeIndex fix)\n",
    "\n",
    "'''def rayleigh_test(theta_rad):\n",
    "    \"\"\"Rayleigh test for non-uniformity on angles in radians.\n",
    "    Returns (Z, p).\"\"\"\n",
    "    x = np.asarray(theta_rad, dtype=float)\n",
    "    x = x[~np.isnan(x)]\n",
    "    n = x.size\n",
    "    if n < 5:\n",
    "        return np.nan, np.nan\n",
    "    C = np.sum(np.cos(x))\n",
    "    S = np.sum(np.sin(x))\n",
    "    R = np.hypot(C, S)\n",
    "    Rbar = R / n\n",
    "    Z = n * Rbar**2\n",
    "    # p-value with small-sample correction\n",
    "    p = np.exp(-Z) * (1 + (2*Z - Z**2)/(4*n)\n",
    "                      - (24*Z - 132*Z**2 + 76*Z**3 - 9*Z**4)/(288*n**2))\n",
    "    return Z, float(np.clip(p, 0.0, 1.0))\n",
    "\n",
    "# Extract seasonal information from the extremes - FIXED VERSION\n",
    "# Ensure we have a proper DataFrame with DatetimeIndex\n",
    "extremes_df = pd.DataFrame({'hs': model.extremes.values}, index=model.extremes.index)\n",
    "extremes_df.index = pd.to_datetime(extremes_df.index)  # Ensure it's a DatetimeIndex\n",
    "\n",
    "extremes_df['month'] = extremes_df.index.month\n",
    "extremes_df['season'] = extremes_df['month'].map({\n",
    "    12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "    3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "    6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "    9: 'Autumn', 10: 'Autumn', 11: 'Autumn'\n",
    "})\n",
    "extremes_df['year'] = extremes_df.index.year\n",
    "\n",
    "# Calculate monthly statistics\n",
    "monthly_counts = extremes_df['month'].value_counts().sort_index()\n",
    "monthly_means = extremes_df.groupby('month')['hs'].mean()\n",
    "monthly_maxs = extremes_df.groupby('month')['hs'].max()\n",
    "monthly_std = extremes_df.groupby('month')['hs'].std()\n",
    "\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "# Circular statistics for seasonality\n",
    "monthly_angles = (extremes_df['month'] - 1) * 2 * np.pi / 12\n",
    "circular_mean = np.arctan2(np.sin(monthly_angles).sum(), np.cos(monthly_angles).sum())\n",
    "circular_mean_month = (circular_mean * 12 / (2 * np.pi) + 1) % 12\n",
    "if circular_mean_month == 0:\n",
    "    circular_mean_month = 12\n",
    "\n",
    "R = np.sqrt(np.sin(monthly_angles).sum()**2 + np.cos(monthly_angles).sum()**2) / len(monthly_angles)\n",
    "circular_variance = 1 - R\n",
    "\n",
    "# Rayleigh test for uniformity\n",
    "rayleigh_Z, rayleigh_p = rayleigh_test(monthly_angles)\n",
    "\n",
    "# Create comprehensive plots\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "\n",
    "# Plot 1: Monthly distribution with dual y-axis\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "bars = ax1.bar(monthly_counts.index, monthly_counts.values, alpha=0.7, color='skyblue', \n",
    "               edgecolor='black', label='Count of Extremes')\n",
    "ax1.set_xlabel('Month')\n",
    "ax1.set_ylabel('Number of Extremes', color='blue')\n",
    "ax1.set_title('Monthly Distribution of Extreme Wave Heights', fontsize=12, fontweight='bold')\n",
    "ax1.set_xticks(range(1, 13))\n",
    "ax1.set_xticklabels(months)\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Add mean values as line plot\n",
    "ax1_twin = ax1.twinx()\n",
    "ax1_twin.plot(monthly_means.index, monthly_means.values, 'ro-', linewidth=2, markersize=6, \n",
    "              color='red', label='Mean Height')\n",
    "ax1_twin.set_ylabel('Mean Wave Height (m)', color='red')\n",
    "ax1_twin.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Combine legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax1_twin.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "# Plot 2: Seasonal box plots\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "seasonal_data = [extremes_df[extremes_df['season'] == season]['hs'].values \n",
    "                 for season in ['Winter', 'Spring', 'Summer', 'Autumn']]\n",
    "\n",
    "box_plot = ax2.boxplot(seasonal_data, labels=['Winter', 'Spring', 'Summer', 'Autumn'], \n",
    "                       patch_artist=True, showfliers=True)\n",
    "colors = ['lightblue', 'lightgreen', 'orange', 'brown']\n",
    "for patch, color in zip(box_plot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax2.set_ylabel('Wave Height (m)')\n",
    "ax2.set_title('Seasonal Distribution of Extreme Wave Heights', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add statistical annotations\n",
    "for i, season in enumerate(['Winter', 'Spring', 'Summer', 'Autumn']):\n",
    "    season_data = extremes_df[extremes_df['season'] == season]['hs']\n",
    "    if len(season_data) > 0:\n",
    "        ax2.text(i+1, season_data.max() + 0.1, f'n={len(season_data)}', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 3: Time series with seasonal coloring\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "scatter = ax3.scatter(extremes_df.index, extremes_df['hs'], \n",
    "                     c=extremes_df['month'], cmap='tab20', s=50, alpha=0.8, edgecolors='black')\n",
    "ax3.set_xlabel('Year')\n",
    "ax3.set_ylabel('Wave Height (m)')\n",
    "ax3.set_title('Extreme Wave Heights Over Time (Colored by Month)', fontsize=12, fontweight='bold')\n",
    "cbar = plt.colorbar(scatter, ax=ax3)\n",
    "cbar.set_label('Month')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Circular histogram (rose plot)\n",
    "ax4 = plt.subplot(3, 3, 4, projection='polar')\n",
    "monthly_counts_polar = monthly_counts.reindex(range(1, 13), fill_value=0)\n",
    "theta = np.linspace(0, 2*np.pi, 13)[:-1]  # 12 months\n",
    "width = 2*np.pi/12\n",
    "bars = ax4.bar(theta, monthly_counts_polar.values, width=width, alpha=0.7, \n",
    "               color=plt.cm.viridis(monthly_counts_polar.values / monthly_counts_polar.max()))\n",
    "ax4.set_theta_zero_location('N')  # January at top\n",
    "ax4.set_theta_direction(-1)  # Clockwise\n",
    "ax4.set_xticks(theta)\n",
    "ax4.set_xticklabels(months)\n",
    "ax4.set_title('Circular Distribution of Extremes\\n(Rose Plot)', fontsize=12, fontweight='bold', pad=20)\n",
    "\n",
    "# Add circular mean arrow\n",
    "ax4.arrow(circular_mean, monthly_counts_polar.max() * 0.8, 0, monthly_counts_polar.max() * 0.2, \n",
    "          head_width=0.2, head_length=0.1, fc='red', ec='red', linewidth=3)\n",
    "ax4.text(circular_mean + 0.3, monthly_counts_polar.max() * 0.9, 'Mean', \n",
    "         fontsize=10, fontweight='bold', color='red')\n",
    "\n",
    "# Plot 5: Monthly trend analysis with error bars\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "ax5.errorbar(monthly_means.index, monthly_means.values, \n",
    "             yerr=monthly_std.values, fmt='o-', capsize=5, capthick=2,\n",
    "             linewidth=2, markersize=8, color='darkblue', alpha=0.8)\n",
    "ax5.fill_between(monthly_means.index, \n",
    "                 monthly_means.values - monthly_std.values,\n",
    "                 monthly_means.values + monthly_std.values,\n",
    "                 alpha=0.3, color='blue')\n",
    "ax5.set_xlabel('Month')\n",
    "ax5.set_ylabel('Mean Wave Height (m)')\n",
    "ax5.set_title('Monthly Mean Wave Heights with Error Bars', fontsize=12, fontweight='bold')\n",
    "ax5.set_xticks(range(1, 13))\n",
    "ax5.set_xticklabels(months)\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Seasonal intensity heatmap\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "yearly_monthly_counts = extremes_df.groupby(['year', 'month']).size().unstack(fill_value=0)\n",
    "yearly_monthly_counts = yearly_monthly_counts.reindex(columns=range(1, 13), fill_value=0)\n",
    "\n",
    "im = ax6.imshow(yearly_monthly_counts.T, cmap='YlOrRd', aspect='auto', interpolation='nearest')\n",
    "ax6.set_xlabel('Year')\n",
    "ax6.set_ylabel('Month')\n",
    "ax6.set_title('Extreme Events Heatmap\\n(Year vs Month)', fontsize=12, fontweight='bold')\n",
    "ax6.set_yticks(range(12))\n",
    "ax6.set_yticklabels(months)\n",
    "ax6.set_xticks(range(0, len(yearly_monthly_counts), max(1, len(yearly_monthly_counts)//10)))\n",
    "ax6.set_xticklabels(yearly_monthly_counts.index[::max(1, len(yearly_monthly_counts)//10)])\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax6)\n",
    "cbar.set_label('Number of Extremes')\n",
    "\n",
    "# Plot 7: Monthly autocorrelation analysis\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "monthly_series = extremes_df.groupby(extremes_df.index.to_period('M'))['hs'].max()\n",
    "monthly_series = monthly_series.reindex(pd.date_range(monthly_series.index[0].start_time, \n",
    "                                                      monthly_series.index[-1].end_time, \n",
    "                                                      freq='M'), fill_value=np.nan)\n",
    "\n",
    "lags = range(1, 13)  # 12 months\n",
    "autocorr = [monthly_series.autocorr(lag=lag) for lag in lags]\n",
    "\n",
    "ax7.bar(lags, autocorr, alpha=0.7, color='green', edgecolor='black')\n",
    "ax7.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "ax7.axhline(y=0.2, color='red', linestyle='--', alpha=0.7, label='Significance threshold')\n",
    "ax7.axhline(y=-0.2, color='red', linestyle='--', alpha=0.7)\n",
    "ax7.set_xlabel('Lag (months)')\n",
    "ax7.set_ylabel('Autocorrelation')\n",
    "ax7.set_title('Monthly Autocorrelation of Extreme Heights', fontsize=12, fontweight='bold')\n",
    "ax7.legend()\n",
    "ax7.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 8: Seasonal statistics summary table\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "ax8.axis('off')\n",
    "ax8.set_title('Seasonal Statistics Summary', fontsize=12, fontweight='bold', pad=20)\n",
    "\n",
    "seasonal_stats = extremes_df.groupby('season')['hs'].agg(['count', 'mean', 'std', 'min', 'max']).round(3)\n",
    "seasonal_stats = seasonal_stats.reindex(['Winter', 'Spring', 'Summer', 'Autumn'])\n",
    "\n",
    "table_data = []\n",
    "for season in ['Winter', 'Spring', 'Summer', 'Autumn']:\n",
    "    if season in seasonal_stats.index:\n",
    "        stats = seasonal_stats.loc[season]\n",
    "        table_data.append([\n",
    "            season,\n",
    "            f\"{stats['count']:.0f}\",\n",
    "            f\"{stats['mean']:.3f}\",\n",
    "            f\"{stats['std']:.3f}\",\n",
    "            f\"{stats['min']:.3f}\",\n",
    "            f\"{stats['max']:.3f}\"\n",
    "        ])\n",
    "\n",
    "table = ax8.table(cellText=table_data,\n",
    "                  colLabels=['Season', 'Count', 'Mean (m)', 'Std (m)', 'Min (m)', 'Max (m)'],\n",
    "                  cellLoc='center',\n",
    "                  loc='center',\n",
    "                  bbox=[0, 0, 1, 1])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 1.5)\n",
    "\n",
    "# Style the table\n",
    "for i in range(len(table_data) + 1):\n",
    "    for j in range(6):\n",
    "        cell = table[(i, j)]\n",
    "        if i == 0:  # Header\n",
    "            cell.set_facecolor('#4CAF50')\n",
    "            cell.set_text_props(weight='bold', color='white')\n",
    "        else:\n",
    "            cell.set_facecolor('#f0f0f0' if i % 2 == 0 else 'white')\n",
    "\n",
    "# Plot 9: Density plot of extremes by season\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "for i, season in enumerate(['Winter', 'Spring', 'Summer', 'Autumn']):\n",
    "    season_data = extremes_df[extremes_df['season'] == season]['hs']\n",
    "    if len(season_data) > 0:\n",
    "        x_range = np.linspace(extremes_df['hs'].min()*0.8, extremes_df['hs'].max()*1.2, 1000)\n",
    "        density = stats.gaussian_kde(season_data)\n",
    "        y_density = density(x_range)\n",
    "        ax9.plot(x_range, y_density, linewidth=2, label=f'{season} (n={len(season_data)})', alpha=0.8)\n",
    "\n",
    "ax9.set_xlabel('Wave Height (m)')\n",
    "ax9.set_ylabel('Density')\n",
    "ax9.set_title('Seasonal Density Distributions', fontsize=12, fontweight='bold')\n",
    "ax9.legend()\n",
    "ax9.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comprehensive analysis results\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE SEASONAL ANALYSIS OF EXTREME WAVE HEIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nDATA SUMMARY:\")\n",
    "print(f\"Total number of extremes: {len(extremes_df)}\")\n",
    "print(f\"Date range: {extremes_df.index.min().strftime('%Y-%m-%d')} to {extremes_df.index.max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\nCIRCULAR STATISTICS:\")\n",
    "print(f\"Circular mean month: {months[int(circular_mean_month)-1]} (month {circular_mean_month:.1f})\")\n",
    "print(f\"Circular variance: {circular_variance:.4f}\")\n",
    "print(f\"Rayleigh test p-value: {rayleigh_p:.6f}\")\n",
    "if rayleigh_p < 0.05:\n",
    "    print(\"→ Significant seasonal clustering (p < 0.05)\")\n",
    "else:\n",
    "    print(\"→ No significant seasonal clustering (p ≥ 0.05)\")\n",
    "\n",
    "print(f\"\\nSEASONAL PATTERN SUMMARY:\")\n",
    "print(f\"Most extreme-prone month: {months[monthly_counts.idxmax()-1]} ({monthly_counts.max()} events)\")\n",
    "print(f\"Least extreme-prone month: {months[monthly_counts.idxmin()-1]} ({monthly_counts.min()} events)\")\n",
    "print(f\"Season with most extremes: {extremes_df['season'].value_counts().index[0]}\")\n",
    "print(f\"Season with highest mean: {extremes_df.groupby('season')['hs'].mean().idxmax()}\")\n",
    "print(f\"Circular concentration: {R:.3f} (1.0 = perfect clustering, 0.0 = uniform)\")\n",
    "\n",
    "print(f\"\\nDETAILED SEASONAL STATISTICS:\")\n",
    "for season in ['Winter', 'Spring', 'Summer', 'Autumn']:\n",
    "    season_data = extremes_df[extremes_df['season'] == season]['hs']\n",
    "    if len(season_data) > 0:\n",
    "        print(f\"\\n{season.upper()}:\")\n",
    "        print(f\"  Number of extremes: {len(season_data)}\")\n",
    "        print(f\"  Mean height: {season_data.mean():.3f} m\")\n",
    "        print(f\"  Standard deviation: {season_data.std():.3f} m\")\n",
    "        print(f\"  Minimum: {season_data.min():.3f} m\")\n",
    "        print(f\"  Maximum: {season_data.max():.3f} m\")\n",
    "        print(f\"  Percentage of total extremes: {len(season_data)/len(extremes_df)*100:.1f}%\")\n",
    "    else:\n",
    "        print(f\"\\n{season.upper()}: No extremes recorded\")\n",
    "\n",
    "# Calculate seasonal risk assessment\n",
    "winter_risk = len(extremes_df[extremes_df['season'] == 'Winter']) / len(extremes_df) * 100\n",
    "summer_risk = len(extremes_df[extremes_df['season'] == 'Summer']) / len(extremes_df) * 100\n",
    "spring_risk = len(extremes_df[extremes_df['season'] == 'Spring']) / len(extremes_df) * 100\n",
    "autumn_risk = len(extremes_df[extremes_df['season'] == 'Autumn']) / len(extremes_df) * 100\n",
    "\n",
    "print(f\"\\nRISK ASSESSMENT:\")\n",
    "print(f\"Winter risk: {winter_risk:.1f}% of all extremes\")\n",
    "print(f\"Spring risk: {spring_risk:.1f}% of all extremes\")\n",
    "print(f\"Summer risk: {summer_risk:.1f}% of all extremes\")\n",
    "print(f\"Autumn risk: {autumn_risk:.1f}% of all extremes\")\n",
    "\n",
    "if summer_risk > 0:\n",
    "    print(f\"Risk ratio (Winter/Summer): {winter_risk/summer_risk:.2f}\")\n",
    "if spring_risk > 0:\n",
    "    print(f\"Risk ratio (Winter/Spring): {winter_risk/spring_risk:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE - All seasonal patterns extracted and visualized!\")\n",
    "print(\"=\"*80)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "After loading the data, apply the block method approach with a block size of 1 year (365.2425 days), where each data block must be at least 90% full to take into account in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EVA(data.hs)\n",
    "model.get_extremes(method=\"BM\", block_size=\"365.2425D\", min_last_block=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.extremes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_extremes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "The parameter alpha specifies the confidence limits (default = 0.95)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_diagnostic(alpha=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "The parameter n_samples indicates the number of bootstrap samples used to estimate the confidence bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = model.get_summary(\n",
    "        return_period=[1, 2, 5, 10, 25, 50, 100, 250, 500, 1000],\n",
    "        alpha=0.95,\n",
    "        n_samples=1000,\n",
    "    )\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Modelling univariate time series: Peaks over threshold (POT) + GPD (Generalized Pareto Distribution)\n",
    "\n",
    "We show as example here a **POT** (peaks over threshold) model fitted to the $H_s$ time series. This analysis first finds values over a specified threshold and then declusters these values using a predefined clustering distance, and finally finds the maximum value within each cluster. \n",
    "\n",
    "The same plot can readily be obtained for the other sea-state parameters.\n",
    "\n",
    "We first can have a look at the quality of the fitted model, and to the corresponding return levels as a function of the selected wave height threshold. The parameters r and alpha specify the minimum time distance (duration) between adjacent clusters and the confidence limits (default = 0.95), respectively.\n",
    "\n",
    "The shape and modified scale parameters define the Generalized Pareto Distribution, and they depend on the threshold value, but should be stable within a range of valid thresholds (e.g. less than ~3m here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameter_stability(ts=data.hs, r='72H', alpha=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "The mean residual life plots the average excess value over a given threshold, and it should be approcimately linear above the threshold for which the GPD model is valid (e.g. <~3m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mean_residual_life(data.hs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "The analysis is completed for both Hs and the wind speed, specifying a window of 72 hours and a quantile of 0.98 for determining the threshold to specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant=0.98\n",
    "models = get_fitted_models(data[[\"hs\",\"wspd\"]],quantile=quant,r=\"72H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0].plot_diagnostic(alpha=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "models[1].plot_diagnostic(alpha=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(get_gpd_parameters(models),columns=[\"mu\",\"sigma\",\"xi\"],index=[\"Hs\",\"Wspd\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_Hs = models[0].get_summary(\n",
    "    return_period=[1, 2, 5, 10, 25, 50, 100],\n",
    "    alpha=0.95,\n",
    "    n_samples=1000,\n",
    ")\n",
    "summary_Wspd = models[1].get_summary(\n",
    "    return_period=[1, 2, 5, 10, 25, 50, 100],\n",
    "    alpha=0.95,\n",
    "    n_samples=1000,\n",
    ")\n",
    "print(summary_Hs)\n",
    "print(summary_Wspd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

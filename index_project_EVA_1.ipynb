{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Wind turbine project\n",
    "\n",
    "This notebook is a brief example of the possibilities offered by the toolbox for modeling extreme values, adapted from the tools provided from the ResourceCode website.\n",
    "\n",
    "It relies on the `pyextreme` library which get installed with the Resourcecode toolbox. Here we demonstrate 2 examples of univariate modeling as shown in class. For more information, see https://georgebv.github.io/pyextremes/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# I. Wave Dynamics\n",
    "\n",
    "Part I of the wind turbine project spans 2 classes and consists of selecting a study site and analyzing the wave conditions at the study site. Last week you already began to select your study site, using criteria based on a wind speed threshold, water depth, and distance from the coast. Over the next 2 classes, you will: (A) characterize the mean wave conditions, and sea- sonal variability, and (B) estimate the extreme wave conditions. You will be asked to write up a concise report of your results of Parts I. A & B of the project (due on 24/10/2025). Some of this information will be needed in Parts II and III of your project to model wave transformation and to estimate the impacts of waves on an offshore wind turbine (OWT) for normal operating conditions and survival during extreme events. For the report concerning Part I of the project, please respond to the questions in I.A. (listed below) and I.B. (provided in next week’s class).\n",
    "\n",
    "## I.A. Characterizing the study site mean wave conditions\n",
    "\n",
    "The objective during this class is to characterize the mean wave conditions, as well as the seasonal and interannual variability. To do so, you can download wave time series from your study site from the ResourceCode wave database, which provides access to long-term (1994- 2020) hindcast simulations of wave conditions extending from 12°W to 13.5°E longitude, and from 36°N to 63°N latitude, covering the European and UK’s North Atlantic coast, Irish sea, the Northern Sea, and La Manche. This database was developed at the Ifremer by the LOPS (Laboratory of Ocean Physics and Satellite remote sensing), validated by the LOPS and LCSM (Marine Structures Laboratory), and analyzed at the LHEEA (Laboratory of Hydrody- namics, Energy and Atmospheric Environment) at Ecole Centrale Nantes.\n",
    "\n",
    "The hindcast uses the WAVEWATCH III (WW3) version 7.08 (WW3DG, 2019) spectral wave model, implemented with unstructured grids that have higher resolution in the coastal zone, thus enabling the reproduction of the wave climate in relatively shallow water. It was forced by ERA5 wind felds with a resolution of 0.25°and by the surface currents generated from an atlas of harmonic tidal constituents obtained from outputs of the MARS 2D circulation model and the FES2014 model. The model is particularly well suited for predicting offshore wave conditions, as will be discussed further in class in the lecture presenting spectral wave models (S6). From the ResourceCode website, look at the ‘Explore’ tab to consult the map showing where wave time series are available (https://resourcecode.ifremer.fr/). Use the available toolbox to download the wave time series (available via the ‘Tools’ tab). Refer to the ResourceCode manual (available on the website and in the class repository) for more information about the available variables (about wind, waves, currents, bathymetry,...) and how they are calculated.\n",
    "\n",
    "---\n",
    "\n",
    "### Import Required Libraries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pymannkendall as mk\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "from pyextremes import (\n",
    "    plot_mean_residual_life,\n",
    "    plot_parameter_stability, \n",
    "    EVA\n",
    ")\n",
    "import resourcecode\n",
    "from IPython import get_ipython\n",
    "\n",
    "from resourcecode.eva import (\n",
    "    get_fitted_models,\n",
    "    get_gpd_parameters,\n",
    ")\n",
    "import warnings\n",
    "from mhkit.wave import resource, contours, graphics\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.tri as mtri\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "# Publication-ready Matplotlib defaults (serif fonts and larger text)\n",
    "mpl.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times New Roman\", \"DejaVu Serif\", \"Georgia\", \"serif\"],\n",
    "    \"mathtext.fontset\": \"dejavuserif\",\n",
    "    \"font.size\": 14,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.titleweight\": \"semibold\",\n",
    "    \"axes.labelsize\": 15,\n",
    "    \"axes.labelpad\": 8,\n",
    "    \"axes.linewidth\": 1.1,\n",
    "    \"xtick.labelsize\": 13,\n",
    "    \"ytick.labelsize\": 13,\n",
    "    \"xtick.major.size\": 6,\n",
    "    \"ytick.major.size\": 6,\n",
    "    \"xtick.major.width\": 1.0,\n",
    "    \"ytick.major.width\": 1.0,\n",
    "    \"legend.fontsize\": 13,\n",
    "    \"legend.title_fontsize\": 14,\n",
    "    \"figure.titlesize\": 20,\n",
    "    \"figure.dpi\": 150,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"savefig.bbox\": \"tight\",\n",
    "})\n",
    "\n",
    "\n",
    "os.makedirs('fig', exist_ok=True)\n",
    "plt.savefig('fig/diagnostic_plot_bm.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Enable inline plotting for Jupyter notebooks\n",
    "get_ipython().run_line_magic(\"matplotlib\", \"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "---\n",
    "### I.A.1. Mean Wave Conditions\n",
    "\n",
    "Please download the variables corresponding to the significant wave height, mean wave period ($T_{m02}$), and mean wave direction. Then plot the time series of these variables during the 26-year period from 1994 to 2020, and calculate the mean significant wave height, period, and direction over the entire available time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Chosen Study Site\n",
    "\n",
    "**Site:** Bretagne Sud 1\n",
    "**Coordinates:** $(47.3236111, -3.5522222)$. Coordinates were obtained by downloading the implantation zone KML files from https://www.eoliennesenmer.fr/facades-maritimes-en-france/facade-nord-atlantique-manche-ouest/projet-en-bretagne-sud/bretagne-sud-1. The KML files were then uploaded to Google Earth Pro, where a point at the centre of the zone was chosen, and those coordinates were used.\n",
    "\n",
    "resourcecode.data.get_closest_point returns two things: the identifier of the grid node it matched (point_id) and the horizontal separation between your target coordinates and that node expressed in meters. So dist_m is that offset—how far (in meters) the chosen ResourceCode point lies from the latitude/longitude you supplied. If it’s near zero you’re right on top of a node; larger values mean the dataset had to pick the nearest available grid point.\n",
    "\n",
    "Therefore, the point analysed is 1408.67 m away from the desired coordinates, which is insignificant and should not impact results much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Variables: definitions and units\n",
    "\n",
    "**Core met-ocean time series**\n",
    "\n",
    "| name   | meaning                                   | units |\n",
    "|--------|-------------------------------------------|-------|\n",
    "| `hs`   | significant wave height $H_{m0}=4\\sqrt{m_0}$ | m     |\n",
    "| `t02`  | mean zero-crossing period $T_{m02}=2\\pi\\sqrt{m_0/m_2}$ | s     |\n",
    "| `dir`  | mean wave direction                       | °     |\n",
    "| `spr`  | directional spreading                     | °     |\n",
    "| `fp`   | spectral peak frequency                   | Hz    |\n",
    "| `Tp`   | peak period $=1/fp$                     | s     |\n",
    "| `uwnd` | eastward wind component                   | m·s⁻¹ |\n",
    "| `vwnd` | northward wind component                  | m·s⁻¹ |\n",
    "| `wspd` | wind speed $\\sqrt{uwnd^2+vwnd^2}$       | m·s⁻¹ |\n",
    "| `wdir` | wind direction                            | °     |\n",
    "| `ucur` | eastward surface current                  | m·s⁻¹ |\n",
    "| `vcur` | northward surface current                 | m·s⁻¹ |\n",
    "| `cspd` | current speed $\\sqrt{ucur^2+vcur^2}$    | m·s⁻¹ |\n",
    "| `cdir` | current direction                         | °     |\n",
    "| `dpt`  | water depth                               | m     |\n",
    "\n",
    "**Spectral moments**\n",
    "\n",
    "| name | meaning                               | units  |\n",
    "|------|----------------------------------------|--------|\n",
    "| `m0` | zeroth moment $\\int S(\\omega)\\,d\\omega$ | m²     |\n",
    "| `m1` | first moment $\\int \\omega S(\\omega)\\,d\\omega$ | m²·s⁻¹ |\n",
    "| `m2` | second moment $\\int \\omega^2 S(\\omega)\\,d\\omega$ | m²·s⁻² |\n",
    "\n",
    "**Extreme value analysis (pyextremes)**\n",
    "\n",
    "| item     | meaning                                  |\n",
    "|----------|------------------------------------------|\n",
    "| BM       | block-maxima extraction                  |\n",
    "| POT      | peaks-over-threshold with declustering   |\n",
    "| GEV $\\mu,\\sigma,\\xi$ | location, scale, shape for BM     |\n",
    "| GPD $\\sigma,\\xi$     | scale, shape for POT at a threshold |\n",
    "| `r`      | min time separation between clusters     |\n",
    "| `alpha`  | confidence level for intervals           |\n",
    "| $z_T$  | return level for period $T$ years      |\n",
    "\n",
    "**Direction conventions**\n",
    "\n",
    "All directions are expressed **clockwise from North**.\n",
    "\n",
    "| Variable | Convention | Notes |\n",
    "|-----------|-------------|-------|\n",
    "| `wdir` | *coming-from* | Derived from (`uwnd`, `vwnd`) using `resourcecode.utils.zmcomp2metconv`. |\n",
    "| `dir_from` | *coming-from* | Use if dataset `dir` is *going-to*: convert by `dir_from = (dir + 180) % 360`. |\n",
    "| `cdir_to` | *going-to* | Derived from (`ucur`, `vcur`); use as-is for current flow direction. |\n",
    "| `cdir_from` | *coming-from* (optional) | For comparison with wave/wind directions, compute `cdir_from = (cdir_to + 180) % 360`. |\n",
    "\n",
    "> Always state the convention in figure captions and keep it consistent across the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = resourcecode.Client()\n",
    "# load the resourcecode dataset from Bretagne Sud 1\n",
    "# find the closest point to the coordinates\n",
    "lat = 47.3236111\n",
    "long = -3.5522222\n",
    "point_id, dist_m = resourcecode.data.get_closest_point(latitude=lat, longitude=long)\n",
    "print(point_id, dist_m)\n",
    "\n",
    "# get the data from the closest point\n",
    "data = client.get_dataframe_from_criteria(\n",
    "    \"\"\"\n",
    "{\n",
    "    \"node\": 117231,\n",
    "    \"start\": 0,\n",
    "    \"end\": 99999903600,\n",
    "    \"parameter\": [\"hs\",\"t02\",\"dir\",\"uwnd\",\"vwnd\",\"ucur\",\"vcur\",\"dpt\", \"tp\"]\n",
    "}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c7b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_min, lat_max = lat - 0.5, lat + 0.5\n",
    "lon_min, lon_max = long - 0.5, long + 0.5\n",
    "selected_node = [point_id]\n",
    "nodes = resourcecode.data.get_grid_field().query(\n",
    "    f\"latitude <= {lat_max} and latitude >= {lat_min} and longitude > {lon_min} and longitude < {lon_max}\"\n",
    ")\n",
    "spec = resourcecode.get_grid_spec().query(\n",
    "    f\"latitude <= {lat_max} and latitude >= {lat_min} and longitude > {lon_min} and longitude < {lon_max}\"\n",
    ")\n",
    "coast = resourcecode.data.get_coastline().query(\n",
    "    f\"latitude <= {lat_max} and latitude >= {lat_min} and longitude > {lon_min} and longitude < {lon_max}\"\n",
    ")\n",
    "islands = resourcecode.data.get_islands().query(\n",
    "    f\"latitude <= {lat_max} and latitude >= {lat_min} and longitude > {lon_min} and longitude < {lon_max}\"\n",
    ")\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(nodes.longitude, nodes.latitude, s=1, label=\"Nodes\")\n",
    "plt.scatter(spec.longitude, spec.latitude, s=2, color=\"orange\", label=\"Spectral grid\")\n",
    "plt.ylim(lat_min, lat_max)\n",
    "plt.xlim(lon_min, lon_max)\n",
    "plt.plot(coast.longitude, coast.latitude, color=\"black\")\n",
    "classes = list(islands.ID.unique())\n",
    "for c in classes:\n",
    "    df2 = islands.loc[islands['ID'] == c]\n",
    "    plt.plot(df2.longitude, df2.latitude, color=\"black\")\n",
    "plt.scatter(\n",
    "    nodes[nodes.node == selected_node[0]].longitude,\n",
    "    nodes[nodes.node == selected_node[0]].latitude,\n",
    "    s=3,\n",
    "    color=\"red\",\n",
    "    label=\"Selected point\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('graphs/nodes_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34327236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the data for plotting\n",
    "tri = resourcecode.get_triangles().to_numpy()-1 #The '-1' is due to the Zero-based numbering of python\n",
    "field_mesh = resourcecode.data.get_grid_field().to_numpy()\n",
    "triang = mtri.Triangulation(field_mesh[:,1], field_mesh[:,2],tri)\n",
    "\n",
    "plotted_nodes = (field_mesh[:,1]<=lon_max) & (field_mesh[:,1]>=lon_min) & (field_mesh[:,2]<=lat_max) & (field_mesh[:,2]>=lat_min)\n",
    "\n",
    "s=field_mesh[:,3]\n",
    "s[np.isnan(s)] = 0 #Due to missing values in bathy\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax0 = fig.add_subplot(111, aspect='equal')\n",
    "\n",
    "plt.ylim(lat_min, lat_max)\n",
    "plt.xlim(lon_min, lon_max)\n",
    "SC = ax0.tripcolor(triang,s, shading='gouraud')\n",
    "SC.set_clim(min(s[plotted_nodes]),max(s[plotted_nodes]))\n",
    "#Plot selected location\n",
    "plt.scatter(\n",
    "    nodes[nodes.node == selected_node[0]].longitude,\n",
    "    nodes[nodes.node == selected_node[0]].latitude,\n",
    "    s=3,\n",
    "    color=\"red\",\n",
    "    label=\"Selected point\",\n",
    ")\n",
    "#Add coastlines and islands\n",
    "plt.plot(coast.longitude, coast.latitude, color=\"black\")\n",
    "classes = list(islands.ID.unique())\n",
    "for c in classes:\n",
    "    df2 = islands.loc[islands['ID'] == c]\n",
    "    plt.plot(df2.longitude, df2.latitude, color=\"black\")\n",
    "\n",
    "# Colorbar.\n",
    "the_divider = make_axes_locatable(ax0)\n",
    "color_axis = the_divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "cbar = plt.colorbar(SC, cax=color_axis)\n",
    "cbar.set_label('Depth (m)', fontsize=18)\n",
    "plt.show()\n",
    "plt.savefig('graphs/bathymetry_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive only what ResourceCode does not already provide\n",
    "# Assumes your download included: hs, t02, dir, uwnd, vwnd (and optionally ucur, vcur, dpt)\n",
    "\n",
    "# Wind: speed (m/s) and coming-from direction (deg)\n",
    "data[\"wspd\"], data[\"wdir\"] = resourcecode.utils.zmcomp2metconv(data[\"uwnd\"], data[\"vwnd\"])\n",
    "\n",
    "# Currents (optional)\n",
    "if {\"ucur\", \"vcur\"}.issubset(data.columns):\n",
    "    data[\"cspd\"], data[\"cdir\"] = resourcecode.utils.zmcomp2metconv(data[\"ucur\"], data[\"vcur\"])\n",
    "\n",
    "# Waves: use provided mean zero-crossing period\n",
    "if \"t02\" in data.columns:\n",
    "    data[\"Tm02\"] = data[\"t02\"]\n",
    "else:\n",
    "    raise KeyError(\"Missing 't02' in the request. Add 't02' to parameter list.\")\n",
    "\n",
    "# Keep dataset wave direction as-is; document convention once in the notebook.\n",
    "# Do NOT overwrite 'dir' or try to recompute Tm02 from hs/dir.\n",
    "\n",
    "data = data.sort_index()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "**Q: Do any trends in the wave height, period, or direction exist over the 26 year time period? Do you expect there to be changes in the mean conditions during the 30-year lifetime of the wind turbine? If so, why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IA.1 — Mean wave conditions (1994–2020): time series, means, linear trends\n",
    "\n",
    "def circmean_deg(x_deg):\n",
    "    \"\"\"Circular mean of degrees in [0, 360).\"\"\"\n",
    "    x = pd.Series(x_deg).dropna().values\n",
    "    if x.size == 0:\n",
    "        return np.nan\n",
    "    r = np.deg2rad(x)\n",
    "    s = np.sin(r).sum()\n",
    "    c = np.cos(r).sum()\n",
    "    return (np.degrees(np.arctan2(s, c)) + 360.0) % 360.0\n",
    "\n",
    "def verdict(p):\n",
    "    return \"significant\" if p < 0.05 else \"not significant\"\n",
    "\n",
    "# helper\n",
    "def wrap360(a):\n",
    "    return (a % 360.0 + 360.0) % 360.0\n",
    "\n",
    "# Select analysis window\n",
    "start = pd.Timestamp(\"1994-01-01\")\n",
    "end   = pd.Timestamp(\"2020-12-31 23:59:59\")\n",
    "needed = [\"hs\", \"Tm02\", \"dir\"]\n",
    "missing = [v for v in needed if v not in data.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing variables for IA.1: {missing}\")\n",
    "\n",
    "df = data.loc[start:end, needed].copy()\n",
    "\n",
    "# Monthly means (direction handled circularly)\n",
    "monthly = pd.DataFrame({\n",
    "    \"hs\":   df[\"hs\"].resample(\"M\").mean(),\n",
    "    \"Tm02\": df[\"Tm02\"].resample(\"M\").mean(),\n",
    "})\n",
    "monthly[\"dir\"] = df[\"dir\"].resample(\"M\").apply(circmean_deg)\n",
    "\n",
    "# Overall means (direction: circular mean)\n",
    "mean_hs   = df[\"hs\"].mean()\n",
    "mean_tm02 = df[\"Tm02\"].mean()\n",
    "mean_dir  = circmean_deg(df[\"dir\"])\n",
    "\n",
    "# Linear trends on monthly means\n",
    "t_years = (monthly.index - monthly.index[0]).days / 365.2425\n",
    "\n",
    "# Hs trend\n",
    "hs_ok = monthly[\"hs\"].dropna()\n",
    "t_hs = t_years[hs_ok.index.get_indexer(hs_ok.index)]\n",
    "hs_reg = stats.linregress(t_hs, hs_ok.values)\n",
    "hs_slope_dec = hs_reg.slope * 10.0      # m per decade\n",
    "hs_delta_30  = hs_reg.slope * 30.0      # m over 30 years\n",
    "\n",
    "# Tm02 trend\n",
    "tm_ok = monthly[\"Tm02\"].dropna()\n",
    "t_tm = t_years[tm_ok.index.get_indexer(tm_ok.index)]\n",
    "tm_reg = stats.linregress(t_tm, tm_ok.values)\n",
    "tm_slope_dec = tm_reg.slope * 10.0      # s per decade\n",
    "tm_delta_30  = tm_reg.slope * 30.0      # s over 30 years\n",
    "\n",
    "# Direction trend: unwrap, regress, report slope in deg/dec\n",
    "dir_ok = monthly[\"dir\"].dropna()\n",
    "t_dir = t_years[dir_ok.index.get_indexer(dir_ok.index)]\n",
    "dir_unwrap = np.degrees(np.unwrap(np.deg2rad(dir_ok.values)))\n",
    "dir_reg = stats.linregress(t_dir, dir_unwrap)\n",
    "dir_slope_dec = dir_reg.slope * 10.0    # deg per decade\n",
    "dir_delta_30  = dir_reg.slope * 30.0    # deg over 30 years\n",
    "\n",
    "# Plot monthly series (1994–2020)\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 9), sharex=True)\n",
    "\n",
    "# Hs with fitted line\n",
    "axes[0].plot(monthly.index, monthly[\"hs\"], lw=0.8)\n",
    "yhat_hs = hs_reg.intercept + hs_reg.slope * t_years\n",
    "axes[0].plot(monthly.index, yhat_hs, lw=1.2)\n",
    "axes[0].set_ylabel(\"$H_S$ (m)\")\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_title(f\"$H_S$ monthly mean | {hs_slope_dec:.3f} m/dec, p={hs_reg.pvalue:.3f} ({verdict(hs_reg.pvalue)})\")\n",
    "\n",
    "# Tm02 with fitted line\n",
    "axes[1].plot(monthly.index, monthly[\"Tm02\"], lw=0.8)\n",
    "yhat_tm = tm_reg.intercept + tm_reg.slope * t_years\n",
    "axes[1].plot(monthly.index, yhat_tm, lw=1.2)\n",
    "axes[1].set_ylabel(\"$T_{m02}$ (s)\")\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_title(f\"Tm02 monthly mean | {tm_slope_dec:.3f} s/dec, p={tm_reg.pvalue:.3f} ({verdict(tm_reg.pvalue)})\")\n",
    "\n",
    "# Direction (circular monthly mean) with fitted line\n",
    "axes[2].plot(monthly.index, monthly[\"dir\"], lw=0.8)\n",
    "yhat_dir_unwrap = dir_reg.intercept + dir_reg.slope * t_years\n",
    "yhat_dir = wrap360(yhat_dir_unwrap)\n",
    "axes[2].plot(monthly.index, yhat_dir, lw=1.2)  # add fit\n",
    "axes[2].set_ylabel(\"Dir (deg, coming-from)\")\n",
    "axes[2].grid(alpha=0.3)\n",
    "axes[2].set_title(\n",
    "    f\"Direction monthly circular mean | {dir_slope_dec:.2f}°/dec, \"\n",
    "    f\"p={dir_reg.pvalue:.3f} ({verdict(dir_reg.pvalue)})\"\n",
    ")\n",
    "axes[2].set_xlabel(\"Year\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fig/IA1_mean_wave_conditions_timeseries.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Printed summary for the report\n",
    "print(\"IA.1 — Mean conditions over 1994–2020\")\n",
    "print(f\"  Mean Hs   : {mean_hs:.3f} m\")\n",
    "print(f\"  Mean Tm02 : {mean_tm02:.3f} s\")\n",
    "print(f\"  Mean Dir  : {mean_dir:.1f}° (coming-from)\")\n",
    "\n",
    "print(\"\\nLinear trends on monthly means (least squares):\")\n",
    "print(f\"  Hs   : {hs_slope_dec:.3f} m/dec  (p={hs_reg.pvalue:.3f}, n={hs_ok.size}), Δ30y={hs_delta_30:.3f} m\")\n",
    "print(f\"  Tm02 : {tm_slope_dec:.3f} s/dec  (p={tm_reg.pvalue:.3f}, n={tm_ok.size}), Δ30y={tm_delta_30:.3f} s\")\n",
    "print(f\"  Dir  : {dir_slope_dec:.2f} °/dec (p={dir_reg.pvalue:.3f}, n={dir_ok.size}), Δ30y={dir_delta_30:.2f} °\")\n",
    "\n",
    "print(\"\\nInterpretation rule-of-thumb: treat p<0.05 as evidence of a trend. Use Δ30y to state expected change over a turbine lifetime.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_hs  = mk.original_test(hs_ok.values, alpha=0.05)\n",
    "result_period = mk.original_test(tm_ok.values, alpha=0.050)\n",
    "result_direction = mk.original_test(dir_ok.values, alpha=0.05)\n",
    "\n",
    "print(\"\\nResults for Significant Wave Height (Hs):\")\n",
    "print(f\"  Trend: {result_hs.trend}\")\n",
    "print(f\"  H (Test Statistic): {result_hs.h}\")\n",
    "print(f\"  P-value: {result_hs.p:.4f}\")\n",
    "print(f\"  Z-Score: {result_hs.z:.4f}\")\n",
    "print(f\"  Tau: {result_hs.Tau:.4f}\")\n",
    "print(f\"  Sen's Slope: {result_hs.slope:.4f}\")\n",
    "print(f\"  Intercept: {result_hs.intercept:.4f}\")\n",
    "\n",
    "print(\"\\nResults for Wave Period (Tm02):\")\n",
    "print(f\"  Trend: {result_period.trend}\")\n",
    "print(f\"  H (Test Statistic): {result_period.h}\")\n",
    "print(f\"  P-value: {result_period.p:.4f}\")\n",
    "print(f\"  Z-Score: {result_period.z:.4f}\")\n",
    "print(f\"  Tau: {result_period.Tau:.4f}\")\n",
    "print(f\"  Sen's Slope: {result_period.slope:.4f}\")\n",
    "print(f\"  Intercept: {result_period.intercept:.4f}\")\n",
    "\n",
    "print(\"\\nResults for Wave Direction:\")\n",
    "print(f\"  Trend: {result_direction.trend}\")\n",
    "print(f\"  H (Test Statistic): {result_direction.h}\")\n",
    "print(f\"  P-value: {result_direction.p:.4f}\")\n",
    "print(f\"  Z-Score: {result_direction.z:.4f}\")\n",
    "print(f\"  Tau: {result_direction.Tau:.4f}\")\n",
    "print(f\"  Sen's Slope: {result_direction.slope:.4f}\")\n",
    "print(f\"  Intercept: {result_direction.intercept:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "**Answer:** No statistically significant trends were detected in wave height ($H_s$), period ($T_{m02}$), or direction using either Ordinary Least Squares (OLS) or the Mann-Kendall (MK) test on monthly mean data. Expected 30-year changes in mean conditions remain negligible relative to natural variability.\n",
    "\n",
    "---\n",
    "### Mean Conditions and Trend Analysis (1994-2020)\n",
    "\n",
    "**Mean conditions**\n",
    "-   $\\overline{H_s} = 2.092\\ \\text{m}$\n",
    "-   $\\overline{T_{m02}} = 5.785\\ \\text{s}$\n",
    "-   Mean direction $= 275.5^\\circ$ (coming from W)\n",
    "\n",
    "---\n",
    "**Trend Analysis Methods**\n",
    "-   Aggregate hourly data to monthly means to reduce noise while retaining seasonal information influence.\n",
    "-   **Method 1: Ordinary Least Squares (OLS)**: Perform regression vs time in years. Direction handled with circular monthly mean, then unwrapped.\n",
    "-   **Method 2: Mann-Kendall (MK) Test**: Apply the non-parametric test to detect monotonic trends in the monthly mean time series.\n",
    "-   Significance for both methods assessed at $\\alpha=0.05$.\n",
    "\n",
    "---\n",
    "**OLS Results** (slope per decade; $\\Delta 30\\text{y}$ is implied 30-year change; $n=324$ monthly means)\n",
    "-   $H_s$: $+0.015\\ \\text{m/dec}$, $p=0.776$, $\\Delta 30\\text{y}\\approx +0.046\\ \\text{m}$. (No significant trend)\n",
    "-   $T_{m02}$: $+0.086\\ \\text{s/dec}$, $p=0.150$, $\\Delta 30\\text{y}\\approx +0.259\\ \\text{s}$. (No significant trend)\n",
    "-   Direction: $+0.75^\\circ/\\text{dec}$, $p=0.349$, $\\Delta 30\\text{y}\\approx +2.26^\\circ$. (No significant trend)\n",
    "\n",
    "---\n",
    "**MK Results (on monthly means)**\n",
    "-   $H_s$: $p=0.8098$ (no significant trend); Sen's slope $\\approx +1.0\\times10^{-4}$ m/month ($\\approx +0.012$ m/decade).\n",
    "-   $T_{m02}$: $p=0.1957$ (no significant trend); Sen's slope $\\approx +7.0\\times10^{-4}$ s/month ($\\approx +0.084$ s/decade).\n",
    "-   Direction: $p=0.4405$ (no significant trend); Sen's slope $\\approx +5.5\\times10^{-3}$ deg/month ($\\approx +0.66^\\circ$/decade).\n",
    "\n",
    "*(Note: Sen's slopes converted approximately from per-month to per-decade for comparison with OLS results)*\n",
    "\n",
    "---\n",
    "**Interpretation**\n",
    "-   Both OLS and Mann-Kendall analyses performed on monthly mean data consistently indicate no statistically significant secular trends in mean $H_s$, $T_{m02}$, or direction over the 1994-2020 period.\n",
    "-   The magnitudes of the calculated slopes (both OLS and Sen's slope) are very small, suggesting that any potential underlying linear or monotonic change over the 26 years is minimal compared to the observed variability.\n",
    "-   Expected 30-year changes based on these negligible trends are small compared to the substantial seasonal and interannual variability present in the data.\n",
    "-   For design purposes over the next 30 years, emphasis should likely remain on characterizing the existing variability (seasonal, interannual) and extreme conditions rather than adjusting significantly for potential drifts in mean conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### I.A.2. Most Common Wave Conditions\n",
    "\n",
    "To identify the most common operating conditions, we will create a 2D histogram (scatter diagram) of significant wave height ($H_{m0}$) versus mean wave period ($T_{m02}$). We will also plot a wave rose to identify the most common wave incidence direction(s).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I.A.2 — Most common wave conditions: (T_m02, H_m0) scatter diagram + deep/linear regimes + wave rose\n",
    "# --------------------------\n",
    "# Inputs and thresholds\n",
    "# --------------------------\n",
    "g = 9.81  # m/s^2\n",
    "lat = float(lat) if \"lat\" in globals() else np.nan\n",
    "long = float(long) if \"long\" in globals() else np.nan\n",
    "\n",
    "# Clean series\n",
    "T = pd.to_numeric(data[\"t02\"], errors=\"coerce\").to_numpy()\n",
    "Hs = pd.to_numeric(data[\"hs\"],  errors=\"coerce\").to_numpy()\n",
    "Dpt = pd.to_numeric(data[\"dpt\"], errors=\"coerce\").to_numpy()\n",
    "Dir = pd.to_numeric(data[\"dir\"], errors=\"coerce\").to_numpy() if \"dir\" in data.columns else None\n",
    "\n",
    "mask = np.isfinite(T) & np.isfinite(Hs) & np.isfinite(Dpt)\n",
    "if Dir is not None:\n",
    "    mask = mask & np.isfinite(Dir)\n",
    "\n",
    "T = T[mask]\n",
    "Hs = Hs[mask]\n",
    "Dpt = Dpt[mask]\n",
    "Dir = Dir[mask] if Dir is not None else None\n",
    "\n",
    "h_mean = float(np.nanmean(Dpt))\n",
    "h_std  = float(np.nanstd(Dpt))\n",
    "\n",
    "# Deep and shallow (linear shallow-water) period thresholds at mean depth\n",
    "T_deep    = 4.0 * np.sqrt(h_mean / g)     # deep-water if T < T_deep\n",
    "T_shallow = 25.0 * np.sqrt(h_mean / g)    # shallow-water (linear nondispersive) if T > T_shallow\n",
    "\n",
    "print(f\"Water depth: mean h = {h_mean:.2f} m (std = {h_std:.2f} m) at ({lat}, {long})\")\n",
    "print(f\"Deep-water threshold: T < {T_deep:.2f} s\")\n",
    "print(f\"Shallow-water (linear) threshold: T > {T_shallow:.2f} s\")\n",
    "\n",
    "# --------------------------\n",
    "# 2D histogram: period on x, Hs on y\n",
    "# --------------------------\n",
    "# Bin ranges, robust to outliers\n",
    "T_max_plot = np.nanpercentile(T, 99.5)\n",
    "Hs_max_plot = np.nanpercentile(Hs, 99.5)\n",
    "T_min_plot = max(0.0, np.nanpercentile(T, 0.5))\n",
    "Hs_min_plot = max(0.0, np.nanpercentile(Hs, 0.5))\n",
    "\n",
    "xbins = np.linspace(T_min_plot, T_max_plot, 120)\n",
    "ybins = np.linspace(Hs_min_plot, Hs_max_plot, 120)\n",
    "\n",
    "H2, xedges, yedges = np.histogram2d(T, Hs, bins=[xbins, ybins], density=True)\n",
    "# Peak (most common) conditions from density maximum\n",
    "imax = np.unravel_index(np.nanargmax(H2), H2.shape)\n",
    "T_mode = 0.5 * (xedges[imax[0]] + xedges[imax[0] + 1])\n",
    "Hs_mode = 0.5 * (yedges[imax[1]] + yedges[imax[1] + 1])\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6.4, 5.8), constrained_layout=True)\n",
    "hist = ax.hist2d(T, Hs, bins=[xbins, ybins], density=True, cmap=\"viridis\")\n",
    "cbar = fig.colorbar(hist[3], ax=ax)\n",
    "cbar.set_label(\"Density\")\n",
    "\n",
    "# Thresholds: vertical lines in T\n",
    "ax.axvline(T_deep, color=\"red\", linestyle=\"--\", linewidth=1.5, label=f\"Deep-water: T<{T_deep:.2f}s\")\n",
    "\n",
    "# Annotate regions\n",
    "y_txt = Hs_min_plot + 0.92 * (Hs_max_plot - Hs_min_plot)\n",
    "ax.text(T_min_plot + 0.02*(T_max_plot-T_min_plot), y_txt, \"Deep-water\", color=\"red\", fontsize=14)\n",
    "\n",
    "# Most common bin marker\n",
    "ax.plot(T_mode, Hs_mode, marker=\"o\", markersize=6, color=\"white\", mec=\"k\", label=f\"Mode ≈ ({T_mode:.2f}s, {Hs_mode:.2f}m)\")\n",
    "ax.annotate(f\"{T_mode:.2f}s, {Hs_mode:.2f}m\", xy=(T_mode, Hs_mode),\n",
    "            xytext=(5, -12), textcoords=\"offset points\", color=\"white\", fontsize=12)\n",
    "\n",
    "ax.set_xlabel(\"Mean Wave Period $T_{m02}$ [s]\")\n",
    "ax.set_ylabel(\"Significant Wave Height $H_{m0}$ [m]\")\n",
    "ax.set_title(\"Most Common Wave Conditions: $T_{m02}$ vs $H_{m0}$\")\n",
    "ax.legend(loc=\"lower right\", frameon=True)\n",
    "Path(\"graphs\").mkdir(parents=True, exist_ok=True)\n",
    "fig.savefig(\"graphs/wave_T_vs_Hs_density.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# Quantify shares for reference\n",
    "pct_deep = np.mean(T < T_deep) * 100.0\n",
    "pct_shallow = np.mean(T > T_shallow) * 100.0\n",
    "pct_transition = 100.0 - pct_deep - pct_shallow\n",
    "print(f\"Share deep-water (T<{T_deep:.2f}s): {pct_deep:.2f}%\")\n",
    "print(f\"Share shallow (linear) (T>{T_shallow:.2f}s): {pct_shallow:.4f}%\")\n",
    "print(f\"Share transition: {pct_transition:.2f}%\")\n",
    "\n",
    "# --------------------------\n",
    "# Wave rose: most common wave incidence directions\n",
    "# --------------------------\n",
    "if Dir is not None:\n",
    "    # Define Hs classes for a classic wave rose (adjust as needed)\n",
    "    hs_bins = [0, 1, 2, 3, 4, 6, np.inf]\n",
    "    hs_labels = [\"0–1\", \"1–2\", \"2–3\", \"3–4\", \"4–6\", \"≥6\"]  # m\n",
    "\n",
    "    # Direction bins (every 10°)\n",
    "    n_dir_bins = 36\n",
    "    dir_edges_deg = np.linspace(0, 360, n_dir_bins + 1)\n",
    "    dir_centers_deg = 0.5 * (dir_edges_deg[:-1] + dir_edges_deg[1:])\n",
    "    dir_edges_rad = np.deg2rad(dir_edges_deg)\n",
    "\n",
    "    # Prepare stacked counts per Hs class\n",
    "    counts_stack = []\n",
    "    for i in range(len(hs_bins) - 1):\n",
    "        mask_h = (Hs >= hs_bins[i]) & (Hs < hs_bins[i + 1])\n",
    "        hist, _ = np.histogram(Dir[mask_h] % 360.0, bins=dir_edges_deg)\n",
    "        counts_stack.append(hist.astype(float))\n",
    "    counts_stack = np.array(counts_stack)  # shape: [n_hs_classes, n_dir_bins]\n",
    "\n",
    "    # Convert counts to percentages\n",
    "    total_counts = counts_stack.sum(axis=0)\n",
    "    total_all = total_counts.sum()\n",
    "    frac_stack = (counts_stack / total_all) * 100.0  # percent of total occurrences\n",
    "\n",
    "    # Polar plot\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, projection=\"polar\")\n",
    "    ax.set_theta_zero_location(\"N\")\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    bottom = np.zeros(n_dir_bins)\n",
    "    width = np.diff(dir_edges_rad)\n",
    "    wave_colors = plt.cm.Blues(np.linspace(0.25, 0.85, len(hs_labels)))\n",
    "    legend_handles = []\n",
    "    legend_labels = []\n",
    "\n",
    "    for i, label in enumerate(hs_labels):\n",
    "        bars = ax.bar(\n",
    "            np.deg2rad(dir_centers_deg), frac_stack[i], width=width,\n",
    "            bottom=bottom, align=\"center\", edgecolor=\"k\", linewidth=0.3,\n",
    "            color=wave_colors[i]\n",
    "        )\n",
    "        legend_handles.append(bars[0])\n",
    "        legend_labels.append(f\"{label} m\")\n",
    "        bottom += frac_stack[i]\n",
    "\n",
    "    ax.set_title(\"Wave Rose (coming-from)\\nStacked by $H_{m0}$ class [% of total]\", va=\"bottom\")\n",
    "    ax.set_rlabel_position(225)\n",
    "    ax.legend(legend_handles, legend_labels, loc=\"lower left\", bbox_to_anchor=(0.95, 0.05), frameon=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"graphs/wave_rose_stacked_Hs.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Also print the dominant direction sector by total frequency\n",
    "    dom_bin = int(np.argmax(total_counts))\n",
    "    dom_dir_center = dir_centers_deg[dom_bin]\n",
    "    print(f\"Most common incidence direction bin center: {dom_dir_center:.1f}° (coming-from)\")\n",
    "else:\n",
    "    print(\"Column 'dir' not found. Skipping wave rose.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "**Q: What is the water depth at this location? Indicate on the histrogram for what wave conditions the waves are considered deep water waves? linear waves?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['dpt'].std())\n",
    "\n",
    "mean_water_depth = data['dpt'].mean()\n",
    "print(f'mean water depth at location (lat, long) = ({lat}, {long}) is {mean_water_depth:.2f} m')\n",
    "\n",
    "# Calculating wave length using the linear wave theory\n",
    "# wave length = g * T^2 / (2 * pi)\n",
    "g = 9.81 # m/s^2\n",
    "wave_length = g * (data['t02']**2) / (2 * np.pi)\n",
    "water_depth = data['dpt']\n",
    "\n",
    "# check condition for deep water waves\n",
    "deep_water_length_condition = 2 * mean_water_depth\n",
    "transition_water_length_condition = 25 * mean_water_depth\n",
    "\n",
    "deep_water_condition = wave_length < deep_water_length_condition\n",
    "# calculate the percentage of deep water waves\n",
    "deep_water_percentage = deep_water_condition.mean()\n",
    "print(f'percentage of deep water waves: {deep_water_percentage:.2%}')\n",
    "\n",
    "# Compute corresponding period for deep water waves\n",
    "deep_water_period_condition = 4 * np.sqrt(mean_water_depth / g)\n",
    "transition_water_period_condition = 25 * np.sqrt(mean_water_depth / g)\n",
    "\n",
    "print(f'deep water period: {deep_water_period_condition:.2f} s')\n",
    "# check condition for linear waves\n",
    "#linear_waves_condition = wave_length > 20 * mean_water_depth\n",
    "\n",
    "### plot the wave length distribution ###\n",
    "Path(\"graphs\").mkdir(parents=True, exist_ok=True)\n",
    "fig, ax = plt.subplots(figsize=(6.4, 4.8), constrained_layout=True)\n",
    "ax.hist(wave_length, bins=50, density=True, alpha=0.7, label='Wave Length Distribution')\n",
    "ax.axvline(x=deep_water_length_condition, color='r', linestyle='--',\n",
    "           label=f'Deep Water Threshold (L < {deep_water_length_condition:.1f}m)')\n",
    "ax.set_xlabel('Wave Length (m)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Wave Length Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "fig.savefig('graphs/wave_length_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "### plot the wave period distribution ###\n",
    "fig, ax = plt.subplots(figsize=(6.4, 4.8), constrained_layout=True)\n",
    "ax.hist(data['t02'], bins=50, density=True, alpha=0.7, label='Wave Period Distribution')\n",
    "ax.axvline(x=deep_water_period_condition, color='r', linestyle='--',\n",
    "           label=f'Deep Water Threshold (T < {deep_water_period_condition:.1f}s)')\n",
    "ax.set_xlabel('Wave Period (s)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Wave Period Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, which='both', alpha=0.3)\n",
    "fig.savefig('graphs/wave_period_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# --- 6. Result Summary ---\n",
    "print(\"\\n--- Result Summary ---\")\n",
    "print(f\"Analysis based on Mean Depth h = {mean_water_depth:.2f} m\")\n",
    "\n",
    "print(\"\\nThresholds:\")\n",
    "print(f\"  Deep Water:     L < {deep_water_length_condition:.1f} m   |   T < {deep_water_period_condition:.2f} s\")\n",
    "print(f\"  Intersection of Deep and Shallow Water:  L > {transition_water_length_condition:.1f} m  |   T > {transition_water_period_condition:.2f} s\")\n",
    "\n",
    "# Compute the percentage of deep water waves based on the approximate wavelength\n",
    "is_deep_water_wave = wave_length < deep_water_length_condition\n",
    "is_transition_zone_wave = (\n",
    "    (deep_water_length_condition < wave_length) &\n",
    "    (wave_length < transition_water_length_condition)\n",
    ")\n",
    "\n",
    "is_deep_water_period = data['t02'] < deep_water_period_condition\n",
    "is_transition_zone_period = (\n",
    "    (deep_water_period_condition < data['t02']) &\n",
    "    (data['t02'] < transition_water_period_condition)\n",
    ")\n",
    "# get a percentage of the boolean series of true and false\n",
    "deep_water_percentage_L_approx = is_deep_water_wave.mean()\n",
    "transition_percentage_L_approx = is_transition_zone_wave.mean()\n",
    "deep_water_percentage_T_approx = is_deep_water_period.mean()\n",
    "transition_percentage_T_approx = is_transition_zone_period.mean()\n",
    "\n",
    "print(\"\\nClassification based on Approximate Wavelength (L_approx = gT² / 2π):\")\n",
    "print(f\"  Percentage Deep Water Wave Lenghts: {deep_water_percentage_L_approx:.2%}\")\n",
    "print(f\"  Percentage Transition Zone Wave Lenghts: {transition_percentage_L_approx:.2%}\")\n",
    "\n",
    "# Characterizing the transition band for T\n",
    "print(\"\\nClassification based on Approximate Wavelength (L_approx = gT² / 2π):\")\n",
    "print(f\"  Percentage Deep Water Wave Periods: {deep_water_percentage_T_approx:.2%}\")\n",
    "print(f\"  Percentage Transition Zone Wave Periods: {transition_percentage_T_approx:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Wave Regime Classification\n",
    "\n",
    "**Method**\n",
    "\n",
    "- Hourly conditions classified into deep, transitional, and shallow regimes using $H_s$, $T_{m02}$, depth $h$, wavelength $L$.\n",
    "- Fixed mean depth $h=93.32\\ \\text{m}$ (depth variability small: $\\sigma_h=1.2\\ \\text{m}$).\n",
    "- Two diagnostics:\n",
    "  1) Period criterion via $T\\sqrt{g/h}$.\n",
    "  2) Approximate deep-water wavelength $L_{\\text{approx}}=\\dfrac{g\\,T_{m02}^2}{2\\pi}$ for a fast screen.\n",
    "- $g=9.81\\ \\text{m s}^{-2}$.\n",
    "\n",
    "---\n",
    "\n",
    "**Thresholds** (at $h=93.32\\ \\text{m}$)\n",
    "\n",
    "- **Deep water**: $h/L>1/2\\ \\Rightarrow\\ L<186.6\\ \\text{m}$; equivalently $T\\sqrt{g/h}<4\\ \\Rightarrow\\ T<12.34\\ \\text{s}$.\n",
    "- **Shallow water**: $h/L<1/25\\ \\Rightarrow\\ L>2333.0\\ \\text{m}$; equivalently $T\\sqrt{g/h}>25\\ \\Rightarrow\\ T>77.11\\ \\text{s}$.\n",
    "- **Transition zone**: between these limits.\n",
    "\n",
    "---\n",
    "\n",
    "**Results**\n",
    "\n",
    "- **Classification by approximate wavelength** $(L_{\\text{approx}})$:\n",
    "  - Deep water ($L_{\\text{approx}}<186.6\\ \\text{m}$): **99.76%**\n",
    "  - Transition zone ($186.6\\le L_{\\text{approx}}\\le 2333.0\\ \\text{m}$): **0.24%**\n",
    "  - Shallow water ($L_{\\text{approx}}>2333.0\\ \\text{m}$): **0.00%**\n",
    "\n",
    "- **Classification by period** $(T_{m02})$:\n",
    "  - Deep water ($T<12.34\\ \\text{s}$): **99.98%**\n",
    "  - Transition zone ($12.34\\le T\\le 77.11\\ \\text{s}$): **0.02%**\n",
    "  - Shallow water ($T>77.11\\ \\text{s}$): **0.00%**\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "- The site operates **almost entirely in deep water**. Period- and wavelength-based diagnostics both yield >99.7% deep conditions; the period test is slightly more conservative here.\n",
    "- **Transition cases are rare** (0.02–0.24%) and occur near the deep/transition boundary; none reach shallow-water criteria.\n",
    "- Using $L_{\\text{approx}}$ is adequate for screening at this depth because periods rarely approach the transitional threshold. For edge cases, the direct period criterion $T\\sqrt{g/h}$ remains the most transparent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### I.A.3. Seasonal Variability\n",
    "\n",
    "Next, we will evaluate temporal variability by calculating and plotting the mean wave height, period, and direction as a function of the month of the year (e.g., mean for January, February, etc.).\n",
    "\n",
    "**Data and method.** Hourly hindcast at coordinates. Monthly climatology across all years.  \n",
    "- Height and period: arithmetic mean with interquartile range (IQR).  \n",
    "- Direction: circular mean with circular standard deviation, expressed as *coming-from* degrees.  \n",
    "- Angles unwrapped around the overall mean for continuity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IA.3 Seasonal variability: monthly climatology of Hs, Tm02, and direction (mean ± IQR) ---\n",
    "\n",
    "# inputs\n",
    "need  = {\"hs\", \"Tm02\", \"dir\"}\n",
    "if not need.issubset(data.columns):\n",
    "    raise KeyError(f\"IA.3 needs columns {sorted(need)} in `data`.\")\n",
    "\n",
    "df3 = data.loc[start:end, [\"hs\", \"Tm02\", \"dir\"]].copy()\n",
    "if not isinstance(df3.index, pd.DatetimeIndex):\n",
    "    raise TypeError(\"`data` index must be a DatetimeIndex.\")\n",
    "\n",
    "# group by calendar month across all years\n",
    "g = df3.groupby(df3.index.month)  # 1..12\n",
    "\n",
    "# numeric climatology for Hs and Tm02\n",
    "hs_mean = g[\"hs\"].mean()\n",
    "hs_q25  = g[\"hs\"].quantile(0.25)\n",
    "hs_q75  = g[\"hs\"].quantile(0.75)\n",
    "\n",
    "tm_mean = g[\"Tm02\"].mean()\n",
    "tm_q25  = g[\"Tm02\"].quantile(0.25)\n",
    "tm_q75  = g[\"Tm02\"].quantile(0.75)\n",
    "\n",
    "n_samples = g.size()\n",
    "\n",
    "# circular climatology for direction (vectorised; stable across pandas versions)\n",
    "dd = df3[[\"dir\"]].copy()\n",
    "m = dd[\"dir\"].notna()\n",
    "dd.loc[m, \"sin\"] = np.sin(np.deg2rad(dd.loc[m, \"dir\"]))\n",
    "dd.loc[m, \"cos\"] = np.cos(np.deg2rad(dd.loc[m, \"dir\"]))\n",
    "\n",
    "dg = dd.groupby(dd.index.month)\n",
    "sin_mean = dg[\"sin\"].mean()\n",
    "cos_mean = dg[\"cos\"].mean()\n",
    "n_dir    = dg[\"dir\"].count()\n",
    "\n",
    "R = np.hypot(sin_mean, cos_mean)                                  # mean resultant length\n",
    "dir_mean = (np.degrees(np.arctan2(sin_mean, cos_mean)) + 360) % 360\n",
    "dir_cstd = np.degrees(np.sqrt(np.maximum(0.0, -2.0 * np.log(np.clip(R, 1e-12, 1.0)))))  # circular std (deg)\n",
    "\n",
    "# unwrap monthly direction around overall circular mean to avoid 0/360 jump\n",
    "sin_all = np.sin(np.deg2rad(df3[\"dir\"].dropna())).mean()\n",
    "cos_all = np.cos(np.deg2rad(df3[\"dir\"].dropna())).mean()\n",
    "overall_dir = (np.degrees(np.arctan2(sin_all, cos_all)) + 360) % 360\n",
    "dir_mean_unwrapped = overall_dir + ((dir_mean - overall_dir + 540.0) % 360.0 - 180.0)\n",
    "\n",
    "# assemble single DataFrame and ensure months 1..12 exist and in order\n",
    "clim = pd.DataFrame({\n",
    "    \"hs_mean\": hs_mean,\n",
    "    \"hs_q25\":  hs_q25,\n",
    "    \"hs_q75\":  hs_q75,\n",
    "    \"Tm02_mean\": tm_mean,\n",
    "    \"Tm02_q25\":  tm_q25,\n",
    "    \"Tm02_q75\":  tm_q75,\n",
    "    \"N_samples\": n_samples,\n",
    "    \"dir_mean\": dir_mean,\n",
    "    \"R\": R,\n",
    "    \"dir_cstd\": dir_cstd,\n",
    "    \"n_dir\": n_dir,\n",
    "    \"dir_mean_unwrapped\": dir_mean_unwrapped,\n",
    "}).reindex(range(1, 13))\n",
    "clim.index.name = \"month\"\n",
    "clim[\"month_name\"] = pd.to_datetime(clim.index, format=\"%m\").month_name().str.slice(0, 3)\n",
    "\n",
    "# save numeric table\n",
    "clim.to_csv(\"fig/IA3_seasonal_climatology.csv\", float_format=\"%.3f\")\n",
    "\n",
    "# plot\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 9), sharex=True)\n",
    "x = np.arange(1, 13)\n",
    "labels = clim[\"month_name\"].values\n",
    "\n",
    "# Hs: mean ± IQR\n",
    "ax = axes[0]\n",
    "ax.plot(x, clim[\"hs_mean\"].values, lw=1.8)\n",
    "ax.fill_between(x, clim[\"hs_q25\"].values, clim[\"hs_q75\"].values, alpha=0.25)\n",
    "ax.set_ylabel(\"Hs (m)\")\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_title(\"Monthly climatology: Hs (mean ± IQR)\")\n",
    "\n",
    "# Tm02: mean ± IQR\n",
    "ax = axes[1]\n",
    "ax.plot(x, clim[\"Tm02_mean\"].values, lw=1.8)\n",
    "ax.fill_between(x, clim[\"Tm02_q25\"].values, clim[\"Tm02_q75\"].values, alpha=0.25)\n",
    "ax.set_ylabel(\"Tm02 (s)\")\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_title(\"Monthly climatology: Tm02 (mean ± IQR)\")\n",
    "\n",
    "# Direction: circular mean ± circular std (unwrapped for continuity)\n",
    "ax = axes[2]\n",
    "y = clim[\"dir_mean_unwrapped\"].values\n",
    "yerr = clim[\"dir_cstd\"].values\n",
    "ax.errorbar(x, y, yerr=yerr, fmt=\"-o\", lw=1.5, capsize=3)\n",
    "pad = 5.0\n",
    "ymin, ymax = np.nanmin(y - yerr), np.nanmax(y + yerr)\n",
    "ax.set_ylim(ymin - pad, ymax + pad)\n",
    "ax.set_ylabel(\"Direction (deg, coming-from)\")\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_title(\"Monthly climatology: wave direction (circular mean ± circular std)\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fig/IA3_seasonal_climatology.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# quick textual summary\n",
    "hs_max_mo = int(clim[\"hs_mean\"].idxmax())\n",
    "hs_min_mo = int(clim[\"hs_mean\"].idxmin())\n",
    "tm_max_mo = int(clim[\"Tm02_mean\"].idxmax())\n",
    "tm_min_mo = int(clim[\"Tm02_mean\"].idxmin())\n",
    "print(\"IA.3 — Seasonal climatology (1994–2020)\")\n",
    "print(f\"  Hs peaks in {pd.to_datetime(hs_max_mo, format='%m').month_name()} \"\n",
    "      f\"and is lowest in {pd.to_datetime(hs_min_mo, format='%m').month_name()}.\")\n",
    "print(f\"  Tm02 peaks in {pd.to_datetime(tm_max_mo, format='%m').month_name()} \"\n",
    "      f\"and is lowest in {pd.to_datetime(tm_min_mo, format='%m').month_name()}.\")\n",
    "print(f\"  Direction mean (overall) ≈ {overall_dir:.1f}° (coming-from).\")\n",
    "print(\"  Saved: fig/IA3_seasonal_climatology.png and fig/IA3_seasonal_climatology.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "\n",
    "**Q: Do you observe any seasonal trends in wave height, period, or direction? If so, why?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "**Significant wave height, $H_s$.** Clear winter maximum and summer minimum. IQR widens in winter and narrows in summer.  \n",
    "**Interpretation.** Boreal-winter extratropical cyclones increase wind speed and fetch over the open North Atlantic, augmenting wind-sea and incoming swell towards south Brittany. Summer conditions are dominated by weaker winds and shorter fetch, so mean $H_s$ decreases and variability contracts. Because wave energy density scales as\n",
    "$$\n",
    "E=\\tfrac{1}{8}\\,\\rho g\\,H_s^2,\n",
    "$$\n",
    "higher winter $H_s$ implies markedly greater wave energy arriving at the site.\n",
    "\n",
    "**Mean zero-crossing period, $T_{m02}$.** Co-varies with $H_s$: longer in winter, shorter in summer.  \n",
    "**Interpretation.** Winter storms generate longer-period swell over long fetches. Deep-water dispersion $\\big(L\\approx gT^2/2\\pi,\\ c_g \\approx gT/4\\pi\\big)$ favours the far-field propagation of longer-period energy into the Bay of Biscay. In summer, local wind-sea contribution increases and typical periods shorten.\n",
    "\n",
    "### Wave direction (coming-from, clockwise from North)\n",
    "\n",
    "**Convention.** $0^\\circ=\\mathrm{N},\\ 90^\\circ=\\mathrm{E},\\ 180^\\circ=\\mathrm{S},\\ 270^\\circ=\\mathrm{W}$.\n",
    "\n",
    "**Seasonal pattern.** The monthly circular mean increases from winter ($\\sim 270^\\circ$) to summer ($\\sim 280^\\circ$), then decreases again in autumn.\n",
    "\n",
    "**Interpretation** Larger angles imply a more westerly approach. Thus, waves are slightly more **W–WSW** in summer and shift a little toward **SW** in winter.\n",
    "\n",
    "**Regional context.** Along the southern Brittany–Bay of Biscay sector, the summer expansion of the Azores High favours a more zonal (westerly) approach at the shelf break. In winter, frequent lows entering the Bay introduce a modest southerly component in the incident swell, yielding the observed decrease in direction angle.\n",
    "\n",
    "**Dispersion note.** The vertical bars are circular standard deviations (spread of hourly directions), not standard errors. They are of similar magnitude across months, so the dataset does **not** support a strong seasonal change in directional spread.\n",
    "\n",
    "## Implications\n",
    "\n",
    "- **Operations.** Highest loads and sea states occur in winter months; scheduling for installation or maintenance is more feasible in late spring–summer.  \n",
    "- **Variability.** Broad winter IQRs indicate stronger interannual modulation of sea states; design and planning should not rely on a single “typical” winter value.  \n",
    "- **Directionality.** The prevailing W–WSW approach is stable enough to justify directional binning around that sector for further analyses and for extreme value modelling in Part II.\n",
    "\n",
    "## Notes and caveats\n",
    "\n",
    "- Monthly means smooth synoptic extremes; use EVA for design loads.  \n",
    "- Direction statistics use circular metrics; reported means reflect modal approach rather than arithmetic averages near $0/360^\\circ$.  \n",
    "- The site is predominantly deep water, so seasonal patterns reflect atmospheric forcing and basin geometry rather than depth-limited effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### I.A.4. Mean Wind and Current Conditions\n",
    "\n",
    "We will also evaluate the wind and current conditions by plotting rose diagrams of their respective velocities and directions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IA.4 Mean wind and current conditions: roses + means (overall, seasonal) ---\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "# Calm thresholds used for ROSES only (kept in means table for reference)\n",
    "CALM_WIND = 0.5    # m/s\n",
    "CALM_CURR = 0.05   # m/s\n",
    "\n",
    "# Speed bins\n",
    "# Wind: [0.5–2), 2–4, 4–6, …, ≥16 m/s\n",
    "WIND_BINS = np.r_[0.5, np.arange(2, 18, 2), np.inf]\n",
    "# Currents: [0.05–0.1), 0.1–0.2, 0.2–0.3, 0.3–0.5, ≥0.5 m/s\n",
    "CURR_BINS = np.array([0.05, 0.10, 0.15, 0.2, np.inf])\n",
    "\n",
    "# Direction sectors\n",
    "WIND_SECTORS = 16   # 22.5°\n",
    "CURR_SECTORS = 36   # 10°\n",
    "\n",
    "# Date window to match the project\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def gradient_colors(cmap_name, n, start=0.25, end=0.9):\n",
    "    \"\"\"Return a single-hue gradient sampled from a Matplotlib colormap.\"\"\"\n",
    "    cmap = plt.get_cmap(cmap_name)\n",
    "    if n <= 1:\n",
    "        return np.array([cmap((start + end) / 2.0)])\n",
    "    return cmap(np.linspace(start, end, n))\n",
    "\n",
    "def season_label(month):\n",
    "    # DJF, MAM, JJA, SON\n",
    "    if month in (12, 1, 2):\n",
    "        return \"DJF\"\n",
    "    if month in (3, 4, 5):\n",
    "        return \"MAM\"\n",
    "    if month in (6, 7, 8):\n",
    "        return \"JJA\"\n",
    "    return \"SON\"\n",
    "\n",
    "def rose_counts(dir_deg, spd, spd_bins, sectors=16, calm_thresh=0.0):\n",
    "    \"\"\"\n",
    "    Return stacked counts per direction sector and speed bin, plus metadata.\n",
    "    Directions: degrees, 0°=N, clockwise positive.\n",
    "    Excludes values with spd <= calm_thresh.\n",
    "    \"\"\"\n",
    "    mask = np.isfinite(dir_deg) & np.isfinite(spd) & (spd > calm_thresh)\n",
    "    d = np.asarray(dir_deg)[mask] % 360.0\n",
    "    s = np.asarray(spd)[mask]\n",
    "\n",
    "    if d.size == 0:\n",
    "        counts = np.zeros((sectors, len(spd_bins)-1), dtype=int)\n",
    "        return counts, np.array([]), 0\n",
    "\n",
    "    width = 360.0 / sectors\n",
    "    sector_idx = np.floor(d / width).astype(int)\n",
    "    sector_idx[sector_idx == sectors] = sectors - 1\n",
    "\n",
    "    bin_idx = np.digitize(s, spd_bins) - 1\n",
    "    bin_idx = np.clip(bin_idx, 0, len(spd_bins)-2)\n",
    "\n",
    "    counts = np.zeros((sectors, len(spd_bins)-1), dtype=int)\n",
    "    for k in range(d.size):\n",
    "        counts[sector_idx[k], bin_idx[k]] += 1\n",
    "\n",
    "    return counts, mask, d.size  # d.size = included samples after calm filter\n",
    "\n",
    "def plot_rose(dir_deg, spd, spd_bins, fname, title, calm_thresh=0.0, sectors=16, cmap_name=\"Reds\"):\n",
    "    \"\"\"\n",
    "    Polar stacked-bar rose. Heights are percentages of included samples.\n",
    "    Direction convention: 0° at North, clockwise positive.\n",
    "    \"\"\"\n",
    "    counts, mask, n_used = rose_counts(dir_deg, spd, spd_bins, sectors, calm_thresh)\n",
    "    if n_used == 0:\n",
    "        print(f\"{title}: no data above calm threshold.\")\n",
    "        return\n",
    "\n",
    "    sector_totals = counts.sum(axis=1).astype(float)\n",
    "    sector_totals[sector_totals == 0] = 1.0\n",
    "    frac = counts / sector_totals[:, None]\n",
    "    pct_sector = 100.0 * counts.sum(axis=1) / n_used\n",
    "\n",
    "    theta = np.deg2rad(np.arange(0, 360, 360/sectors))\n",
    "    width = 2.0 * np.pi / sectors\n",
    "    colors = gradient_colors(cmap_name, len(spd_bins) - 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    ax.set_theta_zero_location(\"N\")\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    bottoms = np.zeros(sectors)\n",
    "    handles = []\n",
    "    labels = []\n",
    "    for j in range(len(spd_bins)-1):\n",
    "        label = f\"{spd_bins[j]:g}–{spd_bins[j+1]:g} m/s\" if np.isfinite(spd_bins[j+1]) else f\"{spd_bins[j]:g}+ m/s\"\n",
    "        labels.append(label)\n",
    "        heights = pct_sector * frac[:, j]\n",
    "        bars = ax.bar(theta, heights, width=width, bottom=bottoms, align=\"edge\",\n",
    "               edgecolor=\"black\", linewidth=0.3, color=colors[j])\n",
    "        handles.append(bars[0])\n",
    "        bottoms += heights\n",
    "\n",
    "    ax.set_rlabel_position(225)\n",
    "    rmax = max(5.0, np.ceil(bottoms.max() / 5.0) * 5.0)\n",
    "    ax.set_ylim(0, rmax)\n",
    "    ax.set_yticks(np.linspace(0, rmax, 5))\n",
    "    ax.set_yticklabels([f\"{v:.0f}%\" for v in np.linspace(0, rmax, 5)])\n",
    "\n",
    "    ax.set_title(title + f\"\\n(calms ≤ {calm_thresh} m/s excluded; n={n_used})\",\n",
    "                 va=\"bottom\")\n",
    "    ax.legend(handles, labels, loc=\"lower left\", bbox_to_anchor=(0.9, -0.02),\n",
    "              frameon=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def mean_table(dir_deg, spd, label, calm_thresh=None):\n",
    "    \"\"\"\n",
    "    Mean speed (arithmetic) and mean direction (circular) overall and by season.\n",
    "    Directions in degrees [0, 360).\n",
    "    calm_thresh listed for reference; does not alter means.\n",
    "    \"\"\"\n",
    "    idx = data.loc[start:end].index\n",
    "    df = pd.DataFrame({\"spd\": spd.loc[idx], \"dir\": dir_deg.loc[idx]})\n",
    "    df[\"season\"] = df.index.month.map(season_label)\n",
    "\n",
    "    def agg(g):\n",
    "        return pd.Series({\n",
    "            \"mean_speed_mps\": g[\"spd\"].mean(),\n",
    "            \"mean_dir_deg\": circmean_deg(g[\"dir\"]),\n",
    "            \"n\": g[\"dir\"].count()\n",
    "        })\n",
    "\n",
    "    overall = agg(df)\n",
    "    by_season = df.groupby(\"season\", sort=False).apply(agg)\n",
    "\n",
    "    overall.name = label\n",
    "    out = {\"overall\": overall.to_frame().T, \"season\": by_season}\n",
    "    out[\"meta\"] = {\"calm_threshold_listed_only_mps\": calm_thresh}\n",
    "    return out\n",
    "\n",
    "# -----------------------\n",
    "# Prepare series and conventions\n",
    "# -----------------------\n",
    "# Wind: already \"coming-from\" via zmcomp2metconv\n",
    "wspd = data[\"wspd\"].copy()\n",
    "wdir_from = data[\"wdir\"].copy()  # coming-from, 0°=N, clockwise\n",
    "\n",
    "# Currents: convert to \"going-to\" for the rose\n",
    "if {\"cspd\", \"cdir\"}.issubset(data.columns):\n",
    "    cspd = data[\"cspd\"].copy()\n",
    "    cdir_to = (data[\"cdir\"] + 180.0) % 360.0\n",
    "else:\n",
    "    cspd = None\n",
    "    cdir_to = None\n",
    "\n",
    "# Restrict to analysis window\n",
    "sel = slice(start, end)\n",
    "wspd = wspd.loc[sel]\n",
    "wdir_from = wdir_from.loc[sel]\n",
    "if cspd is not None:\n",
    "    cspd = cspd.loc[sel]\n",
    "    cdir_to = cdir_to.loc[sel]\n",
    "\n",
    "# -----------------------\n",
    "# 1) Full-period roses\n",
    "# -----------------------\n",
    "plot_rose(\n",
    "    dir_deg=wdir_from, spd=wspd, spd_bins=WIND_BINS,\n",
    "    fname=\"fig/IA4_wind_rose.png\",\n",
    "    title=\"Wind rose (coming-from; 0°=N, clockwise)\",\n",
    "    calm_thresh=CALM_WIND, sectors=WIND_SECTORS, cmap_name=\"Reds\"\n",
    ")\n",
    "\n",
    "if cspd is not None:\n",
    "    plot_rose(\n",
    "        dir_deg=cdir_to, spd=cspd, spd_bins=CURR_BINS,\n",
    "        fname=\"fig/IA4_current_rose.png\",\n",
    "        title=\"Current rose (going-to; 0°=N, clockwise)\",\n",
    "        calm_thresh=CALM_CURR, sectors=CURR_SECTORS, cmap_name=\"Purples\"\n",
    "    )\n",
    "\n",
    "# -----------------------\n",
    "# 2) Seasonal roses (DJF, MAM, JJA, SON)\n",
    "# -----------------------\n",
    "def plot_rose_seasons(dir_series, spd_series, spd_bins, calm_thresh, sectors, title_base, fname, cmap_name=\"Reds\"):\n",
    "    seasons = [\"DJF\", \"MAM\", \"JJA\", \"SON\"]\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10), subplot_kw=dict(polar=True))\n",
    "    axes = axes.ravel()\n",
    "    colors = gradient_colors(cmap_name, len(spd_bins) - 1)\n",
    "    for i, s in enumerate(seasons):\n",
    "        sel = dir_series.index.map(lambda t: season_label(t.month) == s)\n",
    "        d = dir_series[sel]\n",
    "        v = spd_series[sel]\n",
    "        counts, mask, n_used = rose_counts(d.values, v.values, spd_bins, sectors, calm_thresh)\n",
    "        theta = np.deg2rad(np.arange(0, 360, 360/sectors))\n",
    "        width = 2.0 * np.pi / sectors\n",
    "\n",
    "        ax = axes[i]\n",
    "        ax.set_theta_zero_location(\"N\")\n",
    "        ax.set_theta_direction(-1)\n",
    "\n",
    "        if n_used == 0:\n",
    "            ax.set_title(f\"{s} (n=0)\")\n",
    "            continue\n",
    "\n",
    "        sector_totals = counts.sum(axis=1).astype(float)\n",
    "        sector_totals[sector_totals == 0] = 1.0\n",
    "        frac = counts / sector_totals[:, None]\n",
    "        pct_sector = 100.0 * counts.sum(axis=1) / n_used\n",
    "\n",
    "        bottoms = np.zeros(sectors)\n",
    "        for j in range(len(spd_bins)-1):\n",
    "            heights = pct_sector * frac[:, j]\n",
    "            ax.bar(theta, heights, width=width, bottom=bottoms, align=\"edge\",\n",
    "                   edgecolor=\"black\", linewidth=0.3, color=colors[j])\n",
    "            bottoms += heights\n",
    "\n",
    "        ax.set_rlabel_position(225)\n",
    "        rmax = max(5.0, np.ceil(bottoms.max() / 5.0) * 5.0)\n",
    "        ax.set_ylim(0, rmax)\n",
    "        ax.set_yticks(np.linspace(0, rmax, 5))\n",
    "        ax.set_yticklabels([f\"{v:.0f}%\" for v in np.linspace(0, rmax, 5)])\n",
    "        ax.set_title(f\"{s} (calms ≤ {calm_thresh} m/s; n={n_used})\")\n",
    "\n",
    "    fig.suptitle(title_base, y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0.02, 1, 0.95])\n",
    "    plt.savefig(fname, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_rose_seasons(\n",
    "    wdir_from, wspd, WIND_BINS, CALM_WIND, WIND_SECTORS,\n",
    "    \"Wind roses by season (coming-from; 0°=N, clockwise)\",\n",
    "    \"fig/IA4_wind_rose_seasons.png\", cmap_name=\"Reds\"\n",
    ")\n",
    "\n",
    "if cspd is not None:\n",
    "    plot_rose_seasons(\n",
    "        cdir_to, cspd, CURR_BINS, CALM_CURR, CURR_SECTORS,\n",
    "        \"Current roses by season (going-to; 0°=N, clockwise)\",\n",
    "        \"fig/IA4_current_rose_seasons.png\", cmap_name=\"Purples\"\n",
    "    )\n",
    "\n",
    "# -----------------------\n",
    "# 3) Mean conditions tables and CSV\n",
    "# -----------------------\n",
    "wind_stats = mean_table(wdir_from, wspd, label=\"wind\", calm_thresh=CALM_WIND)\n",
    "if cspd is not None:\n",
    "    curr_stats = mean_table(cdir_to, cspd, label=\"current\", calm_thresh=CALM_CURR)\n",
    "\n",
    "frames = []\n",
    "wind_overall = wind_stats[\"overall\"].assign(kind=\"wind\")\n",
    "wind_season  = wind_stats[\"season\"].assign(kind=\"wind\", level=\"season\").reset_index().rename(columns={\"season\":\"group\"})\n",
    "frames += [wind_overall.assign(level=\"overall\", group=\"all\"), wind_season]\n",
    "\n",
    "if cspd is not None:\n",
    "    curr_overall = curr_stats[\"overall\"].assign(kind=\"current\")\n",
    "    curr_season  = curr_stats[\"season\"].assign(kind=\"current\", level=\"season\").reset_index().rename(columns={\"season\":\"group\"})\n",
    "    frames += [curr_overall.assign(level=\"overall\", group=\"all\"), curr_season]\n",
    "\n",
    "stats_df = pd.concat(frames, ignore_index=True)\n",
    "stats_df = stats_df[[\"kind\", \"level\", \"group\", \"mean_speed_mps\", \"mean_dir_deg\", \"n\"]]\n",
    "stats_df.to_csv(\"fig/IA4_mean_wind_current_stats.csv\", index=False, float_format=\"%.3f\")\n",
    "\n",
    "# Print seasonal values for wind and current\n",
    "\n",
    "season_order = CategoricalDtype([\"DJF\", \"MAM\", \"JJA\", \"SON\"], ordered=True)\n",
    "\n",
    "def print_seasonals(kind_label, dir_note):\n",
    "    df = stats_df.query(\"kind == @kind_label and level == 'season'\").copy()\n",
    "    df[\"group\"] = df[\"group\"].astype(season_order)\n",
    "    df = df.sort_values(\"group\")\n",
    "    print(f\"\\nIA.4 — Seasonal means, {kind_label} ({dir_note})\")\n",
    "    print(df[[\"group\", \"mean_speed_mps\", \"mean_dir_deg\", \"n\"]]\n",
    "          .rename(columns={\"group\": \"season\"})\n",
    "          .to_string(index=False,\n",
    "                     formatters={\n",
    "                         \"mean_speed_mps\": lambda v: f\"{v:.3f}\",\n",
    "                         \"mean_dir_deg\":  lambda v: f\"{v:.1f}\",\n",
    "                         \"n\":             lambda v: f\"{int(v)}\"\n",
    "                     }))\n",
    "\n",
    "print_seasonals(\"wind\", \"coming-from\")\n",
    "print_seasonals(\"current\", \"going-to\")\n",
    "\n",
    "print(\"IA.4 — Mean conditions (1994–2020)\")\n",
    "print(stats_df.query(\"level == 'overall'\"))\n",
    "print(\"\\nSaved figures:\")\n",
    "print(\"  fig/IA4_wind_rose.png\")\n",
    "if cspd is not None:\n",
    "    print(\"  fig/IA4_current_rose.png\")\n",
    "print(\"  fig/IA4_wind_rose_seasons.png\")\n",
    "if cspd is not None:\n",
    "    print(\"  fig/IA4_current_rose_seasons.png\")\n",
    "print(\"Saved table: fig/IA4_mean_wind_current_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IA.4 Monthly climatology: wind & current (mean ± IQR; direction circular mean ± std) ---\n",
    "need_wind = {\"wspd\", \"wdir\"}\n",
    "curr_need = {\"cspd\", \"cdir\"}\n",
    "missing = need_wind - set(data.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"IA.4 monthly climatology missing columns {sorted(missing)} in `data`.\")\n",
    "\n",
    "has_currents = curr_need.issubset(data.columns)\n",
    "cols = [\"wspd\", \"wdir\"] + ([\"cspd\", \"cdir\"] if has_currents else [])\n",
    "df_wc = data.loc[start:end, cols].copy()\n",
    "if not isinstance(df_wc.index, pd.DatetimeIndex):\n",
    "    raise TypeError(\"`data` index must be a DatetimeIndex.\")\n",
    "\n",
    "months = pd.Index(range(1, 13), name=\"month\")\n",
    "month_group = df_wc.groupby(df_wc.index.month)\n",
    "\n",
    "wspd_mean = month_group[\"wspd\"].mean().reindex(months)\n",
    "wspd_q25 = month_group[\"wspd\"].quantile(0.25).reindex(months)\n",
    "wspd_q75 = month_group[\"wspd\"].quantile(0.75).reindex(months)\n",
    "\n",
    "if has_currents:\n",
    "    cspd_mean = month_group[\"cspd\"].mean().reindex(months)\n",
    "    cspd_q25 = month_group[\"cspd\"].quantile(0.25).reindex(months)\n",
    "    cspd_q75 = month_group[\"cspd\"].quantile(0.75).reindex(months)\n",
    "\n",
    "def _direction_series(series):\n",
    "    df = series.to_frame(name=\"theta\")\n",
    "    mask = df[\"theta\"].notna()\n",
    "    df.loc[mask, \"sin\"] = np.sin(np.deg2rad(df.loc[mask, \"theta\"]))\n",
    "    df.loc[mask, \"cos\"] = np.cos(np.deg2rad(df.loc[mask, \"theta\"]))\n",
    "    group = df.groupby(df.index.month)\n",
    "    sin_mean = group[\"sin\"].mean()\n",
    "    cos_mean = group[\"cos\"].mean()\n",
    "    counts = group[\"theta\"].count()\n",
    "    R = np.hypot(sin_mean, cos_mean)\n",
    "    mean = (np.degrees(np.arctan2(sin_mean, cos_mean)) + 360.0) % 360.0\n",
    "    cstd = np.degrees(np.sqrt(np.maximum(0.0, -2.0 * np.log(np.clip(R, 1e-12, 1.0)))))\n",
    "    mean = mean.where(counts > 0)\n",
    "    cstd = cstd.where(counts > 0)\n",
    "    R = R.where(counts > 0)\n",
    "    return mean, cstd, R, counts\n",
    "\n",
    "wdir_mean, wdir_cstd, wdir_R, wdir_n = _direction_series(df_wc[\"wdir\"])\n",
    "wdir_mean = wdir_mean.reindex(months)\n",
    "wdir_cstd = wdir_cstd.reindex(months)\n",
    "wdir_R = wdir_R.reindex(months)\n",
    "wdir_n = wdir_n.reindex(months)\n",
    "wdir_raw = df_wc[\"wdir\"].dropna()\n",
    "if wdir_raw.empty:\n",
    "    wind_overall_dir = np.nan\n",
    "    wdir_mean_unwrapped = wdir_mean.copy()\n",
    "else:\n",
    "    sin_all = np.sin(np.deg2rad(wdir_raw)).mean()\n",
    "    cos_all = np.cos(np.deg2rad(wdir_raw)).mean()\n",
    "    wind_overall_dir = (np.degrees(np.arctan2(sin_all, cos_all)) + 360.0) % 360.0\n",
    "    wdir_mean_unwrapped = wind_overall_dir + ((wdir_mean - wind_overall_dir + 540.0) % 360.0 - 180.0)\n",
    "\n",
    "if has_currents:\n",
    "    cdir_mean, cdir_cstd, cdir_R, cdir_n = _direction_series(df_wc[\"cdir\"])\n",
    "    cdir_mean = cdir_mean.reindex(months)\n",
    "    cdir_cstd = cdir_cstd.reindex(months)\n",
    "    cdir_R = cdir_R.reindex(months)\n",
    "    cdir_n = cdir_n.reindex(months)\n",
    "    cdir_raw = df_wc[\"cdir\"].dropna()\n",
    "    if cdir_raw.empty:\n",
    "        curr_overall_dir = np.nan\n",
    "        cdir_mean_unwrapped = cdir_mean.copy()\n",
    "    else:\n",
    "        sin_all = np.sin(np.deg2rad(cdir_raw)).mean()\n",
    "        cos_all = np.cos(np.deg2rad(cdir_raw)).mean()\n",
    "        curr_overall_dir = (np.degrees(np.arctan2(sin_all, cos_all)) + 360.0) % 360.0\n",
    "        cdir_mean_unwrapped = curr_overall_dir + ((cdir_mean - curr_overall_dir + 540.0) % 360.0 - 180.0)\n",
    "\n",
    "clim = pd.DataFrame(index=months)\n",
    "clim[\"month_name\"] = pd.to_datetime(clim.index, format=\"%m\").month_name().str.slice(0, 3)\n",
    "clim[\"wspd_mean\"] = wspd_mean\n",
    "clim[\"wspd_q25\"] = wspd_q25\n",
    "clim[\"wspd_q75\"] = wspd_q75\n",
    "clim[\"wdir_mean\"] = wdir_mean\n",
    "clim[\"wdir_mean_unwrapped\"] = wdir_mean_unwrapped\n",
    "clim[\"wdir_cstd\"] = wdir_cstd\n",
    "clim[\"wdir_R\"] = wdir_R\n",
    "clim[\"wdir_n\"] = wdir_n\n",
    "\n",
    "if has_currents:\n",
    "    clim[\"cspd_mean\"] = cspd_mean\n",
    "    clim[\"cspd_q25\"] = cspd_q25\n",
    "    clim[\"cspd_q75\"] = cspd_q75\n",
    "    clim[\"cdir_mean\"] = cdir_mean\n",
    "    clim[\"cdir_mean_unwrapped\"] = cdir_mean_unwrapped\n",
    "    clim[\"cdir_cstd\"] = cdir_cstd\n",
    "    clim[\"cdir_R\"] = cdir_R\n",
    "    clim[\"cdir_n\"] = cdir_n\n",
    "\n",
    "clim.to_csv(\"fig/IA4_wind_current_monthly_climatology.csv\", float_format=\"%.3f\")\n",
    "\n",
    "x = np.arange(1, 13)\n",
    "labels = clim[\"month_name\"].tolist()\n",
    "n_rows = 4 if has_currents else 2\n",
    "fig, axes = plt.subplots(n_rows, 1, figsize=(12, 3 * n_rows), sharex=True)\n",
    "axes = np.atleast_1d(axes)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(x, clim[\"wspd_mean\"], lw=1.8)\n",
    "ax.fill_between(x, clim[\"wspd_q25\"], clim[\"wspd_q75\"], alpha=0.25)\n",
    "ax.set_ylabel(\"Wind speed (m/s)\")\n",
    "ax.set_title(\"Monthly climatology: wind speed (mean ± IQR)\")\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "y = clim[\"wdir_mean_unwrapped\"].to_numpy()\n",
    "yerr = clim[\"wdir_cstd\"].to_numpy()\n",
    "ax.errorbar(x, y, yerr=yerr, fmt=\"-o\", capsize=3, lw=1.5)\n",
    "if np.isfinite(y).any():\n",
    "    lower = y - np.nan_to_num(yerr, nan=0.0)\n",
    "    upper = y + np.nan_to_num(yerr, nan=0.0)\n",
    "    finite = np.isfinite(lower) & np.isfinite(upper)\n",
    "    if finite.any():\n",
    "        pad = 5.0\n",
    "        ax.set_ylim(lower[finite].min() - pad, upper[finite].max() + pad)\n",
    "ax.set_ylabel(\"Wind direction (deg)\")\n",
    "ax.set_title(\"Monthly climatology: wind direction (circular mean ± std)\")\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "if has_currents:\n",
    "    ax = axes[2]\n",
    "    ax.plot(x, clim[\"cspd_mean\"], lw=1.8)\n",
    "    ax.fill_between(x, clim[\"cspd_q25\"], clim[\"cspd_q75\"], alpha=0.25)\n",
    "    ax.set_ylabel(\"Current speed (m/s)\")\n",
    "    ax.set_title(\"Monthly climatology: current speed (mean ± IQR)\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    ax = axes[3]\n",
    "    y = clim[\"cdir_mean_unwrapped\"].to_numpy()\n",
    "    yerr = clim[\"cdir_cstd\"].to_numpy()\n",
    "    ax.errorbar(x, y, yerr=yerr, fmt=\"-o\", capsize=3, lw=1.5)\n",
    "    if np.isfinite(y).any():\n",
    "        lower = y - np.nan_to_num(yerr, nan=0.0)\n",
    "        upper = y + np.nan_to_num(yerr, nan=0.0)\n",
    "        finite = np.isfinite(lower) & np.isfinite(upper)\n",
    "        if finite.any():\n",
    "            pad = 5.0\n",
    "            ax.set_ylim(lower[finite].min() - pad, upper[finite].max() + pad)\n",
    "    ax.set_ylabel(\"Current direction (deg)\")\n",
    "    ax.set_title(\"Monthly climatology: current direction (circular mean ± std)\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "axes[-1].set_xticks(x)\n",
    "axes[-1].set_xticklabels(labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fig/IA4_wind_current_monthly_climatology.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"IA.4 — Monthly wind/current climatology saved to fig/IA4_wind_current_monthly_climatology.(csv|png)\")\n",
    "if has_currents:\n",
    "    print(\"  Included both wind and current statistics.\")\n",
    "else:\n",
    "    print(\"  Current data unavailable; plotted wind only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "**Q: What are the mean conditions (e.g. velocity and direction)? (if you have time: do you observe any seasonal variability?)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### IA.4  Interpretation of wind and current conditions\n",
    "\n",
    "\n",
    "**Answer.** Winds are W-WNW year-round and strongest in winter (overall mean 7.22 m/s; DJF mean 8.67 m/s). Currents are weak (about 0.10 m/s) and dominantly tidal along a NE-SW axis, so the overall mean direction is not physically representative.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Data basis and conventions\n",
    "\n",
    "\n",
    "- Period analysed: 1994-2020.  \n",
    "- Calms excluded in roses: wind $<0.5$ m/s, current $<0.05$ m/s.  \n",
    "- Wind: **coming-from**; Current: **going-to**; $0^\\circ=\\text{N}$, clockwise.  \n",
    "- Bins used: wind speeds $[0.5,2),[2,4),\\ldots,\\ge 16$ m/s; current speeds $[0.05,0.10),[0.10,0.15),[0.15,0.20),\\ge 0.20$ m/s; wind sectors $22.5^\\circ$; current sectors $10^\\circ$.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Summary statistics\n",
    "\n",
    "\n",
    "**Overall means (1994-2020)**\n",
    "\n",
    "\n",
    "| Variable | Mean speed | Mean direction |\n",
    "|---|---:|---:|\n",
    "| Wind (coming-from) | 7.22 m/s | 296.6 |\n",
    "| Current (going-to) | 0.096 m/s | 285.2* |\n",
    "\n",
    "\n",
    "\\*For reversing tidal currents, a single circular mean direction is not physically informative.\n",
    "\n",
    "\n",
    "**Seasonal means**\n",
    "\n",
    "\n",
    "| Season | Wind speed (m/s) | Wind dir (deg) | Current speed (m/s) | Current dir (deg)* |\n",
    "|---|---:|---:|---:|---:|\n",
    "| DJF | 8.67 | 266.3 | 0.096 | 211.3 |\n",
    "| MAM | 6.95 | 321.1 | 0.096 | 28.3 |\n",
    "| JJA | 6.00 | 302.6 | 0.096 | 228.5 |\n",
    "| SON | 7.31 | 287.5 | 0.096 | 19.0 |\n",
    "\n",
    "\n",
    "\\*Seasonal current means are shown for completeness but the roses indicate a bidirectional tidal regime.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Wind climate: what the roses show\n",
    "\n",
    "\n",
    "- **Directionality.** Dominant **W-WNW** approach. Seasonal veer is modest: closer to **W** in winter (DJF $\\sim266^\\circ$), rotating towards **WNW** in spring and summer (MAM-JJA $\\sim321^\\circ$ to $\\sim303^\\circ$), then easing back in autumn (SON $\\sim288^\\circ$).\n",
    "- **Intensity.** Clear seasonal cycle: **DJF > SON > MAM > JJA** by mean speed. The full-period rose shows most occurrences in the **4-12 m/s** bands, with winter contributing the higher **8-14 m/s** fractions.\n",
    "- **Variability.** Sectoral spread remains broader in MAM/SON than in DJF/JJA, consistent with synoptic variability during shoulder seasons.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Current climate: what the roses show\n",
    "\n",
    "\n",
    "- **Regime.** Two narrow, opposing **going-to** headings dominate, approximately **NE (30^\\circ)** and **SW (210^\\circ)**, confirming a reversing tidal signal.\n",
    "- **Magnitudes.** Most occurrences lie **below 0.30 m/s**; the highest occupied bin is typically **0.15-0.20 m/s**, with scarce excursions beyond. The mean speed of **0.096 m/s** reflects the prevalence of weak flows.\n",
    "- **Seasonality.** The **axis does not shift** with season, and the speed distribution changes little between DJF and JJA in the roses, highlighting tide-dominated currents with weak seasonal modulation.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Site context notes\n",
    "\n",
    "\n",
    "- The study area is **south of Lorient (southern Brittany)**, exposed to the **open North Atlantic**. The prevailing **W-WNW** winds align with the basin-scale westerlies and frequent winter cyclones.  \n",
    "- The **NE-SW** tidal current axis is consistent with coastal geometry that channels reversing flows; without detailed bathymetry/harmonic analysis, treat this as a qualitative inference from the roses.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Caveats and suggested refinements\n",
    "\n",
    "\n",
    "- **Currents:** Do not use circular means for direction. Prefer a **principal axis** metric and report the **two modal headings** with their shares.  \n",
    "- **Quantiles:** Add $P_{50}$, $P_{90}$, $P_{95}$ for wind and current speeds to complement the means (e.g., $P_{90}$ wind speed $\\approx$ x m/s).  \n",
    "- **Calms:** Roses exclude calms by design; if calm frequency is relevant to operations, report the calm fraction separately.  \n",
    "- **Next step:** If needed for design or logistics, compute a tidal ellipse or principal-component axis for the currents, and provide **hour-of-tide** roses to expose phase dependence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### I.A.5. Comparison to Wave Buoy Measurements\n",
    "\n",
    "We can validate the ResourceCode hindcast data against observations from a nearby wave buoy, for example, from the Candhis website.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "**Q: Comparing the hindcast data from ResourceCode to the observations during the time period with overlapping data, what is the RMSD in the wave height, period, and direction between the observations and simulations?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IA.5 — Candhis buoy validation (Belle-Île 05602, year 2020) ---\n",
    "# Goal: RMSD between ResourceCode hindcast and Candhis observations for Hs, Tm02, and mean direction.\n",
    "# Notes:\n",
    "# - Candhis direction: use THETAM (mean, coming-from, true north, clockwise).\n",
    "# - ResourceCode wave direction assumed coming-from in `data['dir']`. If not, set RC_IS_COMING_FROM=False.\n",
    "# - Time matching: nearest within ±30 min.\n",
    "# - QC: drop sentinels (999.*), non-numeric, |SKEW|>0.3, KURT>5, and (if present) non-valid QUALITE.\n",
    "\n",
    "# ---------- Config ----------\n",
    "RC_IS_COMING_FROM = True          # set to False if your ResourceCode 'dir' is going-to\n",
    "TOL = pd.Timedelta(\"30min\")       # matching tolerance\n",
    "CSV_PATHS = [\n",
    "    Path(\"data/Candhis_05602_2020_arch.csv\"),     # preferred relative path\n",
    "    Path(\"/mnt/data/Candhis_05602_2020_arch.csv\") # fallback (chat attachment)\n",
    "]\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "def _first_existing(paths):\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Candhis 2020 CSV not found at any known path.\")\n",
    "\n",
    "def circ_diff_deg(a_deg, b_deg):\n",
    "    \"\"\"Minimal angular difference a-b in degrees in [-180, 180).\"\"\"\n",
    "    d = (a_deg - b_deg + 180.0) % 360.0 - 180.0\n",
    "    return d\n",
    "\n",
    "def rmse(a, b):\n",
    "    a = np.asarray(a, float)\n",
    "    b = np.asarray(b, float)\n",
    "    m = np.isfinite(a) & np.isfinite(b)\n",
    "    if m.sum() == 0:\n",
    "        return np.nan, 0\n",
    "    return float(np.sqrt(np.mean((a[m] - b[m])**2))), int(m.sum())\n",
    "\n",
    "def circ_rmse_deg(a_deg, b_deg):\n",
    "    \"\"\"RMS of circular differences in degrees.\"\"\"\n",
    "    d = circ_diff_deg(np.asarray(a_deg, float), np.asarray(b_deg, float))\n",
    "    m = np.isfinite(d)\n",
    "    if m.sum() == 0:\n",
    "        return np.nan, 0\n",
    "    return float(np.sqrt(np.mean(d[m]**2))), int(m.sum())\n",
    "\n",
    "# ---------- Load Candhis 2020 ----------\n",
    "candhis_csv = _first_existing(CSV_PATHS)\n",
    "\n",
    "cand = pd.read_csv(\n",
    "    candhis_csv,\n",
    "    sep=\";\", engine=\"python\",\n",
    "    parse_dates=[\"DateHeure\"]\n",
    ")\n",
    "\n",
    "# Keep only fields needed + QC fields\n",
    "need_cols = [\"DateHeure\", \"HM0\", \"T02\", \"THETAM\", \"SKEW\", \"KURT\", \"QUALITE\"]\n",
    "for c in need_cols:\n",
    "    if c not in cand.columns:\n",
    "        # QUALITE may be absent; others must exist\n",
    "        if c == \"QUALITE\":\n",
    "            cand[\"QUALITE\"] = np.nan\n",
    "        else:\n",
    "            raise KeyError(f\"Missing '{c}' in Candhis CSV.\")\n",
    "\n",
    "# Convert to numeric and mark sentinels (e.g., 999.*) as NaN\n",
    "num_cols = [\"HM0\", \"T02\", \"THETAM\", \"SKEW\", \"KURT\"]\n",
    "for c in num_cols:\n",
    "    cand[c] = pd.to_numeric(cand[c], errors=\"coerce\")\n",
    "    # Drop obvious sentinels like 999, 999.999, etc.\n",
    "    cand.loc[cand[c] >= 999.0, c] = np.nan\n",
    "\n",
    "# QC filters\n",
    "qc = pd.Series(True, index=cand.index)\n",
    "\n",
    "# 1) Required numeric fields present\n",
    "qc &= cand[\"HM0\"].notna() & cand[\"T02\"].notna() & cand[\"THETAM\"].notna()\n",
    "\n",
    "# 2) Physical sanity\n",
    "qc &= (cand[\"HM0\"] > 0) & (cand[\"T02\"] > 0)\n",
    "\n",
    "# 3) Distribution checks\n",
    "#    Drop timestamps with |SKEW| > 0.3 or KURT > 5 (when available)\n",
    "skew_ok = cand[\"SKEW\"].abs() <= 0.3\n",
    "kurt_ok = cand[\"KURT\"] <= 5.0\n",
    "# If SKEW/KURT missing at a row, do not auto-drop for that reason\n",
    "skew_ok = skew_ok | cand[\"SKEW\"].isna()\n",
    "kurt_ok = kurt_ok | cand[\"KURT\"].isna()\n",
    "qc &= skew_ok & kurt_ok\n",
    "\n",
    "# 4) QUALITE (if present): keep only rows with non-empty labels considered valid\n",
    "#    Many archive files have QUALITE empty; when populated, typical \"good\" tags include letters.\n",
    "if cand[\"QUALITE\"].notna().any():\n",
    "    # Keep non-empty, non-null strings that are not \"M\", \"NA\", or \"ERR\"\n",
    "    q = cand[\"QUALITE\"].astype(str).str.strip().str.upper()\n",
    "    good = ~(q.isna() | (q == \"\") | q.isin({\"M\", \"NA\", \"ERR\"}))\n",
    "    qc &= good\n",
    "\n",
    "cand_qc = cand.loc[qc, [\"DateHeure\", \"HM0\", \"T02\", \"THETAM\"]].copy()\n",
    "cand_qc = cand_qc.rename(columns={\"DateHeure\": \"time\", \"HM0\": \"hs_obs\", \"T02\": \"Tm02_obs\", \"THETAM\": \"dir_obs_deg\"})\n",
    "cand_qc[\"time\"] = pd.to_datetime(cand_qc[\"time\"], utc=True)  # TU = UTC\n",
    "cand_qc = cand_qc.set_index(\"time\").sort_index()\n",
    "\n",
    "# Restrict strictly to 2020 UTC\n",
    "start_utc = pd.Timestamp(\"2020-01-01 00:00:00\", tz=\"UTC\")\n",
    "end_utc   = pd.Timestamp(\"2020-12-31 23:59:59\", tz=\"UTC\")\n",
    "cand_qc = cand_qc.loc[start_utc:end_utc]\n",
    "\n",
    "print(f\"Candhis 2020 after QC: {len(cand_qc)} rows\")\n",
    "\n",
    "# ---------- Prepare ResourceCode subset for 2020 ----------\n",
    "# Assumes your earlier cells defined `data` with columns: 'hs', 'Tm02', 'dir' and a DatetimeIndex in UTC or naive-UTC.\n",
    "if not {\"hs\", \"Tm02\", \"dir\"}.issubset(data.columns):\n",
    "    raise KeyError(\"ResourceCode `data` must include ['hs','Tm02','dir'].\")\n",
    "\n",
    "rc = data[[\"hs\", \"Tm02\", \"dir\"]].copy()\n",
    "\n",
    "# Ensure datetime is tz-aware UTC to compare with Candhis TU\n",
    "if rc.index.tz is None:\n",
    "    rc.index = rc.index.tz_localize(\"UTC\")\n",
    "\n",
    "rc_2020 = rc.loc[start_utc:end_utc].copy()\n",
    "rc_2020 = rc_2020.sort_index()\n",
    "\n",
    "# Direction convention alignment\n",
    "if RC_IS_COMING_FROM:\n",
    "    rc_2020[\"dir_from_deg\"] = rc_2020[\"dir\"] % 360.0\n",
    "else:\n",
    "    # Convert going-to -> coming-from to match THETAM\n",
    "    rc_2020[\"dir_from_deg\"] = (rc_2020[\"dir\"] + 180.0) % 360.0\n",
    "\n",
    "rc_2020 = rc_2020.rename(columns={\"hs\": \"hs_mod\", \"Tm02\": \"Tm02_mod\"})\n",
    "\n",
    "# Time matching (nearest within ±30 min)\n",
    "left  = cand_qc.reset_index().rename(columns={\"time\": \"t_obs\"})\n",
    "right = rc_2020.reset_index().rename(columns={\"index\": \"t_mod\"})\n",
    "\n",
    "pairs = pd.merge_asof(\n",
    "    left.sort_values(\"t_obs\"),\n",
    "    right.sort_values(\"t_mod\"),\n",
    "    left_on=\"t_obs\",\n",
    "    right_on=\"t_mod\",\n",
    "    tolerance=TOL,\n",
    "    direction=\"nearest\",\n",
    ")\n",
    "\n",
    "# Drop non-matches (NaN where no model within tolerance)\n",
    "pairs = pairs.dropna(subset=[\"hs_mod\", \"Tm02_mod\", \"dir_from_deg\"])\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "hs_rmse, n_hs = rmse(pairs[\"hs_mod\"], pairs[\"hs_obs\"])\n",
    "t02_rmse, n_t = rmse(pairs[\"Tm02_mod\"], pairs[\"Tm02_obs\"])\n",
    "dir_rmse, n_d = circ_rmse_deg(pairs[\"dir_from_deg\"], pairs[\"dir_obs_deg\"])\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"metric\": [\"RMSD_Hs (m)\", \"RMSD_Tm02 (s)\", \"RMSD_Dir_THETAM (deg)\"],\n",
    "    \"value\": [hs_rmse, t02_rmse, dir_rmse],\n",
    "    \"N_pairs\": [n_hs, n_t, n_d]\n",
    "})\n",
    "\n",
    "print(\"\\nIA.5 — Candhis vs ResourceCode (2020, Belle-Île 05602)\")\n",
    "print(summary.to_string(index=False, float_format=lambda v: f\"{v:.3f}\"))\n",
    "\n",
    "# Save outputs\n",
    "out_dir = Path(\"fig\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "summary.to_csv(out_dir / \"IA5_rmsd_2020_candhis05602.csv\", index=False)\n",
    "\n",
    "# Optional quick-look plots\n",
    "fig, ax = plt.subplots(3, 1, figsize=(12, 9), sharex=True)\n",
    "ax[0].plot(pairs[\"t_obs\"], pairs[\"hs_obs\"], label=\"Buoy HM0\", lw=0.9)\n",
    "ax[0].plot(pairs[\"t_obs\"], pairs[\"hs_mod\"], label=\"Model Hs\", lw=0.9)\n",
    "ax[0].set_ylabel(\"Hs (m)\")\n",
    "ax[0].grid(alpha=0.3)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(pairs[\"t_obs\"], pairs[\"Tm02_obs\"], label=\"Buoy T02\", lw=0.9)\n",
    "ax[1].plot(pairs[\"t_obs\"], pairs[\"Tm02_mod\"], label=\"Model Tm02\", lw=0.9)\n",
    "ax[1].set_ylabel(\"Tm02 (s)\")\n",
    "ax[1].grid(alpha=0.3)\n",
    "ax[1].legend()\n",
    "\n",
    "# Direction as coming-from; plot circularly unwrapped about buoy direction to visualise differences\n",
    "d_err = circ_diff_deg(pairs[\"dir_from_deg\"], pairs[\"dir_obs_deg\"])\n",
    "ax[2].plot(pairs[\"t_obs\"], d_err, lw=0.8)\n",
    "ax[2].axhline(0, color=\"k\", lw=0.8)\n",
    "ax[2].set_ylabel(\"Dir error (deg)\")\n",
    "ax[2].set_xlabel(\"2020 UTC\")\n",
    "ax[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_dir / \"IA5_timeseries_2020.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Scatter diagnostics (optional)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4.5))\n",
    "axes[0].scatter(pairs[\"hs_obs\"], pairs[\"hs_mod\"], s=4, alpha=0.5)\n",
    "mx = np.nanmax([pairs[\"hs_obs\"].max(), pairs[\"hs_mod\"].max()])\n",
    "axes[0].plot([0, mx], [0, mx], 'k:', lw=1)\n",
    "axes[0].set_xlabel(\"Buoy HM0 (m)\")\n",
    "axes[0].set_ylabel(\"Model Hs (m)\")\n",
    "axes[0].set_title(f\"RMSD={hs_rmse:.2f} m\")\n",
    "\n",
    "axes[1].scatter(pairs[\"Tm02_obs\"], pairs[\"Tm02_mod\"], s=4, alpha=0.5)\n",
    "mx = np.nanmax([pairs[\"Tm02_obs\"].max(), pairs[\"Tm02_mod\"].max()])\n",
    "axes[1].plot([0, mx], [0, mx], 'k:', lw=1)\n",
    "axes[1].set_xlabel(\"Buoy T02 (s)\")\n",
    "axes[1].set_ylabel(\"Model Tm02 (s)\")\n",
    "axes[1].set_title(f\"RMSD={t02_rmse:.2f} s\")\n",
    "\n",
    "axes[2].hist(d_err, bins=72)\n",
    "axes[2].set_xlabel(\"Dir error (deg, THETAM)\")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "axes[2].set_title(f\"circ-RMSD={dir_rmse:.1f}°\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_dir / \"IA5_scatter_2020.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### IA.5 — Buoy–model comparison (Belle-Île 05602 vs ResourceCode), year 2020\n",
    "\n",
    "**RMSD results** (±30 min pairing; QC applied; direction uses Candhis **THETAM**, coming-from):\n",
    "- $H_s$: **0.39 m**  \n",
    "- $T_{m02}$: **0.84 s**  \n",
    "- Mean direction: **21.2°** (circular RMSD)  \n",
    "Pairs used: **15 947** half-hours.\n",
    "\n",
    "---\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "**Wave height.** Agreement is good over the 0–8 m range. Most error accrues during storms, as expected when model and buoy are not co-located.\n",
    "\n",
    "**Period.** Consistent overall. The scatter indicates a mild mismatch at the longest periods.\n",
    "\n",
    "**Direction.** A 21° circular RMSD is reasonable near a coast with mixed sea–swell and turning fronts. Spikes coincide with weak seas or multi-modal spectra where mean direction is less stable.\n",
    "\n",
    "---\n",
    "\n",
    "#### Spatial representativeness and depth effects\n",
    "\n",
    "- **Positions.** Buoy: $(47.3111^\\circ,\\,-3.3111^\\circ)$ at **45 m** depth.  \n",
    "  Model point: $(47.3236^\\circ,\\,-3.5522^\\circ)$ at **93.32 m** depth.  \n",
    "  Horizontal offset ≈ **18 km**.\n",
    "\n",
    "- **Depth regime difference.** Using the deep-water proxy $T_\\text{deep}\\approx 4\\sqrt{h/g}$:\n",
    "  - At **45 m**: $T_\\text{deep}\\approx 8.6$ s. Longer swell can enter transitional depth, with refraction and depth-induced transformation before reaching the buoy.\n",
    "  - At **93.32 m**: $T_\\text{deep}\\approx 12.3$ s. The offshore model point remains deep for a larger share of the spectrum.\n",
    "\n",
    "- **Implications.** Between 93 m and 45 m the wave field rotates and re-distributes energy. This raises direction error and can alter spectral shape enough to shift $T_{m02}$ slightly. Height differences during energetic events also reflect unmodelled small-scale sheltering by Belle-Île and local bathymetry not captured by the point-to-point comparison.\n",
    "\n",
    "---\n",
    "\n",
    "#### February–March 2020 gap\n",
    "\n",
    "The ~**1-month** hole is from the **observations**, not the model. After QC (sentinels removed; $|SKEW|\\le 0.3$; KURT $\\le 5$; QUALITE filter when present), many half-hours in late winter drop out. Typical causes are buoy maintenance, telemetry loss, or spectra failing distribution checks. The line plots bridge over missing points; no data were used there.\n",
    "\n",
    "---\n",
    "\n",
    "#### Answer to the study question\n",
    "\n",
    "For the 2020 overlap, the RMSD between ResourceCode and the Candhis Belle-Île buoy is **0.39 m** for $H_s$, **0.84 s** for $T_{m02}$, and **21.2°** for mean direction (THETAM, circular RMSD). Given the **18 km** separation and **93 m vs 45 m** depths, these errors are consistent with expected coastal transformation between the offshore model point and the nearer-shore buoy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### I.B.1. Block Maxima (BM) Analysis\n",
    "\n",
    "Here, we will apply the Block Maxima (BM) method. This approach involves dividing the $H_s$ time series into non-overlapping blocks. Based on the analysis, we will use a **block size of '365.2425D' (one year)**. The maximum $H_s$ from each annual block is extracted, and these maxima are then fitted to a **Generalized Extreme Value (GEV)** distribution to estimate return levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### I.B.1. Block Maxima (BM) Analysis\n",
    "\n",
    "Here, we will apply the Block Maxima (BM) method. This approach involves dividing the $H_s$ time series into non-overlapping blocks. Based on the analysis, we will use a **block size of '365.2425D' (one year)**. The maximum $H_s$ from each annual block is extracted, and these maxima are then fitted to a **Generalized Extreme Value (GEV)** distribution to estimate return levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create EVA object\n",
    "model_bm = EVA(data.hs)\n",
    "\n",
    "# Step 2: Extract extremes\n",
    "model_bm.get_extremes(\n",
    "    method=\"BM\",\n",
    "    block_size=\"365.2425D\",\n",
    "    extremes_type=\"high\",\n",
    "    errors=\"raise\",\n",
    "    min_last_block=0.9\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Lets check the directions of the extreme heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets find the directions of the extremes from the data\n",
    "extremes_indices = model_bm.extremes.index\n",
    "data_extreme_directions = data.loc[extremes_indices, \"dir\"]\n",
    "data_extreme_tm02 = data.loc[extremes_indices, \"Tm02\"]\n",
    "data_extreme_wspd = data.loc[extremes_indices, \"wspd\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "extremes_df_bm = (\n",
    "    pd.DataFrame({'timestamp': extremes_indices, 'hs': model_bm.extremes.values})\n",
    "    .merge(\n",
    "        data[['dir','wspd','Tm02']].reset_index().rename(columns={'index':'timestamp'}),\n",
    "        on='timestamp',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    ")\n",
    "extremes_df_bm.to_csv('data/extremes_bm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extremes\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Time series with extremes highlighted\n",
    "ax1.plot(data.index, data.hs, 'b-', alpha=0.3, linewidth=0.5, label='Full time series')\n",
    "ax1.scatter(model_bm.extremes.index, model_bm.extremes.values, color='red', s=20, alpha=0.8, label='Extremes (Block Maxima)')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Significant Wave Height (m)')\n",
    "ax1.set_title('Time Series with One-year Block Extremes')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Histogram of extremes\n",
    "ax2.hist(model_bm.extremes.values, bins=15, alpha=0.7, color='red', edgecolor='black')\n",
    "ax2.set_xlabel('Extreme Wave Height (m)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Distribution of Block Maxima Extremes')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/BM_timeseries_hist.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Print some statistics about the extremes\n",
    "print(f\"Number of extremes: {len(model_bm.extremes)}\")\n",
    "print(f\"Mean extreme value: {model_bm.extremes.mean():.3f} m\")\n",
    "print(f\"Maximum extreme value: {model_bm.extremes.max():.3f} m\")\n",
    "print(f\"Minimum extreme value: {model_bm.extremes.min():.3f} m\")\n",
    "print(f\"Standard deviation: {model_bm.extremes.std():.3f} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More detailed manual plotting of extremes\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "\n",
    "# Plot the full time series\n",
    "ax.plot(data.index, data.hs, 'b-', alpha=0.2, linewidth=0.3, label='Full time series')\n",
    "\n",
    "# Highlight extremes with different colors based on magnitude\n",
    "extremes = model_bm.extremes\n",
    "colors = plt.cm.Reds(np.linspace(0.3, 1, len(extremes)))\n",
    "scatter = ax.scatter(extremes.index, extremes.values, c=extremes.values, \n",
    "                    cmap='Reds', s=30, alpha=0.8, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Wave Height (m)')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Significant Wave Height (m)')\n",
    "ax.set_title('Block Maxima Extremes Over Time\\n(Red dots show annual maximum wave heights)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/BM_extremes_over_time.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets execute GEV fit on the extremes\n",
    "model_bm.fit_model()\n",
    "\n",
    "model_bm.plot_diagnostic(alpha=0.95)\n",
    "plt.savefig('graphs/diagnostic_plot_bm.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate return values for 1-year and 50-year period\n",
    "return_period = [1.01,1.1, 1.2, 1.3, 1.4,1.5, 2, 5, 10, 25, 50]\n",
    "summary_bm = model_bm.get_summary(return_period=return_period, alpha=0.95)\n",
    "print(summary_bm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with correct empirical return periods\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Get correct empirical return periods\n",
    "sorted_extremes = np.sort(model_bm.extremes.values)[::-1]  # Sort descending\n",
    "n_extremes = len(sorted_extremes)\n",
    "\n",
    "# Correct empirical return period calculation\n",
    "# For block maxima, empirical return period = (n + 1) / (rank)\n",
    "empirical_return_periods = (n_extremes + 1) / np.arange(1, n_extremes + 1)\n",
    "\n",
    "# Plot your observed data with correct empirical return periods\n",
    "ax.semilogx(empirical_return_periods, sorted_extremes, 'ko', markersize=4, alpha=0.7, label='Observed Extremes (Empirical)')\n",
    "\n",
    "# Add fitted model for comparison\n",
    "\n",
    "ax.semilogx(summary_bm.index, summary_bm['return value'], 'r-', linewidth=2, label='Fitted Model')\n",
    "\n",
    "# Add confidence intervals\n",
    "ax.semilogx(summary_bm.index, summary_bm['upper ci'], 'r--', linewidth=1, alpha=0.7, label='Upper 90% CI')\n",
    "ax.semilogx(summary_bm.index, summary_bm['lower ci'], 'r--', linewidth=1, alpha=0.7, label='Lower 90% CI')\n",
    "\n",
    "# Fill between confidence intervals\n",
    "ax.fill_between(summary_bm.index, summary_bm['lower ci'], summary_bm['upper ci'], alpha=0.2, color='red')\n",
    "\n",
    "# Highlight 50-year point\n",
    "ax.semilogx(50, summary_bm.loc[50, 'return value'], 'ro', markersize=8, label='50-year Return Period')\n",
    "\n",
    "ax.set_xlabel('Return Period (years)')\n",
    "ax.set_ylabel('Significant Wave Height (m)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/gev_fit_bm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### I.B.2. Peak-Over-Threshold (POT) Analysis\n",
    "\n",
    "Next, we will apply the Peak-Over-Threshold (POT) method. This involves selecting an appropriate high threshold for $H_s$. For this analysis, we will use the **98th quantile of the $H_s$ data** as the threshold. To ensure the peaks are independent events, we will use a **48-hour ('2D') declustering window**. The excesses (the peaks that exceed this threshold) are then fitted to a **Generalized Pareto Distribution (GPD)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the threshold for the POT method\n",
    "# an empricial threshold is used here based on the data's return period\n",
    "threshold = data.hs.quantile(0.99)  # meters\n",
    "print(f'Threshold: {threshold} m')\n",
    "\n",
    "# Fit Peaks-over-Threshold model for Hs\n",
    "model_pot = EVA(data.hs)\n",
    "\n",
    "model_pot.get_extremes(\n",
    "    method=\"POT\",\n",
    "    threshold=threshold,\n",
    "    r=\"48H\",\n",
    "    extremes_type=\"high\",\n",
    ")\n",
    "\n",
    "model_pot.fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_period = [1.01,1.1, 1.2, 1.3, 1.4,1.5, 2, 5, 10, 25, 50]\n",
    "summary_pot = model_pot.get_summary(\n",
    "    return_period=return_period,\n",
    "    return_period_size=\"365.2425D\",\n",
    "    alpha=0.95\n",
    ")\n",
    "print(summary_pot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_bm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_pot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bm.plot_extremes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "The parameter alpha specifies the confidence limits (default = 0.95)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pot.plot_diagnostic(alpha=0.95)\n",
    "plt.savefig('graphs/diagnostic_plot_pot.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with extended model to 1 year\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Get correct empirical return periods\n",
    "sorted_extremes = np.sort(model_pot.extremes.values)[::-1]  # Sort descending\n",
    "n_extremes = len(sorted_extremes)\n",
    "\n",
    "# Correct empirical return period calculation\n",
    "# duration in years\n",
    "years = (model_pot.data.index.max() - model_pot.data.index.min()).days / 365.25\n",
    "rate = n_extremes / years  # exceedances per year\n",
    "\n",
    "empirical_return_periods = (n_extremes + 1) / np.arange(1, n_extremes + 1) / rate\n",
    "\n",
    "# Plot your observed data with correct empirical return periods\n",
    "ax.semilogx(empirical_return_periods, sorted_extremes, 'ko', markersize=4, alpha=0.7, label='Observed Extremes (Empirical)')\n",
    "\n",
    "# Add fitted model with extended range to 1 year\n",
    "\n",
    "ax.semilogx(summary_pot.index, summary_pot['return value'], 'r-', linewidth=2, label='Fitted Model')\n",
    "\n",
    "# Add confidence intervals\n",
    "ax.semilogx(summary_pot.index, summary_pot['upper ci'], 'r--', linewidth=1, alpha=0.7, label='Upper 90% CI')\n",
    "ax.semilogx(summary_pot.index, summary_pot['lower ci'], 'r--', linewidth=1, alpha=0.7, label='Lower 90% CI')\n",
    "\n",
    "# Fill between confidence intervals\n",
    "ax.fill_between(summary_pot.index, summary_pot['lower ci'], summary_pot['upper ci'], alpha=0.2, color='red')\n",
    "\n",
    "# Highlight 50-year point\n",
    "ax.semilogx(50, summary_pot.loc[50, 'return value'], 'ro', markersize=8, label='50-year Return Period')\n",
    "\n",
    "# Add threshold as dotted horizontal line\n",
    "ax.axhline(y=threshold, color='blue', linestyle=':', linewidth=2, alpha=0.8, label=f'Threshold = {threshold:.2f} m')\n",
    "ax.set_xlabel('Return Period (years)')\n",
    "ax.set_ylabel('Significant Wave Height (m)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/gpd_fit_pot.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "extremes_indices_pot = model_pot.extremes.index\n",
    "extremes_df_pot = (\n",
    "    pd.DataFrame({'timestamp': extremes_indices_pot, 'hs': model_pot.extremes.values})\n",
    "    .merge(\n",
    "        data[['dir','wspd','Tm02']].reset_index().rename(columns={'index':'timestamp'}),\n",
    "        on='timestamp',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    ")\n",
    "extremes_df_bm.to_csv('data/extremes_pot.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "The parameter n_samples indicates the number of bootstrap samples used to estimate the confidence bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add direction and period to the extremes\n",
    "indices_bm = model_bm.extremes.index\n",
    "indices_pot = model_pot.extremes.index\n",
    "\n",
    "periods_bm = data.loc[indices_bm, ['tp']]\n",
    "periods_pot = data.loc[indices_pot, ['tp']]\n",
    "\n",
    "dir_bm = data.loc[indices_bm, ['dir']]\n",
    "dir_pot = data.loc[indices_pot, ['dir']]\n",
    "\n",
    "# Extract quantiles for the periods\n",
    "\n",
    "q10_periods_pot = periods_pot.quantile(0.10)\n",
    "q50_periods_pot = periods_pot.quantile(0.50)\n",
    "q90_periods_pot = periods_pot.quantile(0.90)\n",
    "\n",
    "q10_periods_bm = periods_bm.quantile(0.10)\n",
    "q50_periods_bm = periods_bm.quantile(0.50)\n",
    "q90_periods_bm = periods_bm.quantile(0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(data.tp, bins=50, color='skyblue', edgecolor='k', alpha=0.7)\n",
    "plt.axvline(q10_periods_pot.values, color='red', linestyle='--', label='10% quantile')\n",
    "plt.axvline(q50_periods_pot.values, color='orange', linestyle='--', label='50% quantile (median)')\n",
    "plt.axvline(q90_periods_pot.values, color='green', linestyle='--', label='90% quantile')\n",
    "plt.xlabel('Peak Wave Period (s)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('graphs/extreme_period_quantiles_pot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(data.tp, bins=50, color='skyblue', edgecolor='k', alpha=0.7)\n",
    "plt.axvline(q10_periods_bm.values, color='red', linestyle='--', label='10% quantile')\n",
    "plt.axvline(q50_periods_bm.values, color='orange', linestyle='--', label='50% quantile (median)')\n",
    "plt.axvline(q90_periods_bm.values, color='green', linestyle='--', label='90% quantile')\n",
    "plt.xlabel('Peak Wave Period (s)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('graphs/extreme_period_quantiles_bm.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two summary dataframes\n",
    "summary_bm['Method'] = 'Block Maxima'\n",
    "summary_pot['Method'] = 'POT'\n",
    "comparison_df = pd.concat([summary_bm, summary_pot])\n",
    "\n",
    "# Format the table for display\n",
    "pd.set_option('display.float_format', lambda x: '{:.2f}'.format(x))\n",
    "comparison_df.to_csv('data/summary_bm_pot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_return_value = summary_bm[\"return value\"] - summary_pot[\"return value\"]\n",
    "delta_lower_ci = summary_bm[\"lower ci\"] - summary_pot[\"lower ci\"]\n",
    "delta_upper_ci = summary_bm[\"upper ci\"] - summary_pot[\"upper ci\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(delta_return_value.values)\n",
    "print(delta_lower_ci.values)\n",
    "print(delta_upper_ci.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_pot.extremes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of associated wave period and direction for extreme events\n",
    "\n",
    "# Get extreme events from both BM and POT methods\n",
    "bm_extremes = model_bm.extremes  # Block Maxima extremes\n",
    "pot_extremes = model_pot.extremes  # POT extremes\n",
    "\n",
    "print(\"=== EXTREME WAVE EVENTS ANALYSIS ===\")\n",
    "print(f\"BM method: {len(bm_extremes)} extremes\")\n",
    "print(f\"POT method: {len(pot_extremes)} extremes\")\n",
    "\n",
    "# Get associated wave periods and directions for BM extremes\n",
    "bm_periods = data.loc[bm_extremes.index, 'Tm02']\n",
    "bm_directions = data.loc[bm_extremes.index, 'dir']\n",
    "\n",
    "# Get associated wave periods and directions for POT extremes  \n",
    "pot_periods = data.loc[pot_extremes.index, 'Tm02']\n",
    "pot_directions = data.loc[pot_extremes.index, 'dir']\n",
    "\n",
    "# Calculate quantiles for wave periods\n",
    "bm_period_quantiles = bm_periods.quantile([0.10, 0.50, 0.90])\n",
    "pot_period_quantiles = pot_periods.quantile([0.10, 0.50, 0.90])\n",
    "\n",
    "print(\"\\n=== WAVE PERIOD QUANTILES (s) ===\")\n",
    "print(\"BM Method:\")\n",
    "print(f\"  10% quantile: {bm_period_quantiles[0.10]:.2f} s\")\n",
    "print(f\"  50% quantile: {bm_period_quantiles[0.50]:.2f} s\") \n",
    "print(f\"  90% quantile: {bm_period_quantiles[0.90]:.2f} s\")\n",
    "\n",
    "print(\"\\nPOT Method:\")\n",
    "print(f\"  10% quantile: {pot_period_quantiles[0.10]:.2f} s\")\n",
    "print(f\"  50% quantile: {pot_period_quantiles[0.50]:.2f} s\")\n",
    "print(f\"  90% quantile: {pot_period_quantiles[0.90]:.2f} s\")\n",
    "\n",
    "# Calculate circular statistics for wave directions\n",
    "def circular_mean_deg(directions):\n",
    "    \"\"\"Calculate circular mean of directions in degrees\"\"\"\n",
    "    directions_rad = np.deg2rad(directions.dropna())\n",
    "    if len(directions_rad) == 0:\n",
    "        return np.nan\n",
    "    mean_x = np.cos(directions_rad).mean()\n",
    "    mean_y = np.sin(directions_rad).mean()\n",
    "    return np.degrees(np.arctan2(mean_y, mean_x)) % 360\n",
    "\n",
    "def circular_std_deg(directions):\n",
    "    \"\"\"Calculate circular standard deviation of directions in degrees\"\"\"\n",
    "    directions_rad = np.deg2rad(directions.dropna())\n",
    "    if len(directions_rad) == 0:\n",
    "        return np.nan\n",
    "    R = np.sqrt(np.cos(directions_rad).mean()**2 + np.sin(directions_rad).mean()**2)\n",
    "    return np.degrees(np.sqrt(-2 * np.log(R)))\n",
    "\n",
    "# Calculate direction statistics for extremes\n",
    "bm_dir_mean = circular_mean_deg(bm_directions)\n",
    "bm_dir_std = circular_std_deg(bm_directions)\n",
    "pot_dir_mean = circular_mean_deg(pot_directions)\n",
    "pot_dir_std = circular_std_deg(pot_directions)\n",
    "\n",
    "# Calculate direction statistics for entire dataset\n",
    "all_dir_mean = circular_mean_deg(data['dir'])\n",
    "all_dir_std = circular_std_deg(data['dir'])\n",
    "\n",
    "print(\"\\n=== WAVE DIRECTION ANALYSIS ===\")\n",
    "print(\"BM Method Extremes:\")\n",
    "print(f\"  Mean direction: {bm_dir_mean:.1f}°\")\n",
    "print(f\"  Circular std: {bm_dir_std:.1f}°\")\n",
    "\n",
    "print(\"\\nPOT Method Extremes:\")\n",
    "print(f\"  Mean direction: {pot_dir_mean:.1f}°\")\n",
    "print(f\"  Circular std: {pot_dir_std:.1f}°\")\n",
    "\n",
    "print(\"\\nEntire Dataset:\")\n",
    "print(f\"  Mean direction: {all_dir_mean:.1f}°\")\n",
    "print(f\"  Circular std: {all_dir_std:.1f}°\")\n",
    "\n",
    "# Create summary table\n",
    "summary_data = {\n",
    "    'Method': ['BM Extremes', 'POT Extremes', 'All Data'],\n",
    "    'Count': [len(bm_extremes), len(pot_extremes), len(data)],\n",
    "    'Mean_Direction_deg': [bm_dir_mean, pot_dir_mean, all_dir_mean],\n",
    "    'Circular_Std_deg': [bm_dir_std, pot_dir_std, all_dir_std],\n",
    "    'Period_10pct_s': [bm_period_quantiles[0.10], pot_period_quantiles[0.10], np.nan],\n",
    "    'Period_50pct_s': [bm_period_quantiles[0.50], pot_period_quantiles[0.50], np.nan],\n",
    "    'Period_90pct_s': [bm_period_quantiles[0.90], pot_period_quantiles[0.90], np.nan]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n=== SUMMARY TABLE ===\")\n",
    "print(summary_df.round(2))\n",
    "\n",
    "# Save to CSV\n",
    "summary_df.to_csv('data/summary_bm_pot.csv', index=False)\n",
    "print(f\"\\nSummary saved to: data/summary_bm_pot.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "### BM vs. POT: Comparison of Extreme Event Characteristics\n",
    "\n",
    "**Method**\n",
    "\n",
    "- Characteristics of extreme wave events were compared using two methods: **Block Maxima (BM)** and **Peak-Over-Threshold (POT)**.\n",
    "- The **BM method** (using annual maxima) identified **27 extreme events**.\n",
    "- The **POT method** (using the 98th quantile and 48-hour declustering) identified **137 extreme events**.\n",
    "- We analyzed:\n",
    "  1.  The physical characteristics (wave period, direction, and seasonality) of the event sets.\n",
    "  2.  A quantitative model comparison by calculating the delta ($\\Delta = \\text{BM value} - \\text{POT value}$) for the $H_s$ return value and its 95% confidence intervals.\n",
    "\n",
    "---\n",
    "\n",
    "**Results: Wave Period (s)**\n",
    "\n",
    "- **BM Extremes:**\n",
    "  - 10% quantile: **8.94 s**\n",
    "  - 50% quantile (Median): **9.47 s**\n",
    "  - 90% quantile: **10.68 s**\n",
    "- **POT Extremes:**\n",
    "  - 10% quantile: **8.40 s**\n",
    "  - 50% quantile (Median): **9.06 s**\n",
    "  - 90% quantile: **10.01 s**\n",
    "\n",
    "---\n",
    "\n",
    "**Results: Event Characteristics (Direction & Seasonality)**\n",
    "\n",
    "**Wave Direction (°)**\n",
    "- **BM Extremes:**\n",
    "  - Mean direction: **255.9°**\n",
    "  - Circular std: **11.6°**\n",
    "- **POT ExtremES:**\n",
    "  - Mean direction: **260.4°**\n",
    "  - Circular std: **12.6°**\n",
    "- **Entire Dataset:**\n",
    "  - Mean direction: **275.5°**\n",
    "  - Circular std: **34.5°**\n",
    "\n",
    "**Seasonality (based on 27 BM events)**\n",
    "- **Winter (Dec, Jan, Feb): 21 events (77.8%)**\n",
    "- Spring (Mar, Apr, May): 2 events (7.4%)\n",
    "- Summer (Jun, Jul, Aug): 0 events (0.0%)\n",
    "- Autumn (Sep, Oct, Nov): 4 events (14.8%)\n",
    "\n",
    "---\n",
    "\n",
    "**Results: Model Deltas (m)**\n",
    "\n",
    "| Return Period (yr) | Return Value Delta (BM-POT) | Lower CI Delta (BM-POT) | Upper CI Delta (BM-POT) |\n",
    "| :--- | :---: | :---: | :---: |\n",
    "| 1.01 | -1.64 | -1.81 | -0.99 |\n",
    "| 1.1 | -1.03 | -1.10 | -0.67 |\n",
    "| 1.2 | -0.81 | -0.86 | -0.54 |\n",
    "| 1.3 | -0.68 | -0.74 | -0.47 |\n",
    "| 1.4 | -0.59 | -0.66 | -0.43 |\n",
    "| 1.5 | -0.53 | -0.60 | -0.41 |\n",
    "| 2.0 | -0.33 | -0.39 | -0.32 |\n",
    "| 5.0 | 0.05 | 0.02 | -0.13 |\n",
    "| 10.0 | 0.31 | 0.24 | 0.01 |\n",
    "| 25.0 | 0.71 | 0.53 | 0.52 |\n",
    "| 50.0 | 1.07 | 0.77 | 0.88 |\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "- **Physical Characteristics:**\n",
    "  - **Seasonality & Direction:** The physical drivers for extreme events are highly consistent. The analysis of annual maxima shows that events are **overwhelmingly concentrated in Winter (77.8%)** and nonexistent in Summer.\n",
    "  - Furthermore, these events originate from a **highly consistent direction** (255.9°-260.4°) that is distinct from the overall mean wave direction (275.5°), suggesting a specific and predictable storm track.\n",
    "  - **Period:** Events identified by the BM method (the largest of the large) are associated with **longer wave periods** than the broader set of POT events (e.g., BM median of 9.47 s vs POT median of 9.06 s).\n",
    "\n",
    "- **Model Divergence:**\n",
    "  - The two models show a **clear structural divergence** depending on the return period.\n",
    "  - **Short-Term Events (< 5 years):** The **POT model is more conservative** (predicts higher $H_s$ values), as shown by the negative deltas. For a 2-year event, the BM estimate is 0.33 m lower.\n",
    "  - **Long-Term Events (> 10 years):** The **BM model becomes more conservative** (predicts higher $H_s$ values), with the 50-year estimate being 1.07 m higher than POT's.\n",
    "  - **Crossover Point:** The models are in closest agreement around the **5-year return period**.\n",
    "\n",
    "- **Conclusion:** This divergence is critical for engineering design. The POT model, based on **137 data points**, is statistically more stable. The BM model, using only 27 annual maxima, is highly sensitive to the few largest events in the record, which appears to be driving its steeper, higher-extrapolated values at the 25- and 50-year periods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "5. (optional) Comparison to multivariate method. If you have time to go further, you may\n",
    "search for the MHKiT python toolbox that will allow you to calculate the 50-year return contour\n",
    "using the modified I-FORM method.\n",
    "Q: Show a graphic of your results and compare this to the values obtained with the univariate\n",
    "analysis (e.g. add those points to the environmental contour plot). Are they similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return period (years) of interest\n",
    "period = 50\n",
    "\n",
    "\n",
    "\n",
    "# Get only the values from the DataFrame\n",
    "hs = data['hs'].values\n",
    "tp = data['tp'].values\n",
    "\n",
    "# Delta time of sea-states\n",
    "dt = (data.index[2] - data.index[1]).seconds\n",
    "\n",
    "# Get the contour values\n",
    "copula = contours.environmental_contours(hs, tp, dt, period, \"PCA\", return_PCA=True)\n",
    "Hm0_contour = copula[\"PCA_x1\"]\n",
    "Te_contour = copula[\"PCA_x2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "# %matplotlib inline\n",
    "ax = graphics.plot_environmental_contour(\n",
    "    tp,\n",
    "    hs,\n",
    "    Te_contour,\n",
    "    Hm0_contour,\n",
    "    data_label=\"Resourcecode dataset\",\n",
    "    contour_label=\"50 Year Contour\",\n",
    "    x_label=\"Energy Period, $Te$ [s]\",\n",
    "    y_label=\"Sig. wave height, $Hm0$ [m]\",\n",
    "    ax=ax,\n",
    ")\n",
    "plt.savefig('graphs/50_year_contour.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
